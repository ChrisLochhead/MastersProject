{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.0.1 (SDL 2.0.14, Python 3.6.10)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "#Pygame imports\n",
    "import pygame\n",
    "import pygame_menu\n",
    "import pygame.freetype\n",
    "\n",
    "#Standard imports\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import os.path\n",
    "\n",
    "#OpenAI gym\n",
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box\n",
    "\n",
    "#Keras RL\n",
    "from rl.agents import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from tensorflow.keras.backend import clear_session\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "#Import local modules\n",
    "sys.path.append(os.getcwd())\n",
    "from ipynb.fs.full.Games.SpaceInvaders import Space_Invaders\n",
    "from ipynb.fs.full.Games.Asteroids import Asteroids\n",
    "pygame.init()\n",
    "\n",
    "#Global window length\n",
    "WINDOW_LENGTH = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Custom_OpenAI_Env(Env):\n",
    "    def __init__(self, screen_width, screen_height, game, \n",
    "                 action_space = Discrete(5)):\n",
    "        # Assign action and observation space\n",
    "        self.action_space = action_space\n",
    "        self.observation_space = Box(0, 255, shape=(1, screen_width, screen_height, ))\n",
    "\n",
    "        #Initialise the game\n",
    "        self.game = game\n",
    "\n",
    "        #Assign colour, get the initial game state and record it \n",
    "        self.state = self.game.get_state()\n",
    "        self.start_state = self.state\n",
    "    \n",
    "        print(\"initialisation complete\")\n",
    "        \n",
    "        self.delay = 1000\n",
    "        \n",
    "    def step(self, action):\n",
    "        # Apply action\n",
    "        self.game.execute_action(action)\n",
    "        \n",
    "        #Call the update loop before getting the state\n",
    "        self.game.update()\n",
    "        self.state = self.game.get_state()\n",
    "        \n",
    "        #Calculate step-based reward\n",
    "        reward = self.game.calculate_reward()\n",
    "        done = self.game.done\n",
    "        \n",
    "        # Set placeholder for info (required for AI env superclass step method)\n",
    "        info = {}\n",
    "        # Return step information\n",
    "        return self.state, reward, done, info\n",
    "    \n",
    "    def render(self, mode):\n",
    "        self.game.render()\n",
    "        \n",
    "    def reset(self):\n",
    "        #Restart the game\n",
    "        self.state = self.start_state\n",
    "        self.game.reset()\n",
    "        return self.game.get_state()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Game menu class controlling the functionality of the entire framework\n",
    "class Game_Menu:\n",
    "\n",
    "    def __init__(self, width, height):\n",
    "        self.menu = None\n",
    "        self.surface = None\n",
    "        self.game = None\n",
    "        self.icon_surface = None\n",
    "        \n",
    "        #Set standard network parameters\n",
    "        self.learning_rates = [0.0001, 0.000001, 0.0000001]\n",
    "        self.episodes = [1, 10, 100]\n",
    "        self.steps = [3000, 50000, 100000]\n",
    "\n",
    "        #Standard parameter indices\n",
    "        self.learning_rate = 0\n",
    "        self.episode = 0\n",
    "        self.step = 0\n",
    "        self.visualize = False\n",
    "\n",
    "        #Asteroid specific parameters\n",
    "        self.scales = [[800,600, 1.0], [200,200, 0.65], [150,150, 0.45]]\n",
    "        self.intensity_rates = [0, 1, 3]\n",
    "        self.player_speeds = [1, 2, 3]\n",
    "\n",
    "        #Scale, player speed and colour are common to both games\n",
    "        self.scale = 0\n",
    "        self.intensity = 0\n",
    "        self.player_speed = 0\n",
    "        self.homogenous_controls = False\n",
    "        self.colour = False\n",
    "        \n",
    "        #Space invaders specific parameters\n",
    "        self.enemy_speeds = [0.5, 1.0, 2.0]\n",
    "        self.enemy_speed = 0\n",
    "        \n",
    "        #Transfer mode \n",
    "        self.transfer = -1\n",
    "        self.test_env = -1\n",
    "        \n",
    "        #Initialise pygame\n",
    "        pygame.display.init()\n",
    "        self.surface = pygame.display.set_mode((800, 600))\n",
    "        self.icon_surface = pygame.image.load(os.path.join(os.getcwd(), \"Dependencies/Resources\", \"Masterslogo.png\"))\n",
    "        pygame.display.set_icon(self.icon_surface)\n",
    "        pygame.display.set_caption(\"Master's Project\")\n",
    "        \n",
    "        #Start the main menu\n",
    "        self.main_menu()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        #Reset all adjustable variables when returning to the main menu.\n",
    "        self.enemy_speed = 0\n",
    "        self.scale = 0\n",
    "        self.intensity = 0\n",
    "        self.player_speed = 0\n",
    "        self.homogenous_controls = False\n",
    "        self.colour = False\n",
    "        self.learning_rate = 0\n",
    "        self.episode = 0\n",
    "        self.step = 0\n",
    "        self.visualize = False\n",
    "        \n",
    "    def main_menu(self):\n",
    "        self.reset_parameters()\n",
    "        self.surface = pygame.display.set_mode((800, 600))\n",
    "        #Disable any menu if it exists\n",
    "        if self.menu:\n",
    "            self.menu.disable()\n",
    "        #Initialise the main menu interface\n",
    "        self.menu = pygame_menu.Menu(600, 800, 'Main Menu',\n",
    "                         theme=pygame_menu.themes.THEME_DARK)\n",
    "        self.menu.add_button('Space Invaders', self.start_space_invaders)\n",
    "        self.menu.add_button('Space Invaders - Training', self.set_game_mode)\n",
    "        self.menu.add_button('Asteroids', self.start_asteroids)\n",
    "        self.menu.add_button('Asteroids - Training', self.set_standard_parameters)\n",
    "        self.menu.add_button('Transfer Learning', self.transfer_menu)\n",
    "        self.menu.add_button('Test - Space Invaders', self.set_test_space_invaders)\n",
    "        self.menu.add_button('Test - Asteroids', self.set_test_asteroids)\n",
    "        self.menu.add_button('Quit', pygame_menu.events.EXIT)\n",
    "        \n",
    "        self.menu.mainloop(self.surface)\n",
    "\n",
    "    def transfer_menu(self):\n",
    "        #Transfer menu, always disable because this will never be the first\n",
    "        #menu created\n",
    "        self.menu.disable()\n",
    "        self.menu = pygame_menu.Menu(600, 800, 'Transfer Learning',\n",
    "                               theme=pygame_menu.themes.THEME_DARK)\n",
    "        self.menu.add_button('Space Invaders -> Asteroids', self.set_transfer_SA)\n",
    "        self.menu.add_button('Asteroids -> Space Invaders', self.set_transfer_AS)\n",
    "        self.menu.add_button('Back', self.main_menu)\n",
    "        \n",
    "        self.menu.mainloop(self.surface)\n",
    "    \n",
    "    \n",
    "    #Common DQN network settings\n",
    "    def set_standard_param(self, arg, param):\n",
    "        #Learning rate \n",
    "        if param == 0:\n",
    "            print(\"changing learning rate\")\n",
    "            if self.learning_rate < 2:\n",
    "                self.learning_rate += 1\n",
    "            else:\n",
    "                self.learning_rate = 0\n",
    "        #Steps\n",
    "        elif param == 1:\n",
    "            if self.step < 2:\n",
    "                self.step += 1\n",
    "            else:\n",
    "                self.step = 0\n",
    "        #Episodes\n",
    "        elif param == 2:\n",
    "            if self.episode < 2:\n",
    "                self.episode += 1\n",
    "            else:\n",
    "                self.episode = 0\n",
    "        #Visualize\n",
    "        elif param == 3:\n",
    "            self.visualize = 1 if self.visualize == 0 else 1 \n",
    "            \n",
    "    def set_game_mode(self):\n",
    "        self.set_standard_parameters(1)\n",
    "        \n",
    "    def set_test_asteroids(self):\n",
    "        self.test_env = 0\n",
    "        self.test_model()\n",
    "    \n",
    "    def set_test_space_invaders(self):\n",
    "        self.test_env = 1\n",
    "        self.test_model()\n",
    "        \n",
    "    #Set transfer mode: Space invaders to Asteroids\n",
    "    def set_transfer_SA(self):\n",
    "        self.transfer = 0\n",
    "        self.set_standard_parameters()\n",
    "        \n",
    "    #Set transfer mode: Asteroids to Space Invaders\n",
    "    def set_transfer_AS(self):\n",
    "        self.transfer = 1\n",
    "        self.set_standard_parameters(1)\n",
    "    \n",
    "    #Setup menu for standard network parameters\n",
    "    def set_standard_parameters(self, gamemode = 0):\n",
    "        print(\"standard params called\")\n",
    "        self.menu.disable()\n",
    "        self.gamemode = gamemode\n",
    "        self.menu = pygame_menu.Menu(600, 800, 'Select Variables',\n",
    "                               theme=pygame_menu.themes.THEME_DARK)\n",
    "\n",
    "        self.menu.add_selector('Learning rate :', [('1e-4', 0),\n",
    "                                                   ('1e-6', 0),\n",
    "                                                   ('1e-7', 0)],\n",
    "                                                   onchange=self.set_standard_param)\n",
    "        self.menu.add_selector('Steps :', [('3000', 1),\n",
    "                                           ('50,000', 1),\n",
    "                                           ('100,000', 1),],\n",
    "                                           onchange=self.set_standard_param)\n",
    "        self.menu.add_selector('Episodes: ', [('1', 2),\n",
    "                                           ('10', 2),\n",
    "                                           ('100', 2),],\n",
    "                                           onchange=self.set_standard_param)\n",
    "        self.menu.add_selector('Visualize', [('False', 3), ('True', 3)],\n",
    "                               onchange=self.set_standard_param)\n",
    "        self.menu.add_button('Back', self.main_menu)\n",
    "        \n",
    "        #Continue based on what game mode is selected\n",
    "        if gamemode == 0:\n",
    "            self.menu.add_button('Continue - Asteroids', self.set_asteroids_parameters)\n",
    "        else:\n",
    "            self.menu.add_button('Continue - Space Invaders', self.set_space_invaders_parameters)\n",
    "        \n",
    "        self.menu.mainloop(self.surface)\n",
    "    \n",
    "    def set_mode_parameter(self, arg, param):\n",
    "        #0 scale, 1 intensity, 2, player speed, 3 homo controls, 4 colours, 5 enemy speed\n",
    "        if param == 0:\n",
    "            self.scale += 1 if self.scale < 2 else 0\n",
    "        if param == 1:\n",
    "            self.intensity += 1 if self.intensity < 2 else 0        \n",
    "        if param == 2:\n",
    "            self.player_speed += 1 if self.player_speed < 2 else 0        \n",
    "        if param == 3:\n",
    "            self.homogenous_controls = True if self.homogenous_controls == False else False\n",
    "        if param == 4:\n",
    "            self.colour = True if self.colour == False else False\n",
    "        if param == 5:\n",
    "            self.enemy_speed += 1 if self.enemy_speed < 2 else 0\n",
    "            \n",
    "    #Menu for setting asteroids game mode specific parameters\n",
    "    def set_asteroids_parameters(self):\n",
    "        self.menu.disable()\n",
    "        self.menu = pygame_menu.Menu(600, 800, 'Select Gameplay Variables',\n",
    "                               theme=pygame_menu.themes.THEME_DARK)\n",
    "\n",
    "        self.menu.add_selector('Scale :', [('(800, 600)', 0), ('(400, 300)', 0),('(150, 150)', 0)],\n",
    "                               onchange=self.set_mode_parameter)\n",
    "        self.menu.add_selector('Intensity:', [('0.1', 1), ('0.5', 1),('1.0', 1)],\n",
    "                               onchange=self.set_mode_parameter)\n",
    "        self.menu.add_selector('Player Speed :', [('1', 2), ('1.5', 2),('3', 2)],\n",
    "                               onchange=self.set_mode_parameter)\n",
    "        self.menu.add_selector('Colour Input: ', [('False', 4), ('True', 4)],\n",
    "                        onchange=self.set_mode_parameter)\n",
    "        self.menu.add_button('Start Training', self.start_asteroids_training)\n",
    "        self.menu.add_button('Back', self.main_menu)\n",
    "        \n",
    "        self.menu.mainloop(self.surface)\n",
    "    \n",
    "    #Menu for setting Space invaders game mode specific parameters\n",
    "    def set_space_invaders_parameters(self):\n",
    "        self.menu.disable()\n",
    "        self.menu = pygame_menu.Menu(600, 800, 'Select Gameplay Variables',\n",
    "                               theme=pygame_menu.themes.THEME_DARK)\n",
    "\n",
    "        self.menu.add_selector('Scale :', [('(800, 600)', 0), ('(300, 300)', 0),\n",
    "                                          ('(150, 150)', 0)],\n",
    "                               onchange=self.set_mode_parameter)\n",
    "        self.menu.add_selector('Enemy Speed:', [('0.5', 5), ('1.0', 5),\n",
    "                                          ('2.0', 5)],\n",
    "                               onchange=self.set_mode_parameter)\n",
    "        self.menu.add_selector('Player Speed :', [('1', 2), ('1.5', 2),\n",
    "                                          ('3', 2)],\n",
    "                               onchange=self.set_mode_parameter)\n",
    "        self.menu.add_selector('Colour Input: ', [('False', 4), ('True', 4)],\n",
    "                        onchange=self.set_mode_parameter)\n",
    "        self.menu.add_selector('Homogenous Controls: ', [('False', 3), ('True', 3)],\n",
    "                onchange=self.set_mode_parameter)\n",
    "        self.menu.add_button('Start Training', self.start_space_invaders_training)\n",
    "        self.menu.add_button('Back', self.main_menu)\n",
    "        \n",
    "        self.menu.mainloop(self.surface)\n",
    "\n",
    "    #Start asteroids as a normal player\n",
    "    def start_asteroids(self):\n",
    "        self.game = Asteroids(800, 600, self.surface, False, 0.125)\n",
    "\n",
    "    #Start space invaders as a normal player\n",
    "    def start_space_invaders(self):\n",
    "        self.game = Space_Invaders(800, 600, self.surface, False, 1.0)\n",
    "        \n",
    "    def start_asteroids_training(self):\n",
    "        self.menu.disable()\n",
    "        print(\"starting asteroid training\")\n",
    "        training_game = Asteroids(self.scales[self.scale][0], self.scales[self.scale][1], \n",
    "                                  self.surface, True, self.scales[self.scale][2], #add player speed\n",
    "                                  player_speed = self.player_speeds[self.player_speed],\n",
    "                                  player_rtspeed = self.player_speeds[self.player_speed], \n",
    "                                  intensity_modifier = self.intensity_rates[self.intensity],\n",
    "                                  has_colour = self.colour)\n",
    "        \n",
    "        self.game = Custom_OpenAI_Env(self.scales[self.scale][0], self.scales[self.scale][1],\n",
    "                                       training_game)\n",
    "        self.build_model()\n",
    "        self.train_model()\n",
    "        if self.transfer == -1:\n",
    "            self.save(\"asteroids\")\n",
    "        else:\n",
    "            self.save(\"Space-Asteroids-Transfer\")\n",
    "        self.main_menu()\n",
    "    \n",
    "    def start_space_invaders_training(self):\n",
    "        self.menu.disable()\n",
    "        print(\"Starting space invaders training\")\n",
    "        training_game = Space_Invaders(self.scales[self.scale][0], self.scales[self.scale][1]\n",
    "                                       , self.surface, True, self.scales[self.scale][2],\n",
    "                                       enemy_speed = self.enemy_speeds[self.enemy_speed],\n",
    "                                       player_speed = self.player_speeds[self.player_speed],\n",
    "                                       game_intensity_modifier = self.intensity_rates[self.intensity],\n",
    "                                       homogenous_controls = self.homogenous_controls,\n",
    "                                       has_colour = self.colour)\n",
    "        \n",
    "        #Assign new action space based on homogenous control setting\n",
    "        action_space= Discrete(5)\n",
    "        self.game = Custom_OpenAI_Env(self.scales[self.scale][0], self.scales[self.scale][1],\n",
    "                                     training_game,\n",
    "                                     action_space = action_space)\n",
    "        self.build_model()\n",
    "        self.train_model()\n",
    "        if self.transfer == -1:\n",
    "            self.save(\"space-invaders\")\n",
    "        else:\n",
    "            self.save(\"Asteroids-Space-Transfer\")\n",
    "        self.main_menu()\n",
    "        \n",
    "    def build_model(self, metrics = ['mae']):\n",
    "        print(\"building model\")\n",
    "        \n",
    "        #Initialise state/action arrays\n",
    "        states = self.game.observation_space.shape\n",
    "        states = (WINDOW_LENGTH, states[1], states[2])\n",
    "        print(states)\n",
    "        self.actions = self.game.action_space.n\n",
    "        \n",
    "        #Initialise the DRL model based on transfer setting\n",
    "        #if self.transfer == -1:\n",
    "        self.model = self.build_network(states, self.actions)\n",
    "\n",
    "        if self.transfer == 0:\n",
    "            print(\"loading space invaders original model.\")\n",
    "            if self.load(\"space-invaders\") != False:\n",
    "                self.load(\"space-invaders\")\n",
    "            else:\n",
    "                self.model = self.build_network(states, self.actions)\n",
    "                \n",
    "        elif self.transfer == 1:\n",
    "            print(\"loading asteroids original model\")\n",
    "            if self.load(\"asteroids\") != False:\n",
    "                self.load(\"asteroids\")\n",
    "            else:\n",
    "                self.model = self.build_network(states, self.actions)\n",
    "\n",
    "        \n",
    "        self.dqn = self.build_agent(self.model, self.actions)\n",
    "        self.dqn.compile(Adam(beta_1 = 0.99, lr=self.learning_rates[self.learning_rate]), metrics=metrics)\n",
    "        \n",
    "    def train_model(self, episodes = 5, verbose = 1):\n",
    "        #Initialise the DQN agent\n",
    "        #Debug info\n",
    "        print(\"parameters: \", \"\\nlearning rate: \", self.learning_rates[self.learning_rate],\n",
    "             \"\\nepisodes: \",  self.episodes[self.episode],\n",
    "              \"\\nsteps: \", self.steps[self.step],\n",
    "              \"\\nvisualize: \", self.visualize,\n",
    "              \"\\nscale: \", self.scales[self.scale],\n",
    "              \"\\nintensity (asteroids only): \", self.intensity_rates[self.intensity],\n",
    "              \"\\nplayer speed : \", self.player_speeds[self.player_speed],\n",
    "              \"\\nhomogenous controls (space invaders only): \", self.homogenous_controls,\n",
    "              \"\\ncolour: \", self.colour)\n",
    "\n",
    "        print(\"beginning training\")\n",
    "        #Fit with openAI gym\n",
    "        self.dqn.fit(self.game, nb_steps=self.steps[self.step], visualize=self.visualize, verbose=2)\n",
    "\n",
    "        print(\"training complete\")\n",
    "        self.test_model()\n",
    "\n",
    "    \n",
    "    def test_model(self):\n",
    "        #Test with scores\n",
    "        action_space= Discrete(5)\n",
    "        \n",
    "        if self.test_env == 1:\n",
    "            training_game = Space_Invaders(self.scales[2][0], self.scales[2][1]\n",
    "                                    , self.surface, True, self.scales[2][2],\n",
    "                                    homogenous_controls = self.homogenous_controls)\n",
    "            self.game = Custom_OpenAI_Env(self.scales[2][0], self.scales[2][1],\n",
    "                                         training_game, colour=self.colour,\n",
    "                                         action_space = action_space)\n",
    "        elif self.test_env == 0:\n",
    "\n",
    "            training_game = Asteroids(self.scales[2][0], self.scales[2][1], \n",
    "                                self.surface, True, self.scales[2][2],\n",
    "                                intensity_modifier = self.intensity_rates[self.intensity])\n",
    "            self.game = Custom_OpenAI_Env(self.scales[2][0], self.scales[2][1],\n",
    "                                           training_game, colour=self.colour)\n",
    "        \n",
    "        if self.test_env != -1:\n",
    "            self.build_model()\n",
    "            \n",
    "        scores = self.dqn.test(self.game, nb_episodes=100, visualize=True)\n",
    "        print(np.mean(scores.history['episode_reward']))\n",
    "        \n",
    "    def save(self, name='default-model'):\n",
    "        #was dqn now model\n",
    "        self.dqn.save_weights(name + \".hdf5\", overwrite=True)\n",
    "        \n",
    "    def load(self, name):\n",
    "        #Load existing weights into the blank model \n",
    "        if os.path.isfile(name + '.hdf5'):\n",
    "            return self.model.load_weights(name + '.hdf5')\n",
    "        print(\"file not found, creating default model.\")\n",
    "        return False\n",
    "    \n",
    "    def build_network(self, states, actions):\n",
    "        model = Sequential()\n",
    "        print(\"shape : \", len(states), actions)\n",
    "        #Convolutional layers\n",
    "        model.add(Conv2D(16, (10,10), activation='relu', input_shape=states, padding='same'))\n",
    "        model.add(MaxPooling2D((2, 2), padding = 'same'))\n",
    "        model.add(LeakyReLU(alpha=0.1))\n",
    "        model.add(Conv2D(32, (5,5), activation='relu', input_shape=states, padding='same'))\n",
    "        model.add(MaxPooling2D((2, 2), padding = 'same'))\n",
    "        model.add(LeakyReLU(alpha=0.1))\n",
    "        model.add(Flatten())\n",
    "        #Fully connected layers\n",
    "        model.add(Dense(256, activation='relu'))\n",
    "        model.add(LeakyReLU(alpha=0.1))\n",
    "        model.add(Dense(32, activation='relu'))\n",
    "        model.add(LeakyReLU(alpha=0.1))\n",
    "        model.add(Dense(32, activation='relu'))\n",
    "        model.add(LeakyReLU(alpha=0.1))\n",
    "        model.add(Dense(16, activation='relu'))\n",
    "        model.add(LeakyReLU(alpha=0.1))\n",
    "        model.add(Dense(self.actions, activation='relu'))\n",
    "        #Debug summary of the model built\n",
    "        print(model.summary())\n",
    "        return model\n",
    "\n",
    "    #Function to create DQN model \n",
    "    def build_agent(self, model, actions, policy_type = 0):\n",
    "        #Explorative policy\n",
    "        policy = BoltzmannQPolicy()\n",
    "        #Exploitative policy\n",
    "        if policy_type != 0:\n",
    "            policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=1., value_min=.1, value_test=.2, nb_steps=10000)\n",
    "        \n",
    "        memory = SequentialMemory(limit=1000, window_length=WINDOW_LENGTH)\n",
    "        dqn = DQNAgent(model=model, memory=memory, policy=policy,\n",
    "                      enable_dueling_network=True, dueling_type='avg', \n",
    "                       nb_actions=actions, nb_steps_warmup=1000\n",
    "                      )\n",
    "        return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standard params called\n",
      "changing learning rate\n",
      "changing learning rate\n",
      "starting asteroid training\n",
      "initialisation complete\n",
      "building model\n",
      "(3, 150, 150)\n",
      "shape :  3 5\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 3, 150, 16)        240016    \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 2, 75, 16)         0         \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 2, 75, 16)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 2, 75, 32)         12832     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 1, 38, 32)         0         \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 1, 38, 32)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1216)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               311552    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 5)                 85        \n",
      "=================================================================\n",
      "Total params: 574,293\n",
      "Trainable params: 574,293\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "parameters:  \n",
      "learning rate:  1e-07 \n",
      "episodes:  100 \n",
      "steps:  100000 \n",
      "visualize:  1 \n",
      "scale:  [150, 150, 0.45] \n",
      "intensity (asteroids only):  0 \n",
      "player speed :  1 \n",
      "homogenous controls (space invaders only):  False \n",
      "colour:  True\n",
      "beginning training\n",
      "Training for 100000 steps ...\n",
      "WARNING:tensorflow:From C:\\Users\\Chris\\anaconda3\\envs\\masters\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_v1.py:2070: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "    93/100000: episode: 1, duration: 1.641s, episode steps:  93, steps per second:  57, episode reward:  1.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.957 [0.000, 4.000],  loss: --, mae: --, mean_q: --\n",
      "   174/100000: episode: 2, duration: 1.351s, episode steps:  81, steps per second:  60, episode reward:  2.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.111 [0.000, 4.000],  loss: --, mae: --, mean_q: --\n",
      "   254/100000: episode: 3, duration: 1.310s, episode steps:  80, steps per second:  61, episode reward:  1.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.075 [0.000, 4.000],  loss: --, mae: --, mean_q: --\n",
      "   302/100000: episode: 4, duration: 0.793s, episode steps:  48, steps per second:  61, episode reward:  1.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.146 [0.000, 4.000],  loss: --, mae: --, mean_q: --\n",
      "   545/100000: episode: 5, duration: 4.034s, episode steps: 243, steps per second:  60, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.864 [0.000, 4.000],  loss: --, mae: --, mean_q: --\n",
      "   587/100000: episode: 6, duration: 0.705s, episode steps:  42, steps per second:  60, episode reward:  1.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.524 [0.000, 4.000],  loss: --, mae: --, mean_q: --\n",
      "   630/100000: episode: 7, duration: 0.720s, episode steps:  43, steps per second:  60, episode reward:  1.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.930 [0.000, 4.000],  loss: --, mae: --, mean_q: --\n",
      "   678/100000: episode: 8, duration: 0.805s, episode steps:  48, steps per second:  60, episode reward:  1.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.479 [0.000, 4.000],  loss: --, mae: --, mean_q: --\n",
      "   725/100000: episode: 9, duration: 0.790s, episode steps:  47, steps per second:  59, episode reward:  1.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.021 [0.000, 4.000],  loss: --, mae: --, mean_q: --\n",
      "   883/100000: episode: 10, duration: 2.605s, episode steps: 158, steps per second:  61, episode reward:  3.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.791 [0.000, 4.000],  loss: --, mae: --, mean_q: --\n",
      "   982/100000: episode: 11, duration: 1.636s, episode steps:  99, steps per second:  61, episode reward:  2.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.000 [0.000, 4.000],  loss: --, mae: --, mean_q: --\n",
      "  1042/100000: episode: 12, duration: 8.231s, episode steps:  60, steps per second:   7, episode reward:  2.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 2.550 [0.000, 4.000],  loss: 7.124681, mae: 5.730741, mean_q: 9.734556\n",
      "  1139/100000: episode: 13, duration: 17.370s, episode steps:  97, steps per second:   6, episode reward:  2.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.186 [0.000, 4.000],  loss: 6.701853, mae: 5.929616, mean_q: 10.042617\n",
      "  1177/100000: episode: 14, duration: 6.880s, episode steps:  38, steps per second:   6, episode reward:  2.000, mean reward:  0.053 [ 0.000,  1.000], mean action: 1.763 [0.000, 4.000],  loss: 6.034481, mae: 5.857978, mean_q: 10.055946\n",
      "  1220/100000: episode: 15, duration: 7.825s, episode steps:  43, steps per second:   5, episode reward:  1.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.977 [0.000, 4.000],  loss: 6.546247, mae: 5.998365, mean_q: 10.190125\n",
      "  1266/100000: episode: 16, duration: 8.363s, episode steps:  46, steps per second:   6, episode reward:  1.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.848 [0.000, 4.000],  loss: 6.218089, mae: 5.772037, mean_q: 9.777602\n",
      "  1307/100000: episode: 17, duration: 7.521s, episode steps:  41, steps per second:   5, episode reward:  1.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.732 [0.000, 4.000],  loss: 6.114770, mae: 5.722458, mean_q: 9.730925\n",
      "  1370/100000: episode: 18, duration: 11.204s, episode steps:  63, steps per second:   6, episode reward:  1.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.873 [0.000, 4.000],  loss: 5.723937, mae: 5.884473, mean_q: 9.993958\n",
      "  1508/100000: episode: 19, duration: 24.336s, episode steps: 138, steps per second:   6, episode reward:  2.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.891 [0.000, 4.000],  loss: 6.076299, mae: 5.472522, mean_q: 9.271095\n",
      "  1641/100000: episode: 20, duration: 23.808s, episode steps: 133, steps per second:   6, episode reward:  1.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.925 [0.000, 4.000],  loss: 6.018673, mae: 5.368262, mean_q: 9.110515\n",
      "  1703/100000: episode: 21, duration: 11.252s, episode steps:  62, steps per second:   6, episode reward:  1.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.919 [0.000, 4.000],  loss: 5.065660, mae: 5.426954, mean_q: 9.271194\n",
      "  1766/100000: episode: 22, duration: 11.387s, episode steps:  63, steps per second:   6, episode reward:  2.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 2.111 [0.000, 4.000],  loss: 5.195802, mae: 5.441662, mean_q: 9.297862\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1831/100000: episode: 23, duration: 11.701s, episode steps:  65, steps per second:   6, episode reward:  2.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 1.923 [0.000, 4.000],  loss: 5.614073, mae: 5.574471, mean_q: 9.529521\n",
      "  1872/100000: episode: 24, duration: 7.324s, episode steps:  41, steps per second:   6, episode reward:  1.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.951 [0.000, 4.000],  loss: 6.114672, mae: 5.413899, mean_q: 9.266161\n",
      "  1945/100000: episode: 25, duration: 12.767s, episode steps:  73, steps per second:   6, episode reward:  1.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.945 [0.000, 4.000],  loss: 7.323336, mae: 5.631390, mean_q: 9.578229\n",
      "  2041/100000: episode: 26, duration: 16.749s, episode steps:  96, steps per second:   6, episode reward:  1.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.802 [0.000, 4.000],  loss: 6.286077, mae: 5.862550, mean_q: 9.978385\n",
      "  2237/100000: episode: 27, duration: 33.980s, episode steps: 196, steps per second:   6, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.980 [0.000, 4.000],  loss: 7.615231, mae: 6.330687, mean_q: 10.723312\n",
      "  2286/100000: episode: 28, duration: 8.669s, episode steps:  49, steps per second:   6, episode reward:  2.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 2.245 [0.000, 4.000],  loss: 7.951691, mae: 7.398653, mean_q: 12.372046\n",
      "  2412/100000: episode: 29, duration: 21.841s, episode steps: 126, steps per second:   6, episode reward:  2.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.802 [0.000, 4.000],  loss: 8.554732, mae: 7.308120, mean_q: 12.333162\n",
      "  2566/100000: episode: 30, duration: 26.701s, episode steps: 154, steps per second:   6, episode reward:  2.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.006 [0.000, 4.000],  loss: 8.829905, mae: 7.458517, mean_q: 12.445983\n",
      "  2619/100000: episode: 31, duration: 9.286s, episode steps:  53, steps per second:   6, episode reward:  1.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.774 [0.000, 4.000],  loss: 7.942560, mae: 7.592268, mean_q: 12.830663\n",
      "  2924/100000: episode: 32, duration: 52.777s, episode steps: 305, steps per second:   6, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.813 [0.000, 4.000],  loss: 8.498072, mae: 8.067757, mean_q: 13.387344\n",
      "  3063/100000: episode: 33, duration: 24.144s, episode steps: 139, steps per second:   6, episode reward:  2.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.281 [0.000, 4.000],  loss: 7.334699, mae: 8.134121, mean_q: 13.532059\n",
      "  3176/100000: episode: 34, duration: 20.208s, episode steps: 113, steps per second:   6, episode reward:  2.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.044 [0.000, 4.000],  loss: 7.432857, mae: 8.144269, mean_q: 13.559519\n",
      "  3357/100000: episode: 35, duration: 31.483s, episode steps: 181, steps per second:   6, episode reward:  2.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.160 [0.000, 4.000],  loss: 7.223271, mae: 7.538028, mean_q: 12.508001\n",
      "  3492/100000: episode: 36, duration: 23.715s, episode steps: 135, steps per second:   6, episode reward:  1.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.052 [0.000, 4.000],  loss: 6.914060, mae: 7.681316, mean_q: 12.820647\n",
      "  3670/100000: episode: 37, duration: 30.756s, episode steps: 178, steps per second:   6, episode reward:  2.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.169 [0.000, 4.000],  loss: 6.673967, mae: 7.730054, mean_q: 12.868551\n",
      "  3720/100000: episode: 38, duration: 8.758s, episode steps:  50, steps per second:   6, episode reward:  1.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.820 [0.000, 4.000],  loss: 7.291780, mae: 7.603163, mean_q: 12.792758\n",
      "  3834/100000: episode: 39, duration: 19.787s, episode steps: 114, steps per second:   6, episode reward:  1.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.491 [0.000, 4.000],  loss: 7.226425, mae: 7.471457, mean_q: 12.538216\n",
      "  3876/100000: episode: 40, duration: 7.408s, episode steps:  42, steps per second:   6, episode reward:  1.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.262 [0.000, 4.000],  loss: 6.462114, mae: 7.237594, mean_q: 12.206482\n",
      "  3936/100000: episode: 41, duration: 10.436s, episode steps:  60, steps per second:   6, episode reward:  1.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.033 [0.000, 4.000],  loss: 7.335432, mae: 7.440042, mean_q: 12.475974\n",
      "  4261/100000: episode: 42, duration: 56.167s, episode steps: 325, steps per second:   6, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.945 [0.000, 4.000],  loss: 7.049063, mae: 7.787216, mean_q: 13.085681\n",
      "  4304/100000: episode: 43, duration: 7.533s, episode steps:  43, steps per second:   6, episode reward:  1.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.953 [0.000, 4.000],  loss: 7.007367, mae: 7.646980, mean_q: 12.863142\n",
      "  4366/100000: episode: 44, duration: 10.812s, episode steps:  62, steps per second:   6, episode reward:  2.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 2.129 [0.000, 4.000],  loss: 6.886261, mae: 7.468996, mean_q: 12.638330\n",
      "  4439/100000: episode: 45, duration: 12.733s, episode steps:  73, steps per second:   6, episode reward:  2.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.904 [0.000, 4.000],  loss: 7.314709, mae: 7.326760, mean_q: 12.318488\n",
      "  4477/100000: episode: 46, duration: 6.727s, episode steps:  38, steps per second:   6, episode reward:  1.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.184 [0.000, 4.000],  loss: 5.769866, mae: 6.410851, mean_q: 10.860641\n",
      "  4547/100000: episode: 47, duration: 12.233s, episode steps:  70, steps per second:   6, episode reward:  1.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.357 [0.000, 4.000],  loss: 6.853107, mae: 6.676452, mean_q: 11.281772\n",
      "  4638/100000: episode: 48, duration: 15.857s, episode steps:  91, steps per second:   6, episode reward:  2.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.857 [0.000, 4.000],  loss: 7.115392, mae: 6.751483, mean_q: 11.434894\n",
      "  4754/100000: episode: 49, duration: 20.167s, episode steps: 116, steps per second:   6, episode reward:  3.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.819 [0.000, 4.000],  loss: 6.389307, mae: 6.268631, mean_q: 10.545787\n",
      "  4909/100000: episode: 50, duration: 26.895s, episode steps: 155, steps per second:   6, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.103 [0.000, 4.000],  loss: 4.608665, mae: 5.571758, mean_q: 9.511188\n",
      "  5017/100000: episode: 51, duration: 18.696s, episode steps: 108, steps per second:   6, episode reward:  2.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.111 [0.000, 4.000],  loss: 6.245872, mae: 6.174810, mean_q: 10.481772\n",
      "  5071/100000: episode: 52, duration: 9.439s, episode steps:  54, steps per second:   6, episode reward:  1.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.611 [0.000, 4.000],  loss: 4.637706, mae: 5.154799, mean_q: 8.752486\n",
      "  5115/100000: episode: 53, duration: 7.754s, episode steps:  44, steps per second:   6, episode reward:  2.000, mean reward:  0.045 [ 0.000,  1.000], mean action: 2.000 [0.000, 4.000],  loss: 6.134599, mae: 5.378714, mean_q: 9.013963\n",
      "  5155/100000: episode: 54, duration: 7.062s, episode steps:  40, steps per second:   6, episode reward:  1.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.900 [0.000, 4.000],  loss: 5.927116, mae: 5.316398, mean_q: 8.930175\n",
      "  5262/100000: episode: 55, duration: 18.587s, episode steps: 107, steps per second:   6, episode reward:  1.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.841 [0.000, 4.000],  loss: 5.944898, mae: 5.107340, mean_q: 8.561021\n",
      "  5336/100000: episode: 56, duration: 12.949s, episode steps:  74, steps per second:   6, episode reward:  1.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.311 [0.000, 4.000],  loss: 7.074136, mae: 5.530681, mean_q: 9.278090\n",
      "  5418/100000: episode: 57, duration: 14.344s, episode steps:  82, steps per second:   6, episode reward:  2.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.110 [0.000, 4.000],  loss: 7.534171, mae: 5.904464, mean_q: 9.816607\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5460/100000: episode: 58, duration: 7.392s, episode steps:  42, steps per second:   6, episode reward:  1.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.238 [0.000, 4.000],  loss: 7.903409, mae: 5.894479, mean_q: 9.797974\n",
      "  5593/100000: episode: 59, duration: 23.082s, episode steps: 133, steps per second:   6, episode reward:  1.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.699 [0.000, 4.000],  loss: 6.681366, mae: 6.021786, mean_q: 10.012145\n",
      "  5666/100000: episode: 60, duration: 12.719s, episode steps:  73, steps per second:   6, episode reward:  1.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.192 [0.000, 4.000],  loss: 7.383044, mae: 6.896339, mean_q: 11.451265\n",
      "  5753/100000: episode: 61, duration: 15.168s, episode steps:  87, steps per second:   6, episode reward:  2.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.218 [0.000, 4.000],  loss: 7.390929, mae: 7.161745, mean_q: 11.892189\n",
      "  5906/100000: episode: 62, duration: 26.610s, episode steps: 153, steps per second:   6, episode reward:  3.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.359 [0.000, 4.000],  loss: 8.563564, mae: 7.350098, mean_q: 12.112873\n",
      "  5976/100000: episode: 63, duration: 12.270s, episode steps:  70, steps per second:   6, episode reward:  1.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.071 [0.000, 4.000],  loss: 7.063306, mae: 7.152902, mean_q: 11.769751\n",
      "  6079/100000: episode: 64, duration: 17.960s, episode steps: 103, steps per second:   6, episode reward:  2.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.903 [0.000, 4.000],  loss: 8.046658, mae: 7.381124, mean_q: 12.114176\n",
      "  6118/100000: episode: 65, duration: 6.902s, episode steps:  39, steps per second:   6, episode reward:  2.000, mean reward:  0.051 [ 0.000,  1.000], mean action: 1.769 [0.000, 4.000],  loss: 7.417771, mae: 6.889894, mean_q: 11.344262\n",
      "  6166/100000: episode: 66, duration: 8.458s, episode steps:  48, steps per second:   6, episode reward:  1.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.875 [0.000, 4.000],  loss: 8.588830, mae: 7.366695, mean_q: 12.109406\n",
      "  6203/100000: episode: 67, duration: 6.514s, episode steps:  37, steps per second:   6, episode reward:  1.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.135 [0.000, 4.000],  loss: 7.995928, mae: 7.285416, mean_q: 11.872128\n",
      "  6256/100000: episode: 68, duration: 9.323s, episode steps:  53, steps per second:   6, episode reward:  1.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.472 [0.000, 4.000],  loss: 7.750754, mae: 6.667037, mean_q: 10.917015\n",
      "  6296/100000: episode: 69, duration: 7.049s, episode steps:  40, steps per second:   6, episode reward:  2.000, mean reward:  0.050 [ 0.000,  1.000], mean action: 2.125 [0.000, 4.000],  loss: 7.340727, mae: 7.018250, mean_q: 11.528139\n",
      "  6374/100000: episode: 70, duration: 13.587s, episode steps:  78, steps per second:   6, episode reward:  2.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.013 [0.000, 4.000],  loss: 8.769478, mae: 6.660940, mean_q: 10.926210\n",
      "  6563/100000: episode: 71, duration: 32.663s, episode steps: 189, steps per second:   6, episode reward:  2.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.735 [0.000, 4.000],  loss: 7.548220, mae: 6.441035, mean_q: 10.666104\n",
      "  6602/100000: episode: 72, duration: 6.864s, episode steps:  39, steps per second:   6, episode reward:  2.000, mean reward:  0.051 [ 0.000,  1.000], mean action: 2.282 [0.000, 4.000],  loss: 6.279196, mae: 5.888989, mean_q: 9.796407\n",
      "  6648/100000: episode: 73, duration: 8.103s, episode steps:  46, steps per second:   6, episode reward:  1.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.109 [0.000, 4.000],  loss: 6.804579, mae: 5.885834, mean_q: 9.823380\n",
      "  6716/100000: episode: 74, duration: 11.895s, episode steps:  68, steps per second:   6, episode reward:  1.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.221 [0.000, 4.000],  loss: 6.203516, mae: 5.664285, mean_q: 9.526878\n",
      "  6884/100000: episode: 75, duration: 28.996s, episode steps: 168, steps per second:   6, episode reward:  2.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.911 [0.000, 4.000],  loss: 6.836947, mae: 5.749930, mean_q: 9.635832\n",
      "  6937/100000: episode: 76, duration: 9.269s, episode steps:  53, steps per second:   6, episode reward:  1.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.981 [0.000, 4.000],  loss: 7.737041, mae: 6.482206, mean_q: 10.836171\n",
      "  7115/100000: episode: 77, duration: 30.816s, episode steps: 178, steps per second:   6, episode reward:  2.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.163 [0.000, 4.000],  loss: 7.166102, mae: 6.152212, mean_q: 10.237489\n",
      "  7237/100000: episode: 78, duration: 21.057s, episode steps: 122, steps per second:   6, episode reward:  2.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.967 [0.000, 4.000],  loss: 6.915556, mae: 6.650681, mean_q: 10.992350\n",
      "  7289/100000: episode: 79, duration: 9.101s, episode steps:  52, steps per second:   6, episode reward:  1.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.846 [0.000, 4.000],  loss: 6.705294, mae: 6.820554, mean_q: 11.220986\n",
      "  7333/100000: episode: 80, duration: 7.712s, episode steps:  44, steps per second:   6, episode reward:  1.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.295 [0.000, 4.000],  loss: 7.027039, mae: 6.933898, mean_q: 11.391395\n",
      "  7379/100000: episode: 81, duration: 8.087s, episode steps:  46, steps per second:   6, episode reward:  1.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.783 [0.000, 4.000],  loss: 7.187328, mae: 7.080045, mean_q: 11.582705\n",
      "  7467/100000: episode: 82, duration: 15.280s, episode steps:  88, steps per second:   6, episode reward:  1.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.023 [0.000, 4.000],  loss: 8.628874, mae: 7.037822, mean_q: 11.487182\n",
      "  7503/100000: episode: 83, duration: 6.396s, episode steps:  36, steps per second:   6, episode reward:  1.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.889 [0.000, 4.000],  loss: 8.113617, mae: 6.453154, mean_q: 10.575333\n",
      "  7576/100000: episode: 84, duration: 12.706s, episode steps:  73, steps per second:   6, episode reward:  2.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.712 [0.000, 4.000],  loss: 8.311326, mae: 6.633282, mean_q: 10.869654\n",
      "  7892/100000: episode: 85, duration: 54.656s, episode steps: 316, steps per second:   6, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.187 [0.000, 4.000],  loss: 8.087506, mae: 6.664855, mean_q: 10.978893\n",
      "  7960/100000: episode: 86, duration: 11.848s, episode steps:  68, steps per second:   6, episode reward:  1.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.912 [0.000, 4.000],  loss: 7.416516, mae: 6.960690, mean_q: 11.470201\n",
      "  8010/100000: episode: 87, duration: 8.744s, episode steps:  50, steps per second:   6, episode reward:  1.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.700 [0.000, 4.000],  loss: 8.058235, mae: 7.352184, mean_q: 12.170161\n",
      "  8077/100000: episode: 88, duration: 11.699s, episode steps:  67, steps per second:   6, episode reward:  1.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.940 [0.000, 4.000],  loss: 7.657090, mae: 6.819259, mean_q: 11.363400\n",
      "  8176/100000: episode: 89, duration: 17.209s, episode steps:  99, steps per second:   6, episode reward:  1.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.333 [0.000, 4.000],  loss: 8.344417, mae: 6.824293, mean_q: 11.469166\n",
      "  8311/100000: episode: 90, duration: 23.466s, episode steps: 135, steps per second:   6, episode reward:  2.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.985 [0.000, 4.000],  loss: 9.826071, mae: 7.273822, mean_q: 12.171584\n",
      "  8586/100000: episode: 91, duration: 47.424s, episode steps: 275, steps per second:   6, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.978 [0.000, 4.000],  loss: 9.236491, mae: 7.663391, mean_q: 12.822573\n",
      "  8651/100000: episode: 92, duration: 11.320s, episode steps:  65, steps per second:   6, episode reward:  2.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 1.938 [0.000, 4.000],  loss: 8.127878, mae: 7.635684, mean_q: 12.767977\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  8696/100000: episode: 93, duration: 7.880s, episode steps:  45, steps per second:   6, episode reward:  1.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.756 [0.000, 4.000],  loss: 8.717838, mae: 8.097464, mean_q: 13.531757\n",
      "  8746/100000: episode: 94, duration: 8.719s, episode steps:  50, steps per second:   6, episode reward:  1.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.520 [0.000, 4.000],  loss: 9.825883, mae: 7.997077, mean_q: 13.258331\n",
      "  8793/100000: episode: 95, duration: 8.233s, episode steps:  47, steps per second:   6, episode reward:  1.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.404 [0.000, 4.000],  loss: 7.921573, mae: 7.792444, mean_q: 12.966427\n",
      "  8866/100000: episode: 96, duration: 12.736s, episode steps:  73, steps per second:   6, episode reward:  2.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.192 [0.000, 4.000],  loss: 8.771295, mae: 7.467428, mean_q: 12.395802\n",
      "  8942/100000: episode: 97, duration: 13.263s, episode steps:  76, steps per second:   6, episode reward:  1.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.842 [0.000, 4.000],  loss: 8.304768, mae: 7.046740, mean_q: 11.695872\n",
      "  9153/100000: episode: 98, duration: 36.429s, episode steps: 211, steps per second:   6, episode reward:  3.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.645 [0.000, 4.000],  loss: 8.258714, mae: 7.657587, mean_q: 12.640565\n",
      "  9203/100000: episode: 99, duration: 8.729s, episode steps:  50, steps per second:   6, episode reward:  1.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.820 [0.000, 4.000],  loss: 7.496205, mae: 7.684256, mean_q: 12.582407\n",
      "  9301/100000: episode: 100, duration: 17.003s, episode steps:  98, steps per second:   6, episode reward:  2.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.867 [0.000, 4.000],  loss: 7.140488, mae: 7.615832, mean_q: 12.529842\n",
      "  9372/100000: episode: 101, duration: 12.357s, episode steps:  71, steps per second:   6, episode reward:  2.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.915 [0.000, 4.000],  loss: 6.793466, mae: 7.812218, mean_q: 12.923106\n",
      "  9488/100000: episode: 102, duration: 20.100s, episode steps: 116, steps per second:   6, episode reward:  1.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.690 [0.000, 4.000],  loss: 6.728611, mae: 7.253726, mean_q: 11.964632\n",
      "  9529/100000: episode: 103, duration: 7.209s, episode steps:  41, steps per second:   6, episode reward:  1.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.098 [0.000, 4.000],  loss: 7.538498, mae: 7.338984, mean_q: 12.017313\n",
      "  9596/100000: episode: 104, duration: 11.773s, episode steps:  67, steps per second:   6, episode reward:  1.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.687 [0.000, 4.000],  loss: 7.208365, mae: 7.350372, mean_q: 12.047220\n",
      "  9643/100000: episode: 105, duration: 8.267s, episode steps:  47, steps per second:   6, episode reward:  1.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.681 [0.000, 4.000],  loss: 7.945352, mae: 7.762632, mean_q: 12.656545\n",
      "  9827/100000: episode: 106, duration: 31.764s, episode steps: 184, steps per second:   6, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.935 [0.000, 4.000],  loss: 7.058072, mae: 7.821836, mean_q: 12.855639\n",
      "  9895/100000: episode: 107, duration: 11.874s, episode steps:  68, steps per second:   6, episode reward:  2.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 1.971 [0.000, 4.000],  loss: 6.659917, mae: 7.941250, mean_q: 13.157407\n",
      "  9949/100000: episode: 108, duration: 9.459s, episode steps:  54, steps per second:   6, episode reward:  1.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.019 [0.000, 4.000],  loss: 8.318333, mae: 8.028161, mean_q: 13.129779\n",
      " 10016/100000: episode: 109, duration: 11.680s, episode steps:  67, steps per second:   6, episode reward:  1.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.821 [0.000, 4.000],  loss: 8.505281, mae: 7.845039, mean_q: 12.837477\n",
      " 10066/100000: episode: 110, duration: 8.859s, episode steps:  50, steps per second:   6, episode reward:  1.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.500 [0.000, 4.000],  loss: 6.692812, mae: 7.709381, mean_q: 12.603132\n",
      " 10109/100000: episode: 111, duration: 7.613s, episode steps:  43, steps per second:   6, episode reward:  1.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.721 [0.000, 4.000],  loss: 7.298138, mae: 7.351537, mean_q: 11.953835\n",
      " 10180/100000: episode: 112, duration: 12.384s, episode steps:  71, steps per second:   6, episode reward:  1.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.845 [0.000, 4.000],  loss: 7.019900, mae: 7.171249, mean_q: 11.724940\n",
      " 10305/100000: episode: 113, duration: 21.672s, episode steps: 125, steps per second:   6, episode reward:  1.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.000 [0.000, 4.000],  loss: 6.771530, mae: 6.612838, mean_q: 10.871244\n",
      " 10352/100000: episode: 114, duration: 8.264s, episode steps:  47, steps per second:   6, episode reward:  1.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.362 [0.000, 4.000],  loss: 6.315008, mae: 6.732706, mean_q: 11.049877\n",
      " 10408/100000: episode: 115, duration: 9.842s, episode steps:  56, steps per second:   6, episode reward:  2.000, mean reward:  0.036 [ 0.000,  1.000], mean action: 2.500 [0.000, 4.000],  loss: 6.892998, mae: 6.482513, mean_q: 10.581151\n",
      " 10487/100000: episode: 116, duration: 13.816s, episode steps:  79, steps per second:   6, episode reward:  1.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.987 [0.000, 4.000],  loss: 7.262037, mae: 6.541609, mean_q: 10.769547\n",
      " 10583/100000: episode: 117, duration: 16.691s, episode steps:  96, steps per second:   6, episode reward:  1.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.250 [0.000, 4.000],  loss: 6.366781, mae: 6.577414, mean_q: 10.863083\n",
      " 10635/100000: episode: 118, duration: 9.108s, episode steps:  52, steps per second:   6, episode reward:  1.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.404 [0.000, 4.000],  loss: 8.429072, mae: 6.795846, mean_q: 11.221762\n",
      " 10736/100000: episode: 119, duration: 17.525s, episode steps: 101, steps per second:   6, episode reward:  2.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.723 [0.000, 4.000],  loss: 8.368633, mae: 6.777373, mean_q: 11.143124\n",
      " 10782/100000: episode: 120, duration: 8.095s, episode steps:  46, steps per second:   6, episode reward:  2.000, mean reward:  0.043 [ 0.000,  1.000], mean action: 2.065 [0.000, 4.000],  loss: 7.080514, mae: 6.026321, mean_q: 9.919571\n",
      " 10839/100000: episode: 121, duration: 9.976s, episode steps:  57, steps per second:   6, episode reward:  2.000, mean reward:  0.035 [ 0.000,  1.000], mean action: 2.105 [0.000, 4.000],  loss: 7.695077, mae: 6.143371, mean_q: 10.077269\n",
      " 10895/100000: episode: 122, duration: 9.838s, episode steps:  56, steps per second:   6, episode reward:  1.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.857 [0.000, 4.000],  loss: 7.243961, mae: 5.952098, mean_q: 9.802922\n",
      " 11029/100000: episode: 123, duration: 23.333s, episode steps: 134, steps per second:   6, episode reward:  3.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.060 [0.000, 4.000],  loss: 6.763920, mae: 5.866735, mean_q: 9.685759\n",
      " 11111/100000: episode: 124, duration: 14.288s, episode steps:  82, steps per second:   6, episode reward:  2.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.695 [0.000, 4.000],  loss: 6.472999, mae: 5.920943, mean_q: 9.749714\n",
      " 11292/100000: episode: 125, duration: 31.352s, episode steps: 181, steps per second:   6, episode reward:  2.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.110 [0.000, 4.000],  loss: 6.396909, mae: 6.113927, mean_q: 10.116136\n",
      " 11331/100000: episode: 126, duration: 6.875s, episode steps:  39, steps per second:   6, episode reward:  1.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.128 [0.000, 4.000],  loss: 7.239429, mae: 6.372378, mean_q: 10.501290\n",
      " 11382/100000: episode: 127, duration: 8.950s, episode steps:  51, steps per second:   6, episode reward:  1.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.157 [0.000, 4.000],  loss: 6.333822, mae: 6.557985, mean_q: 10.881334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 11430/100000: episode: 128, duration: 8.446s, episode steps:  48, steps per second:   6, episode reward:  1.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.333 [0.000, 4.000],  loss: 6.836053, mae: 6.303256, mean_q: 10.422276\n",
      " 11522/100000: episode: 129, duration: 16.012s, episode steps:  92, steps per second:   6, episode reward:  1.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.870 [0.000, 4.000],  loss: 6.444311, mae: 6.280526, mean_q: 10.366632\n",
      " 11628/100000: episode: 130, duration: 18.438s, episode steps: 106, steps per second:   6, episode reward:  1.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.302 [0.000, 4.000],  loss: 6.323218, mae: 6.176347, mean_q: 10.176941\n",
      " 11804/100000: episode: 131, duration: 30.514s, episode steps: 176, steps per second:   6, episode reward:  2.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.972 [0.000, 4.000],  loss: 6.486554, mae: 6.904442, mean_q: 11.286271\n",
      " 11903/100000: episode: 132, duration: 17.224s, episode steps:  99, steps per second:   6, episode reward:  2.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.929 [0.000, 4.000],  loss: 6.503317, mae: 7.648353, mean_q: 12.440585\n",
      " 11977/100000: episode: 133, duration: 12.860s, episode steps:  74, steps per second:   6, episode reward:  2.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.365 [0.000, 4.000],  loss: 6.547250, mae: 7.359656, mean_q: 11.987827\n",
      " 12063/100000: episode: 134, duration: 14.975s, episode steps:  86, steps per second:   6, episode reward:  1.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.244 [0.000, 4.000],  loss: 7.022704, mae: 7.405931, mean_q: 12.072108\n",
      " 12300/100000: episode: 135, duration: 40.961s, episode steps: 237, steps per second:   6, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.165 [0.000, 4.000],  loss: 7.431203, mae: 7.877930, mean_q: 12.935507\n",
      " 12434/100000: episode: 136, duration: 23.183s, episode steps: 134, steps per second:   6, episode reward:  2.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.679 [0.000, 4.000],  loss: 6.916187, mae: 7.767343, mean_q: 12.699653\n",
      " 12492/100000: episode: 137, duration: 10.132s, episode steps:  58, steps per second:   6, episode reward:  2.000, mean reward:  0.034 [ 0.000,  1.000], mean action: 1.948 [0.000, 4.000],  loss: 6.658253, mae: 8.131164, mean_q: 13.242279\n",
      " 12543/100000: episode: 138, duration: 8.922s, episode steps:  51, steps per second:   6, episode reward:  1.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.039 [0.000, 4.000],  loss: 7.686807, mae: 8.050067, mean_q: 13.075127\n",
      " 12590/100000: episode: 139, duration: 8.199s, episode steps:  47, steps per second:   6, episode reward:  1.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.234 [0.000, 4.000],  loss: 6.621296, mae: 7.785398, mean_q: 12.656983\n",
      " 12697/100000: episode: 140, duration: 18.549s, episode steps: 107, steps per second:   6, episode reward:  3.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.879 [0.000, 4.000],  loss: 6.379168, mae: 7.451726, mean_q: 12.119884\n",
      " 12754/100000: episode: 141, duration: 9.976s, episode steps:  57, steps per second:   6, episode reward:  1.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.000 [0.000, 4.000],  loss: 6.495386, mae: 7.049225, mean_q: 11.540021\n",
      " 12819/100000: episode: 142, duration: 11.343s, episode steps:  65, steps per second:   6, episode reward:  1.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.677 [0.000, 4.000],  loss: 5.405954, mae: 6.388431, mean_q: 10.516451\n",
      " 13225/100000: episode: 143, duration: 69.841s, episode steps: 406, steps per second:   6, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.091 [0.000, 4.000],  loss: 5.744357, mae: 6.791184, mean_q: 11.139716\n",
      " 13270/100000: episode: 144, duration: 7.870s, episode steps:  45, steps per second:   6, episode reward:  2.000, mean reward:  0.044 [ 0.000,  1.000], mean action: 1.978 [0.000, 4.000],  loss: 5.601977, mae: 7.251688, mean_q: 11.977963\n",
      " 13393/100000: episode: 145, duration: 21.276s, episode steps: 123, steps per second:   6, episode reward:  1.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.276 [0.000, 4.000],  loss: 5.284849, mae: 7.048536, mean_q: 11.662044\n",
      " 13439/100000: episode: 146, duration: 8.052s, episode steps:  46, steps per second:   6, episode reward:  1.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.174 [0.000, 4.000],  loss: 5.102568, mae: 6.639862, mean_q: 11.084485\n",
      " 13554/100000: episode: 147, duration: 19.902s, episode steps: 115, steps per second:   6, episode reward:  2.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.365 [0.000, 4.000],  loss: 5.501687, mae: 6.815720, mean_q: 11.333927\n",
      " 13587/100000: episode: 148, duration: 5.801s, episode steps:  33, steps per second:   6, episode reward:  1.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.182 [0.000, 4.000],  loss: 5.416317, mae: 7.287980, mean_q: 12.056767\n",
      " 13675/100000: episode: 149, duration: 15.312s, episode steps:  88, steps per second:   6, episode reward:  1.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.068 [0.000, 4.000],  loss: 6.530615, mae: 7.169948, mean_q: 11.886033\n",
      " 13722/100000: episode: 150, duration: 8.249s, episode steps:  47, steps per second:   6, episode reward:  1.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.702 [0.000, 4.000],  loss: 7.109975, mae: 7.763395, mean_q: 12.777832\n",
      " 13782/100000: episode: 151, duration: 10.496s, episode steps:  60, steps per second:   6, episode reward:  2.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 2.083 [0.000, 4.000],  loss: 6.841833, mae: 7.473024, mean_q: 12.369069\n",
      " 13846/100000: episode: 152, duration: 11.140s, episode steps:  64, steps per second:   6, episode reward:  1.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.359 [0.000, 4.000],  loss: 6.294336, mae: 7.382957, mean_q: 12.331196\n",
      " 13895/100000: episode: 153, duration: 8.581s, episode steps:  49, steps per second:   6, episode reward:  1.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.020 [0.000, 4.000],  loss: 7.475711, mae: 7.550622, mean_q: 12.558562\n",
      " 14019/100000: episode: 154, duration: 21.508s, episode steps: 124, steps per second:   6, episode reward:  1.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.169 [0.000, 4.000],  loss: 6.761395, mae: 7.082716, mean_q: 11.778451\n",
      " 14074/100000: episode: 155, duration: 9.635s, episode steps:  55, steps per second:   6, episode reward:  1.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.764 [0.000, 4.000],  loss: 6.943424, mae: 6.420126, mean_q: 10.669662\n",
      " 14168/100000: episode: 156, duration: 16.363s, episode steps:  94, steps per second:   6, episode reward:  1.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.532 [0.000, 4.000],  loss: 6.831699, mae: 6.421744, mean_q: 10.629385\n",
      " 14201/100000: episode: 157, duration: 5.854s, episode steps:  33, steps per second:   6, episode reward:  1.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 1.909 [0.000, 4.000],  loss: 7.660933, mae: 6.601672, mean_q: 10.990666\n",
      " 14269/100000: episode: 158, duration: 11.879s, episode steps:  68, steps per second:   6, episode reward:  1.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.015 [0.000, 4.000],  loss: 8.001413, mae: 6.550200, mean_q: 10.765863\n",
      " 14317/100000: episode: 159, duration: 8.521s, episode steps:  48, steps per second:   6, episode reward:  1.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.812 [0.000, 4.000],  loss: 6.695335, mae: 6.394849, mean_q: 10.533889\n",
      " 14372/100000: episode: 160, duration: 9.676s, episode steps:  55, steps per second:   6, episode reward:  2.000, mean reward:  0.036 [ 0.000,  1.000], mean action: 2.400 [0.000, 4.000],  loss: 6.699952, mae: 6.200132, mean_q: 10.181786\n",
      " 14495/100000: episode: 161, duration: 21.443s, episode steps: 123, steps per second:   6, episode reward:  1.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.293 [0.000, 4.000],  loss: 5.879592, mae: 6.082101, mean_q: 10.001995\n",
      " 14553/100000: episode: 162, duration: 10.178s, episode steps:  58, steps per second:   6, episode reward:  1.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.052 [0.000, 4.000],  loss: 6.544912, mae: 6.199696, mean_q: 10.158965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 14613/100000: episode: 163, duration: 10.514s, episode steps:  60, steps per second:   6, episode reward:  2.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 1.817 [0.000, 4.000],  loss: 5.680422, mae: 6.066197, mean_q: 9.998399\n",
      " 14955/100000: episode: 164, duration: 59.039s, episode steps: 342, steps per second:   6, episode reward:  1.000, mean reward:  0.003 [ 0.000,  1.000], mean action: 2.386 [0.000, 4.000],  loss: 5.961192, mae: 6.457204, mean_q: 10.731126\n",
      " 14994/100000: episode: 165, duration: 6.906s, episode steps:  39, steps per second:   6, episode reward:  1.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.051 [0.000, 4.000],  loss: 6.413867, mae: 7.509166, mean_q: 12.546541\n",
      " 15041/100000: episode: 166, duration: 8.246s, episode steps:  47, steps per second:   6, episode reward:  2.000, mean reward:  0.043 [ 0.000,  1.000], mean action: 2.000 [0.000, 4.000],  loss: 6.457944, mae: 7.225306, mean_q: 12.093929\n",
      " 15168/100000: episode: 167, duration: 22.029s, episode steps: 127, steps per second:   6, episode reward:  2.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.339 [0.000, 4.000],  loss: 6.000657, mae: 6.847268, mean_q: 11.515151\n",
      " 15215/100000: episode: 168, duration: 8.255s, episode steps:  47, steps per second:   6, episode reward:  1.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.085 [0.000, 4.000],  loss: 6.344201, mae: 7.145749, mean_q: 12.097136\n",
      " 15280/100000: episode: 169, duration: 11.310s, episode steps:  65, steps per second:   6, episode reward:  2.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 1.954 [0.000, 4.000],  loss: 6.144120, mae: 7.209005, mean_q: 12.145917\n",
      " 15338/100000: episode: 170, duration: 10.131s, episode steps:  58, steps per second:   6, episode reward:  2.000, mean reward:  0.034 [ 0.000,  1.000], mean action: 1.862 [0.000, 4.000],  loss: 5.855525, mae: 6.730146, mean_q: 11.393337\n",
      " 15411/100000: episode: 171, duration: 12.715s, episode steps:  73, steps per second:   6, episode reward:  1.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.164 [0.000, 4.000],  loss: 6.827889, mae: 7.184681, mean_q: 12.120659\n",
      " 15506/100000: episode: 172, duration: 16.533s, episode steps:  95, steps per second:   6, episode reward:  2.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.063 [0.000, 4.000],  loss: 6.210343, mae: 7.329268, mean_q: 12.417822\n",
      " 15555/100000: episode: 173, duration: 8.580s, episode steps:  49, steps per second:   6, episode reward:  1.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.776 [0.000, 4.000],  loss: 6.370550, mae: 7.240792, mean_q: 12.212729\n",
      " 15650/100000: episode: 174, duration: 16.507s, episode steps:  95, steps per second:   6, episode reward:  2.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.042 [0.000, 4.000],  loss: 5.978573, mae: 6.903207, mean_q: 11.645812\n",
      " 15770/100000: episode: 175, duration: 20.831s, episode steps: 120, steps per second:   6, episode reward:  3.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.983 [0.000, 4.000],  loss: 5.470109, mae: 6.328347, mean_q: 10.719380\n",
      " 15829/100000: episode: 176, duration: 10.323s, episode steps:  59, steps per second:   6, episode reward:  1.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.322 [0.000, 4.000],  loss: 5.841322, mae: 6.023919, mean_q: 10.011092\n",
      " 15923/100000: episode: 177, duration: 16.364s, episode steps:  94, steps per second:   6, episode reward:  1.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.213 [0.000, 4.000],  loss: 6.396188, mae: 5.762280, mean_q: 9.622504\n",
      " 15958/100000: episode: 178, duration: 6.203s, episode steps:  35, steps per second:   6, episode reward:  1.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 1.971 [0.000, 4.000],  loss: 5.934381, mae: 5.439266, mean_q: 9.094252\n",
      " 16014/100000: episode: 179, duration: 9.812s, episode steps:  56, steps per second:   6, episode reward:  1.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.875 [0.000, 4.000],  loss: 6.154231, mae: 5.528645, mean_q: 9.168641\n",
      " 16201/100000: episode: 180, duration: 32.425s, episode steps: 187, steps per second:   6, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 2.064 [0.000, 4.000],  loss: 6.047145, mae: 5.561760, mean_q: 9.263218\n",
      " 16247/100000: episode: 181, duration: 8.103s, episode steps:  46, steps per second:   6, episode reward:  1.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.717 [0.000, 4.000],  loss: 5.409892, mae: 5.719841, mean_q: 9.515410\n",
      " 16506/100000: episode: 182, duration: 44.709s, episode steps: 259, steps per second:   6, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.270 [0.000, 4.000],  loss: 5.776776, mae: 6.191023, mean_q: 10.313360\n",
      " 16543/100000: episode: 183, duration: 6.522s, episode steps:  37, steps per second:   6, episode reward:  1.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.486 [0.000, 4.000],  loss: 5.299690, mae: 6.468055, mean_q: 10.881142\n",
      " 16594/100000: episode: 184, duration: 8.923s, episode steps:  51, steps per second:   6, episode reward:  1.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.961 [0.000, 4.000],  loss: 6.002020, mae: 6.824264, mean_q: 11.399607\n",
      " 16650/100000: episode: 185, duration: 9.777s, episode steps:  56, steps per second:   6, episode reward:  2.000, mean reward:  0.036 [ 0.000,  1.000], mean action: 1.893 [0.000, 4.000],  loss: 5.515667, mae: 6.885812, mean_q: 11.475817\n",
      " 16954/100000: episode: 186, duration: 52.312s, episode steps: 304, steps per second:   6, episode reward:  4.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.171 [0.000, 4.000],  loss: 5.999140, mae: 6.950717, mean_q: 11.588787\n",
      " 17184/100000: episode: 187, duration: 39.505s, episode steps: 230, steps per second:   6, episode reward:  3.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.278 [0.000, 4.000],  loss: 6.090910, mae: 7.350411, mean_q: 12.155214\n",
      " 17236/100000: episode: 188, duration: 9.069s, episode steps:  52, steps per second:   6, episode reward:  2.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 2.135 [0.000, 4.000],  loss: 5.923316, mae: 7.434842, mean_q: 12.191626\n",
      " 17294/100000: episode: 189, duration: 10.118s, episode steps:  58, steps per second:   6, episode reward:  1.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.931 [0.000, 4.000],  loss: 5.334595, mae: 7.156234, mean_q: 11.800570\n",
      " 17340/100000: episode: 190, duration: 8.080s, episode steps:  46, steps per second:   6, episode reward:  1.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.348 [0.000, 4.000],  loss: 5.812637, mae: 7.172206, mean_q: 11.785074\n",
      " 17472/100000: episode: 191, duration: 22.743s, episode steps: 132, steps per second:   6, episode reward:  1.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.985 [0.000, 4.000],  loss: 4.901082, mae: 6.528070, mean_q: 10.672141\n",
      " 17519/100000: episode: 192, duration: 8.225s, episode steps:  47, steps per second:   6, episode reward:  1.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.213 [0.000, 4.000],  loss: 4.528431, mae: 6.062208, mean_q: 9.916588\n",
      " 17555/100000: episode: 193, duration: 6.352s, episode steps:  36, steps per second:   6, episode reward:  1.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.500 [0.000, 4.000],  loss: 4.665480, mae: 6.165561, mean_q: 10.106049\n",
      " 17716/100000: episode: 194, duration: 27.819s, episode steps: 161, steps per second:   6, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.373 [0.000, 4.000],  loss: 5.439983, mae: 6.480257, mean_q: 10.595914\n",
      " 17762/100000: episode: 195, duration: 8.103s, episode steps:  46, steps per second:   6, episode reward:  1.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.000 [0.000, 4.000],  loss: 5.996155, mae: 7.010871, mean_q: 11.486670\n",
      " 17825/100000: episode: 196, duration: 10.989s, episode steps:  63, steps per second:   6, episode reward:  1.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.952 [0.000, 4.000],  loss: 5.648274, mae: 7.116170, mean_q: 11.703407\n",
      " 17866/100000: episode: 197, duration: 7.239s, episode steps:  41, steps per second:   6, episode reward:  1.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.390 [0.000, 4.000],  loss: 6.898257, mae: 7.372680, mean_q: 12.058684\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 18094/100000: episode: 198, duration: 39.399s, episode steps: 228, steps per second:   6, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.254 [0.000, 4.000],  loss: 6.401051, mae: 7.526092, mean_q: 12.382620\n",
      " 18167/100000: episode: 199, duration: 12.692s, episode steps:  73, steps per second:   6, episode reward:  2.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.041 [0.000, 4.000],  loss: 6.650132, mae: 7.414240, mean_q: 12.319550\n",
      " 18255/100000: episode: 200, duration: 15.288s, episode steps:  88, steps per second:   6, episode reward:  1.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.000 [0.000, 4.000],  loss: 7.156430, mae: 7.773210, mean_q: 12.841800\n",
      " 18310/100000: episode: 201, duration: 9.603s, episode steps:  55, steps per second:   6, episode reward:  1.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.055 [0.000, 4.000],  loss: 7.967523, mae: 8.030756, mean_q: 13.224784\n",
      " 18508/100000: episode: 202, duration: 34.207s, episode steps: 198, steps per second:   6, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.177 [0.000, 4.000],  loss: 8.208156, mae: 8.660337, mean_q: 14.199986\n",
      " 18604/100000: episode: 203, duration: 16.682s, episode steps:  96, steps per second:   6, episode reward:  1.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.260 [0.000, 4.000],  loss: 8.250100, mae: 8.969994, mean_q: 14.775318\n",
      " 18650/100000: episode: 204, duration: 8.088s, episode steps:  46, steps per second:   6, episode reward:  1.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.130 [0.000, 4.000],  loss: 8.246748, mae: 8.620480, mean_q: 14.303239\n",
      " 18702/100000: episode: 205, duration: 9.101s, episode steps:  52, steps per second:   6, episode reward:  2.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 2.250 [0.000, 4.000],  loss: 8.230425, mae: 8.514792, mean_q: 13.960264\n",
      " 18810/100000: episode: 206, duration: 18.797s, episode steps: 108, steps per second:   6, episode reward:  2.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.333 [0.000, 4.000],  loss: 7.665545, mae: 8.435320, mean_q: 13.865044\n",
      " 18955/100000: episode: 207, duration: 25.027s, episode steps: 145, steps per second:   6, episode reward:  1.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.710 [0.000, 4.000],  loss: 8.132763, mae: 8.783429, mean_q: 14.443558\n",
      " 19012/100000: episode: 208, duration: 9.973s, episode steps:  57, steps per second:   6, episode reward:  1.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.772 [0.000, 4.000],  loss: 7.528792, mae: 8.743675, mean_q: 14.372133\n",
      " 19089/100000: episode: 209, duration: 13.383s, episode steps:  77, steps per second:   6, episode reward:  1.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.052 [0.000, 4.000],  loss: 6.812933, mae: 8.452326, mean_q: 13.900354\n",
      " 19150/100000: episode: 210, duration: 10.610s, episode steps:  61, steps per second:   6, episode reward:  1.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.279 [0.000, 4.000],  loss: 7.143808, mae: 7.880021, mean_q: 12.912442\n",
      " 19215/100000: episode: 211, duration: 11.310s, episode steps:  65, steps per second:   6, episode reward:  1.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.062 [0.000, 4.000],  loss: 6.516420, mae: 7.943780, mean_q: 12.981604\n",
      " 19267/100000: episode: 212, duration: 9.119s, episode steps:  52, steps per second:   6, episode reward:  1.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.135 [0.000, 4.000],  loss: 6.588506, mae: 8.117348, mean_q: 13.262794\n",
      " 19325/100000: episode: 213, duration: 10.130s, episode steps:  58, steps per second:   6, episode reward:  1.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.879 [0.000, 4.000],  loss: 6.043081, mae: 7.622865, mean_q: 12.566758\n",
      " 19424/100000: episode: 214, duration: 17.179s, episode steps:  99, steps per second:   6, episode reward:  1.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.788 [0.000, 4.000],  loss: 6.569935, mae: 7.494419, mean_q: 12.354816\n",
      " 19543/100000: episode: 215, duration: 20.629s, episode steps: 119, steps per second:   6, episode reward:  2.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.067 [0.000, 4.000],  loss: 5.560057, mae: 6.896035, mean_q: 11.335355\n",
      " 19625/100000: episode: 216, duration: 14.227s, episode steps:  82, steps per second:   6, episode reward:  2.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.988 [0.000, 4.000],  loss: 5.201385, mae: 6.290596, mean_q: 10.332470\n",
      " 19719/100000: episode: 217, duration: 16.345s, episode steps:  94, steps per second:   6, episode reward:  1.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.298 [0.000, 4.000],  loss: 5.312223, mae: 6.384939, mean_q: 10.379594\n",
      " 19807/100000: episode: 218, duration: 15.273s, episode steps:  88, steps per second:   6, episode reward:  2.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.205 [0.000, 4.000],  loss: 5.598167, mae: 6.420256, mean_q: 10.451591\n",
      " 19869/100000: episode: 219, duration: 10.830s, episode steps:  62, steps per second:   6, episode reward:  1.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.661 [0.000, 4.000],  loss: 5.229282, mae: 6.269549, mean_q: 10.222803\n",
      " 19908/100000: episode: 220, duration: 6.863s, episode steps:  39, steps per second:   6, episode reward:  1.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.000 [0.000, 4.000],  loss: 4.929712, mae: 5.801283, mean_q: 9.449319\n",
      " 19966/100000: episode: 221, duration: 10.142s, episode steps:  58, steps per second:   6, episode reward:  1.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.069 [0.000, 4.000],  loss: 4.970277, mae: 5.603269, mean_q: 9.220768\n",
      " 20020/100000: episode: 222, duration: 9.445s, episode steps:  54, steps per second:   6, episode reward:  2.000, mean reward:  0.037 [ 0.000,  1.000], mean action: 1.741 [0.000, 4.000],  loss: 5.275693, mae: 5.527227, mean_q: 9.023441\n",
      " 20082/100000: episode: 223, duration: 10.807s, episode steps:  62, steps per second:   6, episode reward:  1.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.145 [0.000, 4.000],  loss: 5.225372, mae: 5.653050, mean_q: 9.252798\n",
      " 20138/100000: episode: 224, duration: 9.789s, episode steps:  56, steps per second:   6, episode reward:  2.000, mean reward:  0.036 [ 0.000,  1.000], mean action: 1.946 [0.000, 4.000],  loss: 5.153835, mae: 5.794570, mean_q: 9.534277\n",
      " 20202/100000: episode: 225, duration: 11.147s, episode steps:  64, steps per second:   6, episode reward:  1.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.594 [0.000, 4.000],  loss: 4.723348, mae: 5.727903, mean_q: 9.424035\n",
      " 20273/100000: episode: 226, duration: 12.377s, episode steps:  71, steps per second:   6, episode reward:  2.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.070 [0.000, 4.000],  loss: 5.240155, mae: 5.492470, mean_q: 9.115187\n",
      " 20320/100000: episode: 227, duration: 8.252s, episode steps:  47, steps per second:   6, episode reward:  1.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.787 [0.000, 4.000],  loss: 4.879575, mae: 5.655449, mean_q: 9.334276\n",
      " 20390/100000: episode: 228, duration: 12.172s, episode steps:  70, steps per second:   6, episode reward:  2.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 1.843 [0.000, 4.000],  loss: 4.743674, mae: 5.511883, mean_q: 9.071823\n",
      " 20434/100000: episode: 229, duration: 7.758s, episode steps:  44, steps per second:   6, episode reward:  1.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.114 [0.000, 4.000],  loss: 4.370227, mae: 5.013176, mean_q: 8.325764\n",
      " 20507/100000: episode: 230, duration: 12.716s, episode steps:  73, steps per second:   6, episode reward:  1.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.205 [0.000, 4.000],  loss: 4.502643, mae: 5.035185, mean_q: 8.375422\n",
      " 20596/100000: episode: 231, duration: 15.460s, episode steps:  89, steps per second:   6, episode reward:  2.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.124 [0.000, 4.000],  loss: 5.161529, mae: 5.465501, mean_q: 9.075120\n",
      " 20695/100000: episode: 232, duration: 17.165s, episode steps:  99, steps per second:   6, episode reward:  1.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.091 [0.000, 4.000],  loss: 5.515898, mae: 5.702140, mean_q: 9.428764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 20733/100000: episode: 233, duration: 6.711s, episode steps:  38, steps per second:   6, episode reward:  2.000, mean reward:  0.053 [ 0.000,  1.000], mean action: 1.816 [0.000, 4.000],  loss: 4.779156, mae: 5.372038, mean_q: 8.931964\n",
      " 20856/100000: episode: 234, duration: 21.286s, episode steps: 123, steps per second:   6, episode reward:  2.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.065 [0.000, 4.000],  loss: 5.433859, mae: 5.566198, mean_q: 9.237506\n",
      " 20992/100000: episode: 235, duration: 23.530s, episode steps: 136, steps per second:   6, episode reward:  3.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.544 [0.000, 4.000],  loss: 6.057072, mae: 5.544964, mean_q: 9.213945\n",
      " 21114/100000: episode: 236, duration: 21.042s, episode steps: 122, steps per second:   6, episode reward:  1.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.213 [0.000, 4.000],  loss: 6.068618, mae: 5.712463, mean_q: 9.623606\n",
      " 21169/100000: episode: 237, duration: 9.619s, episode steps:  55, steps per second:   6, episode reward:  1.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.764 [0.000, 4.000],  loss: 6.085850, mae: 6.196891, mean_q: 10.445056\n",
      " 21212/100000: episode: 238, duration: 7.554s, episode steps:  43, steps per second:   6, episode reward:  1.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.256 [0.000, 4.000],  loss: 5.900200, mae: 6.223087, mean_q: 10.409595\n",
      " 21265/100000: episode: 239, duration: 9.273s, episode steps:  53, steps per second:   6, episode reward:  1.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.547 [0.000, 4.000],  loss: 6.828949, mae: 6.203717, mean_q: 10.319215\n",
      " 21378/100000: episode: 240, duration: 19.558s, episode steps: 113, steps per second:   6, episode reward:  2.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.221 [0.000, 4.000],  loss: 6.948392, mae: 6.554798, mean_q: 10.955541\n",
      " 21451/100000: episode: 241, duration: 12.708s, episode steps:  73, steps per second:   6, episode reward:  1.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.973 [0.000, 4.000],  loss: 7.381967, mae: 6.907712, mean_q: 11.532193\n",
      " 21555/100000: episode: 242, duration: 18.031s, episode steps: 104, steps per second:   6, episode reward:  2.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.096 [0.000, 4.000],  loss: 7.626181, mae: 7.147095, mean_q: 11.967615\n",
      " 21604/100000: episode: 243, duration: 8.576s, episode steps:  49, steps per second:   6, episode reward:  1.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.735 [0.000, 4.000],  loss: 7.782111, mae: 7.295787, mean_q: 12.159658\n",
      " 21656/100000: episode: 244, duration: 9.081s, episode steps:  52, steps per second:   6, episode reward:  1.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.077 [0.000, 4.000],  loss: 7.998977, mae: 7.046137, mean_q: 11.884253\n",
      " 21727/100000: episode: 245, duration: 12.346s, episode steps:  71, steps per second:   6, episode reward:  1.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.324 [0.000, 4.000],  loss: 7.190973, mae: 6.793702, mean_q: 11.464323\n",
      " 21829/100000: episode: 246, duration: 17.642s, episode steps: 102, steps per second:   6, episode reward:  1.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.382 [0.000, 4.000],  loss: 7.339715, mae: 7.183408, mean_q: 12.094317\n",
      " 21872/100000: episode: 247, duration: 7.547s, episode steps:  43, steps per second:   6, episode reward:  1.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.140 [0.000, 4.000],  loss: 7.483621, mae: 7.279178, mean_q: 12.242328\n",
      " 21915/100000: episode: 248, duration: 7.513s, episode steps:  43, steps per second:   6, episode reward:  1.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.140 [0.000, 4.000],  loss: 6.936491, mae: 7.157983, mean_q: 12.047126\n",
      " 22074/100000: episode: 249, duration: 27.455s, episode steps: 159, steps per second:   6, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.296 [0.000, 4.000],  loss: 6.564035, mae: 7.102720, mean_q: 11.926692\n",
      " 22125/100000: episode: 250, duration: 8.906s, episode steps:  51, steps per second:   6, episode reward:  2.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 1.510 [0.000, 4.000],  loss: 8.069720, mae: 7.192715, mean_q: 12.010022\n",
      " 22183/100000: episode: 251, duration: 10.101s, episode steps:  58, steps per second:   6, episode reward:  1.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.172 [0.000, 4.000],  loss: 7.766656, mae: 6.888534, mean_q: 11.563709\n",
      " 22254/100000: episode: 252, duration: 12.344s, episode steps:  71, steps per second:   6, episode reward:  2.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.169 [0.000, 4.000],  loss: 7.176460, mae: 6.985690, mean_q: 11.744894\n",
      " 22322/100000: episode: 253, duration: 11.843s, episode steps:  68, steps per second:   6, episode reward:  1.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.588 [0.000, 4.000],  loss: 6.047412, mae: 6.612218, mean_q: 11.248476\n",
      " 22365/100000: episode: 254, duration: 7.576s, episode steps:  43, steps per second:   6, episode reward:  2.000, mean reward:  0.047 [ 0.000,  1.000], mean action: 2.047 [0.000, 4.000],  loss: 6.280345, mae: 6.755023, mean_q: 11.486271\n",
      " 22638/100000: episode: 255, duration: 46.963s, episode steps: 273, steps per second:   6, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 2.337 [0.000, 4.000],  loss: 5.626891, mae: 6.602849, mean_q: 11.228055\n",
      " 22677/100000: episode: 256, duration: 6.898s, episode steps:  39, steps per second:   6, episode reward:  1.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.051 [0.000, 4.000],  loss: 6.183140, mae: 7.098762, mean_q: 11.981033\n",
      " 22763/100000: episode: 257, duration: 14.936s, episode steps:  86, steps per second:   6, episode reward:  1.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.895 [0.000, 4.000],  loss: 5.751169, mae: 7.299594, mean_q: 12.269153\n",
      " 22808/100000: episode: 258, duration: 7.940s, episode steps:  45, steps per second:   6, episode reward:  1.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.533 [0.000, 4.000],  loss: 5.598575, mae: 6.957176, mean_q: 11.766749\n",
      " 22902/100000: episode: 259, duration: 16.282s, episode steps:  94, steps per second:   6, episode reward:  2.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.053 [0.000, 4.000],  loss: 6.659878, mae: 7.205387, mean_q: 12.136467\n",
      " 22956/100000: episode: 260, duration: 9.431s, episode steps:  54, steps per second:   6, episode reward:  2.000, mean reward:  0.037 [ 0.000,  1.000], mean action: 2.000 [0.000, 4.000],  loss: 6.751349, mae: 7.570440, mean_q: 12.744660\n",
      " 22999/100000: episode: 261, duration: 7.533s, episode steps:  43, steps per second:   6, episode reward:  1.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.860 [0.000, 4.000],  loss: 7.078797, mae: 7.409717, mean_q: 12.385009\n",
      " 23073/100000: episode: 262, duration: 12.891s, episode steps:  74, steps per second:   6, episode reward:  1.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.149 [0.000, 4.000],  loss: 6.008092, mae: 6.959820, mean_q: 11.597043\n",
      " 23113/100000: episode: 263, duration: 7.043s, episode steps:  40, steps per second:   6, episode reward:  2.000, mean reward:  0.050 [ 0.000,  1.000], mean action: 2.000 [0.000, 4.000],  loss: 6.354459, mae: 7.082709, mean_q: 11.699330\n",
      " 23363/100000: episode: 264, duration: 43.120s, episode steps: 250, steps per second:   6, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.968 [0.000, 4.000],  loss: 6.138556, mae: 7.271385, mean_q: 12.093194\n",
      " 23474/100000: episode: 265, duration: 19.168s, episode steps: 111, steps per second:   6, episode reward:  1.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.468 [0.000, 4.000],  loss: 6.272195, mae: 7.868588, mean_q: 13.025391\n",
      " 23527/100000: episode: 266, duration: 9.280s, episode steps:  53, steps per second:   6, episode reward:  1.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.925 [0.000, 4.000],  loss: 6.536164, mae: 7.811332, mean_q: 12.928944\n",
      " 23617/100000: episode: 267, duration: 15.575s, episode steps:  90, steps per second:   6, episode reward:  2.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.878 [0.000, 4.000],  loss: 5.911551, mae: 7.478958, mean_q: 12.385373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 23703/100000: episode: 268, duration: 14.904s, episode steps:  86, steps per second:   6, episode reward:  1.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.081 [0.000, 4.000],  loss: 5.349967, mae: 7.051135, mean_q: 11.712268\n",
      " 23797/100000: episode: 269, duration: 16.270s, episode steps:  94, steps per second:   6, episode reward:  1.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.223 [0.000, 4.000],  loss: 5.173464, mae: 6.908716, mean_q: 11.509309\n",
      " 23904/100000: episode: 270, duration: 18.513s, episode steps: 107, steps per second:   6, episode reward:  1.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.486 [0.000, 4.000],  loss: 5.656140, mae: 7.279716, mean_q: 12.080683\n",
      " 24014/100000: episode: 271, duration: 18.977s, episode steps: 110, steps per second:   6, episode reward:  1.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.118 [0.000, 4.000],  loss: 5.763171, mae: 7.336510, mean_q: 12.162008\n",
      " 24052/100000: episode: 272, duration: 6.680s, episode steps:  38, steps per second:   6, episode reward:  2.000, mean reward:  0.053 [ 0.000,  1.000], mean action: 1.868 [0.000, 4.000],  loss: 5.657789, mae: 7.549171, mean_q: 12.457553\n",
      " 24156/100000: episode: 273, duration: 17.900s, episode steps: 104, steps per second:   6, episode reward:  1.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.163 [0.000, 4.000],  loss: 5.288271, mae: 7.165989, mean_q: 11.898498\n",
      " 24254/100000: episode: 274, duration: 16.920s, episode steps:  98, steps per second:   6, episode reward:  2.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.245 [0.000, 4.000],  loss: 6.854300, mae: 7.432407, mean_q: 12.242801\n",
      " 24376/100000: episode: 275, duration: 21.009s, episode steps: 122, steps per second:   6, episode reward:  1.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.451 [0.000, 4.000],  loss: 6.493480, mae: 7.153762, mean_q: 11.808582\n",
      " 24619/100000: episode: 276, duration: 41.703s, episode steps: 243, steps per second:   6, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 2.021 [0.000, 4.000],  loss: 6.255501, mae: 6.993854, mean_q: 11.540648\n",
      " 24694/100000: episode: 277, duration: 12.927s, episode steps:  75, steps per second:   6, episode reward:  1.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.107 [0.000, 4.000],  loss: 6.205332, mae: 6.668647, mean_q: 11.035405\n",
      " 24772/100000: episode: 278, duration: 13.547s, episode steps:  78, steps per second:   6, episode reward:  2.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.821 [0.000, 4.000],  loss: 5.865727, mae: 6.558067, mean_q: 10.835340\n",
      " 24919/100000: episode: 279, duration: 25.262s, episode steps: 147, steps per second:   6, episode reward:  1.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.020 [0.000, 4.000],  loss: 5.752438, mae: 6.359383, mean_q: 10.446424\n",
      " 25026/100000: episode: 280, duration: 18.435s, episode steps: 107, steps per second:   6, episode reward:  1.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.243 [0.000, 4.000],  loss: 6.356364, mae: 6.968510, mean_q: 11.487470\n",
      " 25092/100000: episode: 281, duration: 11.450s, episode steps:  66, steps per second:   6, episode reward:  2.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.045 [0.000, 4.000],  loss: 6.406593, mae: 7.291824, mean_q: 11.961146\n",
      " 25298/100000: episode: 282, duration: 35.364s, episode steps: 206, steps per second:   6, episode reward:  3.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.097 [0.000, 4.000],  loss: 5.733855, mae: 6.443797, mean_q: 10.566828\n",
      " 25347/100000: episode: 283, duration: 8.526s, episode steps:  49, steps per second:   6, episode reward:  1.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.245 [0.000, 4.000],  loss: 5.279971, mae: 6.189496, mean_q: 10.105712\n",
      " 25427/100000: episode: 284, duration: 13.830s, episode steps:  80, steps per second:   6, episode reward:  1.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.275 [0.000, 4.000],  loss: 4.733707, mae: 6.027969, mean_q: 9.911677\n",
      " 25479/100000: episode: 285, duration: 9.072s, episode steps:  52, steps per second:   6, episode reward:  1.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.173 [0.000, 4.000],  loss: 5.581863, mae: 6.031718, mean_q: 10.002379\n",
      " 25528/100000: episode: 286, duration: 8.546s, episode steps:  49, steps per second:   6, episode reward:  2.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 2.245 [0.000, 4.000],  loss: 5.404623, mae: 5.982631, mean_q: 9.865267\n",
      " 25571/100000: episode: 287, duration: 7.537s, episode steps:  43, steps per second:   6, episode reward:  1.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.116 [0.000, 4.000],  loss: 7.380834, mae: 6.043787, mean_q: 9.916754\n",
      " 25618/100000: episode: 288, duration: 8.215s, episode steps:  47, steps per second:   6, episode reward:  2.000, mean reward:  0.043 [ 0.000,  1.000], mean action: 1.979 [0.000, 4.000],  loss: 7.005519, mae: 6.245792, mean_q: 10.278091\n",
      " 25761/100000: episode: 289, duration: 24.747s, episode steps: 143, steps per second:   6, episode reward:  1.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.559 [0.000, 4.000],  loss: 6.707335, mae: 6.559723, mean_q: 10.703420\n",
      " 25833/100000: episode: 290, duration: 12.521s, episode steps:  72, steps per second:   6, episode reward:  1.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.333 [0.000, 4.000],  loss: 7.218348, mae: 7.129591, mean_q: 11.585261\n",
      " 25882/100000: episode: 291, duration: 8.584s, episode steps:  49, steps per second:   6, episode reward:  1.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.061 [0.000, 4.000],  loss: 7.427328, mae: 6.926746, mean_q: 11.389212\n",
      " 25937/100000: episode: 292, duration: 9.573s, episode steps:  55, steps per second:   6, episode reward:  2.000, mean reward:  0.036 [ 0.000,  1.000], mean action: 2.145 [0.000, 4.000],  loss: 6.768017, mae: 6.762470, mean_q: 11.000620\n",
      " 25992/100000: episode: 293, duration: 9.599s, episode steps:  55, steps per second:   6, episode reward:  2.000, mean reward:  0.036 [ 0.000,  1.000], mean action: 2.145 [0.000, 4.000],  loss: 6.379435, mae: 6.383132, mean_q: 10.359228\n",
      " 26060/100000: episode: 294, duration: 11.825s, episode steps:  68, steps per second:   6, episode reward:  1.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.029 [0.000, 4.000],  loss: 4.915127, mae: 5.953249, mean_q: 9.760937\n",
      " 26108/100000: episode: 295, duration: 8.394s, episode steps:  48, steps per second:   6, episode reward:  1.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.104 [0.000, 4.000],  loss: 5.362647, mae: 6.231977, mean_q: 10.234076\n",
      " 26173/100000: episode: 296, duration: 11.357s, episode steps:  65, steps per second:   6, episode reward:  2.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.431 [0.000, 4.000],  loss: 4.776298, mae: 5.901794, mean_q: 9.706422\n",
      " 26217/100000: episode: 297, duration: 7.719s, episode steps:  44, steps per second:   6, episode reward:  1.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.205 [0.000, 4.000],  loss: 5.060408, mae: 5.909526, mean_q: 9.731006\n",
      " 26258/100000: episode: 298, duration: 7.186s, episode steps:  41, steps per second:   6, episode reward:  1.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.927 [0.000, 4.000],  loss: 5.324505, mae: 6.270643, mean_q: 10.226567\n",
      " 26295/100000: episode: 299, duration: 6.522s, episode steps:  37, steps per second:   6, episode reward:  1.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.784 [0.000, 4.000],  loss: 5.011199, mae: 6.111980, mean_q: 9.989210\n",
      " 26348/100000: episode: 300, duration: 9.275s, episode steps:  53, steps per second:   6, episode reward:  1.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.264 [0.000, 4.000],  loss: 5.776000, mae: 6.405368, mean_q: 10.406837\n",
      " 26419/100000: episode: 301, duration: 12.336s, episode steps:  71, steps per second:   6, episode reward:  2.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.070 [0.000, 4.000],  loss: 5.678843, mae: 6.229669, mean_q: 10.072974\n",
      " 26487/100000: episode: 302, duration: 11.848s, episode steps:  68, steps per second:   6, episode reward:  3.000, mean reward:  0.044 [ 0.000,  1.000], mean action: 2.132 [0.000, 4.000],  loss: 4.955961, mae: 5.989919, mean_q: 9.679183\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 26542/100000: episode: 303, duration: 9.599s, episode steps:  55, steps per second:   6, episode reward:  2.000, mean reward:  0.036 [ 0.000,  1.000], mean action: 1.982 [0.000, 4.000],  loss: 4.778219, mae: 5.985465, mean_q: 9.652264\n",
      " 26580/100000: episode: 304, duration: 6.685s, episode steps:  38, steps per second:   6, episode reward:  2.000, mean reward:  0.053 [ 0.000,  1.000], mean action: 2.000 [0.000, 4.000],  loss: 4.879708, mae: 5.738923, mean_q: 9.239436\n",
      " 26753/100000: episode: 305, duration: 29.851s, episode steps: 173, steps per second:   6, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.046 [0.000, 4.000],  loss: 4.681337, mae: 5.440833, mean_q: 8.809487\n",
      " 26815/100000: episode: 306, duration: 10.806s, episode steps:  62, steps per second:   6, episode reward:  1.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.323 [0.000, 4.000],  loss: 3.881965, mae: 4.838525, mean_q: 8.028243\n",
      " 26871/100000: episode: 307, duration: 9.820s, episode steps:  56, steps per second:   6, episode reward:  1.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.857 [0.000, 4.000],  loss: 4.443332, mae: 4.884743, mean_q: 8.007532\n",
      " 26910/100000: episode: 308, duration: 6.896s, episode steps:  39, steps per second:   6, episode reward:  1.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.128 [0.000, 4.000],  loss: 4.613989, mae: 5.003980, mean_q: 8.243649\n",
      " 27189/100000: episode: 309, duration: 48.023s, episode steps: 279, steps per second:   6, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.330 [0.000, 4.000],  loss: 4.883463, mae: 5.330281, mean_q: 8.863292\n",
      " 27251/100000: episode: 310, duration: 10.775s, episode steps:  62, steps per second:   6, episode reward:  3.000, mean reward:  0.048 [ 0.000,  1.000], mean action: 1.790 [0.000, 4.000],  loss: 5.396929, mae: 6.315618, mean_q: 10.587915\n",
      " 27314/100000: episode: 311, duration: 10.937s, episode steps:  63, steps per second:   6, episode reward:  1.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.905 [0.000, 4.000],  loss: 6.415654, mae: 6.412520, mean_q: 10.674452\n",
      " 27351/100000: episode: 312, duration: 6.510s, episode steps:  37, steps per second:   6, episode reward:  1.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.703 [0.000, 4.000],  loss: 7.940529, mae: 6.515575, mean_q: 10.827603\n",
      " 27428/100000: episode: 313, duration: 13.356s, episode steps:  77, steps per second:   6, episode reward:  2.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.130 [0.000, 4.000],  loss: 6.639846, mae: 6.313835, mean_q: 10.557305\n",
      " 27502/100000: episode: 314, duration: 12.818s, episode steps:  74, steps per second:   6, episode reward:  2.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.459 [0.000, 4.000],  loss: 6.593656, mae: 6.597567, mean_q: 11.026386\n",
      " 27579/100000: episode: 315, duration: 13.449s, episode steps:  77, steps per second:   6, episode reward:  3.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 2.325 [0.000, 4.000],  loss: 6.510558, mae: 6.845143, mean_q: 11.466270\n",
      " 27660/100000: episode: 316, duration: 14.015s, episode steps:  81, steps per second:   6, episode reward:  2.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.148 [0.000, 4.000],  loss: 6.654689, mae: 6.802053, mean_q: 11.553034\n",
      " 27708/100000: episode: 317, duration: 8.366s, episode steps:  48, steps per second:   6, episode reward:  1.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.417 [0.000, 4.000],  loss: 7.630183, mae: 7.072638, mean_q: 11.893399\n",
      " 27767/100000: episode: 318, duration: 10.292s, episode steps:  59, steps per second:   6, episode reward:  1.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.186 [0.000, 4.000],  loss: 5.986856, mae: 6.833296, mean_q: 11.503449\n",
      " 27897/100000: episode: 319, duration: 22.447s, episode steps: 130, steps per second:   6, episode reward:  2.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.485 [0.000, 4.000],  loss: 5.266960, mae: 6.488679, mean_q: 11.020957\n",
      " 27943/100000: episode: 320, duration: 8.033s, episode steps:  46, steps per second:   6, episode reward:  1.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.130 [0.000, 4.000],  loss: 4.746524, mae: 6.666970, mean_q: 11.325555\n",
      " 28085/100000: episode: 321, duration: 24.523s, episode steps: 142, steps per second:   6, episode reward:  1.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.908 [0.000, 4.000],  loss: 5.659405, mae: 6.695972, mean_q: 11.268147\n",
      " 28158/100000: episode: 322, duration: 12.647s, episode steps:  73, steps per second:   6, episode reward:  2.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.986 [0.000, 4.000],  loss: 6.481048, mae: 6.789768, mean_q: 11.128841\n",
      " 28202/100000: episode: 323, duration: 7.742s, episode steps:  44, steps per second:   6, episode reward:  1.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.159 [0.000, 4.000],  loss: 5.942791, mae: 6.429200, mean_q: 10.578460\n",
      " 28292/100000: episode: 324, duration: 15.585s, episode steps:  90, steps per second:   6, episode reward:  2.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.944 [0.000, 4.000],  loss: 4.982145, mae: 6.426435, mean_q: 10.591952\n",
      " 28342/100000: episode: 325, duration: 8.758s, episode steps:  50, steps per second:   6, episode reward:  1.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.000 [0.000, 4.000],  loss: 4.711618, mae: 5.991529, mean_q: 9.839358\n",
      " 28409/100000: episode: 326, duration: 11.698s, episode steps:  67, steps per second:   6, episode reward:  2.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.343 [0.000, 4.000],  loss: 4.646697, mae: 6.014434, mean_q: 9.919353\n",
      " 28471/100000: episode: 327, duration: 10.801s, episode steps:  62, steps per second:   6, episode reward:  2.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 2.177 [0.000, 4.000],  loss: 4.592540, mae: 6.231210, mean_q: 10.306885\n",
      " 28528/100000: episode: 328, duration: 9.951s, episode steps:  57, steps per second:   6, episode reward:  1.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.246 [0.000, 4.000],  loss: 4.508091, mae: 5.895302, mean_q: 9.714820\n",
      " 28587/100000: episode: 329, duration: 10.276s, episode steps:  59, steps per second:   6, episode reward:  3.000, mean reward:  0.051 [ 0.000,  1.000], mean action: 1.966 [0.000, 4.000],  loss: 4.160264, mae: 5.876989, mean_q: 9.648413\n",
      " 28641/100000: episode: 330, duration: 9.437s, episode steps:  54, steps per second:   6, episode reward:  1.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.037 [0.000, 4.000],  loss: 4.727369, mae: 5.771205, mean_q: 9.416058\n",
      " 28691/100000: episode: 331, duration: 8.744s, episode steps:  50, steps per second:   6, episode reward:  2.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 2.340 [0.000, 4.000],  loss: 4.000371, mae: 5.557962, mean_q: 9.168514\n",
      " 28745/100000: episode: 332, duration: 9.418s, episode steps:  54, steps per second:   6, episode reward:  2.000, mean reward:  0.037 [ 0.000,  1.000], mean action: 2.000 [0.000, 4.000],  loss: 4.859652, mae: 5.560507, mean_q: 9.047366\n",
      " 28822/100000: episode: 333, duration: 13.396s, episode steps:  77, steps per second:   6, episode reward:  2.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.221 [0.000, 4.000],  loss: 4.025353, mae: 5.571757, mean_q: 9.046432\n",
      " 28927/100000: episode: 334, duration: 18.182s, episode steps: 105, steps per second:   6, episode reward:  2.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.933 [0.000, 4.000],  loss: 3.791631, mae: 5.372919, mean_q: 8.716677\n",
      " 28992/100000: episode: 335, duration: 11.305s, episode steps:  65, steps per second:   6, episode reward:  1.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.954 [0.000, 4.000],  loss: 3.982892, mae: 5.839921, mean_q: 9.449683\n",
      " 29108/100000: episode: 336, duration: 20.113s, episode steps: 116, steps per second:   6, episode reward:  1.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.207 [0.000, 4.000],  loss: 4.679889, mae: 5.559137, mean_q: 9.112133\n",
      " 29158/100000: episode: 337, duration: 8.755s, episode steps:  50, steps per second:   6, episode reward:  1.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.840 [0.000, 4.000],  loss: 4.800423, mae: 5.988523, mean_q: 9.792242\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 29208/100000: episode: 338, duration: 8.734s, episode steps:  50, steps per second:   6, episode reward:  2.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 2.000 [0.000, 4.000],  loss: 5.334792, mae: 6.209164, mean_q: 10.131711\n",
      " 29292/100000: episode: 339, duration: 14.566s, episode steps:  84, steps per second:   6, episode reward:  1.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.619 [0.000, 4.000],  loss: 5.177652, mae: 6.145382, mean_q: 10.066989\n",
      " 29598/100000: episode: 340, duration: 52.758s, episode steps: 306, steps per second:   6, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.288 [0.000, 4.000],  loss: 5.693681, mae: 7.333718, mean_q: 12.056602\n",
      " 29690/100000: episode: 341, duration: 15.897s, episode steps:  92, steps per second:   6, episode reward:  1.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.402 [0.000, 4.000],  loss: 6.468036, mae: 8.288663, mean_q: 13.655005\n",
      " 29735/100000: episode: 342, duration: 7.864s, episode steps:  45, steps per second:   6, episode reward:  2.000, mean reward:  0.044 [ 0.000,  1.000], mean action: 2.178 [0.000, 4.000],  loss: 7.312134, mae: 8.196028, mean_q: 13.549978\n",
      " 29823/100000: episode: 343, duration: 15.204s, episode steps:  88, steps per second:   6, episode reward:  1.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.534 [0.000, 4.000],  loss: 5.998425, mae: 8.414196, mean_q: 13.848050\n",
      " 29880/100000: episode: 344, duration: 9.928s, episode steps:  57, steps per second:   6, episode reward:  2.000, mean reward:  0.035 [ 0.000,  1.000], mean action: 2.018 [0.000, 4.000],  loss: 6.376675, mae: 8.731579, mean_q: 14.320617\n",
      " 29945/100000: episode: 345, duration: 11.283s, episode steps:  65, steps per second:   6, episode reward:  1.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.123 [0.000, 4.000],  loss: 7.452421, mae: 8.370893, mean_q: 13.708579\n",
      " 30012/100000: episode: 346, duration: 11.652s, episode steps:  67, steps per second:   6, episode reward:  1.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.687 [0.000, 4.000],  loss: 7.151170, mae: 8.331080, mean_q: 13.614909\n",
      " 30070/100000: episode: 347, duration: 10.094s, episode steps:  58, steps per second:   6, episode reward:  1.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.362 [0.000, 4.000],  loss: 6.700684, mae: 8.157715, mean_q: 13.294514\n",
      " 30106/100000: episode: 348, duration: 6.324s, episode steps:  36, steps per second:   6, episode reward:  1.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.861 [0.000, 4.000],  loss: 6.424002, mae: 7.412391, mean_q: 12.245748\n",
      " 30155/100000: episode: 349, duration: 8.604s, episode steps:  49, steps per second:   6, episode reward:  1.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.449 [0.000, 4.000],  loss: 5.601311, mae: 7.354936, mean_q: 12.085243\n",
      " 30211/100000: episode: 350, duration: 9.792s, episode steps:  56, steps per second:   6, episode reward:  1.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.036 [0.000, 4.000],  loss: 5.285238, mae: 7.236549, mean_q: 11.951665\n",
      " 30263/100000: episode: 351, duration: 9.106s, episode steps:  52, steps per second:   6, episode reward:  2.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 1.827 [0.000, 4.000],  loss: 5.694573, mae: 7.253893, mean_q: 11.909367\n",
      " 30336/100000: episode: 352, duration: 12.710s, episode steps:  73, steps per second:   6, episode reward:  2.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.274 [0.000, 4.000],  loss: 5.035066, mae: 7.020117, mean_q: 11.508656\n",
      " 30458/100000: episode: 353, duration: 21.136s, episode steps: 122, steps per second:   6, episode reward:  2.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.369 [0.000, 4.000],  loss: 4.991467, mae: 6.265158, mean_q: 10.300967\n",
      " 30547/100000: episode: 354, duration: 15.438s, episode steps:  89, steps per second:   6, episode reward:  1.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.011 [0.000, 4.000],  loss: 4.819743, mae: 5.937549, mean_q: 9.789315\n",
      " 30587/100000: episode: 355, duration: 7.059s, episode steps:  40, steps per second:   6, episode reward:  1.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.200 [0.000, 4.000],  loss: 4.628547, mae: 5.977005, mean_q: 9.901081\n",
      " 30659/100000: episode: 356, duration: 12.547s, episode steps:  72, steps per second:   6, episode reward:  1.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.917 [0.000, 4.000],  loss: 5.452921, mae: 5.987917, mean_q: 9.843013\n",
      " 30797/100000: episode: 357, duration: 23.843s, episode steps: 138, steps per second:   6, episode reward:  1.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.036 [0.000, 4.000],  loss: 5.623429, mae: 6.294895, mean_q: 10.332948\n",
      " 30848/100000: episode: 358, duration: 8.927s, episode steps:  51, steps per second:   6, episode reward:  1.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.137 [0.000, 4.000],  loss: 5.783360, mae: 6.449207, mean_q: 10.614980\n",
      " 30950/100000: episode: 359, duration: 17.680s, episode steps: 102, steps per second:   6, episode reward:  3.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 1.931 [0.000, 4.000],  loss: 5.455824, mae: 6.401809, mean_q: 10.589773\n",
      " 31126/100000: episode: 360, duration: 30.380s, episode steps: 176, steps per second:   6, episode reward:  2.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.034 [0.000, 4.000],  loss: 5.401814, mae: 6.551374, mean_q: 10.760925\n",
      " 31159/100000: episode: 361, duration: 5.842s, episode steps:  33, steps per second:   6, episode reward:  1.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.152 [0.000, 4.000],  loss: 5.217589, mae: 6.895067, mean_q: 11.332498\n",
      " 31218/100000: episode: 362, duration: 10.312s, episode steps:  59, steps per second:   6, episode reward:  1.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.898 [0.000, 4.000],  loss: 6.017145, mae: 7.120445, mean_q: 11.597599\n",
      " 31272/100000: episode: 363, duration: 9.425s, episode steps:  54, steps per second:   6, episode reward:  1.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.148 [0.000, 4.000],  loss: 5.449146, mae: 7.196751, mean_q: 11.754069\n",
      " 31351/100000: episode: 364, duration: 13.763s, episode steps:  79, steps per second:   6, episode reward:  2.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.696 [0.000, 4.000],  loss: 6.218253, mae: 7.309567, mean_q: 11.885182\n",
      " 31402/100000: episode: 365, duration: 8.901s, episode steps:  51, steps per second:   6, episode reward:  1.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.098 [0.000, 4.000],  loss: 5.734047, mae: 7.415388, mean_q: 12.022735\n",
      " 31543/100000: episode: 366, duration: 24.354s, episode steps: 141, steps per second:   6, episode reward:  2.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.901 [0.000, 4.000],  loss: 5.950877, mae: 7.472312, mean_q: 12.006906\n",
      " 31639/100000: episode: 367, duration: 16.677s, episode steps:  96, steps per second:   6, episode reward:  1.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.531 [0.000, 4.000],  loss: 5.742615, mae: 7.171011, mean_q: 11.476311\n",
      " 31702/100000: episode: 368, duration: 10.973s, episode steps:  63, steps per second:   6, episode reward:  1.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.190 [0.000, 4.000],  loss: 5.012934, mae: 6.957365, mean_q: 11.229554\n",
      " 31791/100000: episode: 369, duration: 15.439s, episode steps:  89, steps per second:   6, episode reward:  1.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.124 [0.000, 4.000],  loss: 5.007016, mae: 6.763747, mean_q: 10.942443\n",
      " 31835/100000: episode: 370, duration: 7.729s, episode steps:  44, steps per second:   6, episode reward:  1.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.909 [0.000, 4.000],  loss: 5.910135, mae: 6.960869, mean_q: 11.244208\n",
      " 31932/100000: episode: 371, duration: 16.817s, episode steps:  97, steps per second:   6, episode reward:  3.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.103 [0.000, 4.000],  loss: 5.631446, mae: 6.762343, mean_q: 10.857977\n",
      " 32205/100000: episode: 372, duration: 47.084s, episode steps: 273, steps per second:   6, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.952 [0.000, 4.000],  loss: 4.488245, mae: 6.168558, mean_q: 9.972626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 32258/100000: episode: 373, duration: 9.233s, episode steps:  53, steps per second:   6, episode reward:  1.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.528 [0.000, 4.000],  loss: 4.202546, mae: 6.129601, mean_q: 9.992194\n",
      " 32346/100000: episode: 374, duration: 15.261s, episode steps:  88, steps per second:   6, episode reward:  1.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.784 [0.000, 4.000],  loss: 4.109660, mae: 6.031889, mean_q: 9.836317\n",
      " 32448/100000: episode: 375, duration: 17.624s, episode steps: 102, steps per second:   6, episode reward:  2.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.990 [0.000, 4.000],  loss: 4.316694, mae: 6.287016, mean_q: 10.242890\n",
      " 32597/100000: episode: 376, duration: 25.716s, episode steps: 149, steps per second:   6, episode reward:  1.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.268 [0.000, 4.000],  loss: 4.379536, mae: 6.410317, mean_q: 10.516931\n",
      " 32658/100000: episode: 377, duration: 10.593s, episode steps:  61, steps per second:   6, episode reward:  1.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.377 [0.000, 4.000],  loss: 4.777722, mae: 7.244184, mean_q: 11.871750\n",
      " 32694/100000: episode: 378, duration: 6.350s, episode steps:  36, steps per second:   6, episode reward:  1.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.250 [0.000, 4.000],  loss: 5.546749, mae: 7.193082, mean_q: 11.792977\n",
      " 32745/100000: episode: 379, duration: 8.924s, episode steps:  51, steps per second:   6, episode reward:  2.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 2.176 [0.000, 4.000],  loss: 5.315778, mae: 7.202625, mean_q: 11.853415\n",
      " 32878/100000: episode: 380, duration: 22.994s, episode steps: 133, steps per second:   6, episode reward:  1.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.203 [0.000, 4.000],  loss: 5.266278, mae: 7.041766, mean_q: 11.530393\n",
      " 33163/100000: episode: 381, duration: 48.913s, episode steps: 285, steps per second:   6, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 2.411 [0.000, 4.000],  loss: 6.584126, mae: 8.475853, mean_q: 13.889674\n",
      " 33266/100000: episode: 382, duration: 17.783s, episode steps: 103, steps per second:   6, episode reward:  1.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.068 [0.000, 4.000],  loss: 7.404226, mae: 9.464345, mean_q: 15.519537\n",
      " 33403/100000: episode: 383, duration: 23.653s, episode steps: 137, steps per second:   6, episode reward:  1.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.394 [0.000, 4.000],  loss: 7.141989, mae: 9.522656, mean_q: 15.646708\n",
      " 33452/100000: episode: 384, duration: 8.541s, episode steps:  49, steps per second:   6, episode reward:  1.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.224 [0.000, 4.000],  loss: 7.481814, mae: 9.602908, mean_q: 15.791113\n",
      " 33564/100000: episode: 385, duration: 19.354s, episode steps: 112, steps per second:   6, episode reward:  2.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.580 [0.000, 4.000],  loss: 7.134626, mae: 9.613366, mean_q: 15.834085\n",
      " 33607/100000: episode: 386, duration: 7.518s, episode steps:  43, steps per second:   6, episode reward:  1.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.093 [0.000, 4.000],  loss: 7.006492, mae: 8.889258, mean_q: 14.678315\n",
      " 33649/100000: episode: 387, duration: 7.331s, episode steps:  42, steps per second:   6, episode reward:  1.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.310 [0.000, 4.000],  loss: 6.702161, mae: 8.916937, mean_q: 14.765895\n",
      " 33839/100000: episode: 388, duration: 32.726s, episode steps: 190, steps per second:   6, episode reward:  2.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.347 [0.000, 4.000],  loss: 6.397656, mae: 8.833330, mean_q: 14.604536\n",
      " 33897/100000: episode: 389, duration: 10.058s, episode steps:  58, steps per second:   6, episode reward:  1.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.069 [0.000, 4.000],  loss: 6.877658, mae: 8.127834, mean_q: 13.475354\n",
      " 34060/100000: episode: 390, duration: 28.114s, episode steps: 163, steps per second:   6, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.166 [0.000, 4.000],  loss: 6.952317, mae: 7.961170, mean_q: 13.120084\n",
      " 34114/100000: episode: 391, duration: 9.405s, episode steps:  54, steps per second:   6, episode reward:  2.000, mean reward:  0.037 [ 0.000,  1.000], mean action: 2.259 [0.000, 4.000],  loss: 5.741007, mae: 7.771739, mean_q: 12.849769\n",
      " 34152/100000: episode: 392, duration: 6.679s, episode steps:  38, steps per second:   6, episode reward:  1.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.579 [0.000, 4.000],  loss: 6.740564, mae: 7.619372, mean_q: 12.582194\n",
      " 34212/100000: episode: 393, duration: 10.462s, episode steps:  60, steps per second:   6, episode reward:  2.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 1.933 [0.000, 4.000],  loss: 5.924387, mae: 7.515593, mean_q: 12.351012\n",
      " 34287/100000: episode: 394, duration: 13.022s, episode steps:  75, steps per second:   6, episode reward:  1.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.240 [0.000, 4.000],  loss: 6.057560, mae: 7.055671, mean_q: 11.710514\n",
      " 34384/100000: episode: 395, duration: 16.783s, episode steps:  97, steps per second:   6, episode reward:  1.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.876 [0.000, 4.000],  loss: 6.332729, mae: 7.019634, mean_q: 11.608893\n",
      " 34455/100000: episode: 396, duration: 12.327s, episode steps:  71, steps per second:   6, episode reward:  1.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.099 [0.000, 4.000],  loss: 6.526368, mae: 6.612126, mean_q: 10.927674\n",
      " 34559/100000: episode: 397, duration: 17.961s, episode steps: 104, steps per second:   6, episode reward:  1.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.183 [0.000, 4.000],  loss: 6.351298, mae: 6.498277, mean_q: 10.780372\n",
      " 34625/100000: episode: 398, duration: 11.445s, episode steps:  66, steps per second:   6, episode reward:  1.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.212 [0.000, 4.000],  loss: 6.958367, mae: 6.760752, mean_q: 11.135349\n",
      " 34661/100000: episode: 399, duration: 6.360s, episode steps:  36, steps per second:   6, episode reward:  2.000, mean reward:  0.056 [ 0.000,  1.000], mean action: 2.472 [0.000, 4.000],  loss: 6.856093, mae: 6.963253, mean_q: 11.517814\n",
      " 34711/100000: episode: 400, duration: 8.726s, episode steps:  50, steps per second:   6, episode reward:  1.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.280 [0.000, 4.000],  loss: 6.922124, mae: 7.144077, mean_q: 11.778395\n",
      " 35085/100000: episode: 401, duration: 64.376s, episode steps: 374, steps per second:   6, episode reward:  1.000, mean reward:  0.003 [ 0.000,  1.000], mean action: 2.222 [0.000, 4.000],  loss: 6.777685, mae: 7.898965, mean_q: 12.958342\n",
      " 35185/100000: episode: 402, duration: 17.323s, episode steps: 100, steps per second:   6, episode reward:  2.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.460 [0.000, 4.000],  loss: 5.979730, mae: 8.038706, mean_q: 13.202139\n",
      " 35346/100000: episode: 403, duration: 27.713s, episode steps: 161, steps per second:   6, episode reward:  2.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.311 [0.000, 4.000],  loss: 5.750862, mae: 8.358503, mean_q: 13.723862\n",
      " 35391/100000: episode: 404, duration: 7.867s, episode steps:  45, steps per second:   6, episode reward:  2.000, mean reward:  0.044 [ 0.000,  1.000], mean action: 1.978 [0.000, 4.000],  loss: 6.100425, mae: 8.482101, mean_q: 13.875260\n",
      " 35459/100000: episode: 405, duration: 11.833s, episode steps:  68, steps per second:   6, episode reward:  1.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.235 [0.000, 4.000],  loss: 6.625408, mae: 8.706578, mean_q: 14.206158\n",
      " 35537/100000: episode: 406, duration: 13.515s, episode steps:  78, steps per second:   6, episode reward:  1.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.436 [0.000, 4.000],  loss: 5.907897, mae: 8.694084, mean_q: 14.131291\n",
      " 35578/100000: episode: 407, duration: 7.197s, episode steps:  41, steps per second:   6, episode reward:  1.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.024 [0.000, 4.000],  loss: 6.606188, mae: 8.837533, mean_q: 14.407864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 35717/100000: episode: 408, duration: 23.997s, episode steps: 139, steps per second:   6, episode reward:  2.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.439 [0.000, 4.000],  loss: 5.573839, mae: 8.730906, mean_q: 14.218329\n",
      " 35752/100000: episode: 409, duration: 6.153s, episode steps:  35, steps per second:   6, episode reward:  1.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.314 [0.000, 4.000],  loss: 4.979828, mae: 8.749703, mean_q: 14.308395\n",
      " 35792/100000: episode: 410, duration: 7.045s, episode steps:  40, steps per second:   6, episode reward:  1.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.850 [0.000, 4.000],  loss: 5.201047, mae: 8.431284, mean_q: 13.734650\n",
      " 35845/100000: episode: 411, duration: 9.266s, episode steps:  53, steps per second:   6, episode reward:  1.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.245 [0.000, 4.000],  loss: 4.839299, mae: 7.812365, mean_q: 12.792150\n",
      " 35885/100000: episode: 412, duration: 7.047s, episode steps:  40, steps per second:   6, episode reward:  1.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.050 [0.000, 4.000],  loss: 6.033771, mae: 7.129721, mean_q: 11.618566\n",
      " 35933/100000: episode: 413, duration: 8.406s, episode steps:  48, steps per second:   6, episode reward:  1.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.417 [0.000, 4.000],  loss: 4.857293, mae: 7.126155, mean_q: 11.751189\n",
      " 35984/100000: episode: 414, duration: 8.908s, episode steps:  51, steps per second:   6, episode reward:  2.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 2.314 [0.000, 4.000],  loss: 5.843286, mae: 7.105192, mean_q: 11.638353\n",
      " 36038/100000: episode: 415, duration: 9.454s, episode steps:  54, steps per second:   6, episode reward:  1.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.963 [0.000, 4.000],  loss: 5.255137, mae: 6.993391, mean_q: 11.484050\n",
      " 36161/100000: episode: 416, duration: 21.397s, episode steps: 123, steps per second:   6, episode reward:  1.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.276 [0.000, 4.000],  loss: 5.552779, mae: 6.781546, mean_q: 11.107866\n",
      " 36202/100000: episode: 417, duration: 7.201s, episode steps:  41, steps per second:   6, episode reward:  2.000, mean reward:  0.049 [ 0.000,  1.000], mean action: 1.854 [0.000, 4.000],  loss: 5.068102, mae: 6.933766, mean_q: 11.504094\n",
      " 36294/100000: episode: 418, duration: 15.978s, episode steps:  92, steps per second:   6, episode reward:  1.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.859 [0.000, 4.000],  loss: 6.234015, mae: 7.144904, mean_q: 11.697527\n",
      " 36379/100000: episode: 419, duration: 14.785s, episode steps:  85, steps per second:   6, episode reward:  2.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.588 [0.000, 4.000],  loss: 5.670564, mae: 6.941466, mean_q: 11.455260\n",
      " 36525/100000: episode: 420, duration: 25.325s, episode steps: 146, steps per second:   6, episode reward:  1.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.699 [0.000, 4.000],  loss: 6.349635, mae: 7.197754, mean_q: 12.073368\n",
      " 36683/100000: episode: 421, duration: 27.308s, episode steps: 158, steps per second:   6, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.165 [0.000, 4.000],  loss: 5.797565, mae: 7.491142, mean_q: 12.697766\n",
      " 36736/100000: episode: 422, duration: 9.251s, episode steps:  53, steps per second:   6, episode reward:  1.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.642 [0.000, 4.000],  loss: 6.641620, mae: 7.811326, mean_q: 13.224316\n",
      " 36852/100000: episode: 423, duration: 20.073s, episode steps: 116, steps per second:   6, episode reward:  2.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.250 [0.000, 4.000],  loss: 7.008815, mae: 7.881320, mean_q: 13.407137\n",
      " 36931/100000: episode: 424, duration: 13.691s, episode steps:  79, steps per second:   6, episode reward:  3.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 2.152 [0.000, 4.000],  loss: 6.168509, mae: 7.787601, mean_q: 13.271129\n",
      " 37002/100000: episode: 425, duration: 12.299s, episode steps:  71, steps per second:   6, episode reward:  1.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.592 [0.000, 4.000],  loss: 6.498841, mae: 7.917712, mean_q: 13.373843\n",
      " 37074/100000: episode: 426, duration: 12.524s, episode steps:  72, steps per second:   6, episode reward:  1.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.625 [0.000, 4.000],  loss: 5.847357, mae: 8.163273, mean_q: 13.916504\n",
      " 37169/100000: episode: 427, duration: 16.424s, episode steps:  95, steps per second:   6, episode reward:  1.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.221 [0.000, 4.000],  loss: 6.247322, mae: 7.813612, mean_q: 13.323084\n",
      " 37469/100000: episode: 428, duration: 51.613s, episode steps: 300, steps per second:   6, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.890 [0.000, 4.000],  loss: 5.851068, mae: 7.209852, mean_q: 12.254127\n",
      " 37620/100000: episode: 429, duration: 25.933s, episode steps: 151, steps per second:   6, episode reward:  2.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.152 [0.000, 4.000],  loss: 5.714532, mae: 7.352240, mean_q: 12.192308\n",
      " 37688/100000: episode: 430, duration: 11.806s, episode steps:  68, steps per second:   6, episode reward:  1.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.912 [0.000, 4.000],  loss: 5.196218, mae: 7.371464, mean_q: 12.118375\n",
      " 37757/100000: episode: 431, duration: 12.006s, episode steps:  69, steps per second:   6, episode reward:  1.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.058 [0.000, 4.000],  loss: 5.174157, mae: 7.023320, mean_q: 11.600574\n",
      " 37825/100000: episode: 432, duration: 11.812s, episode steps:  68, steps per second:   6, episode reward:  1.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.397 [0.000, 4.000],  loss: 4.544698, mae: 6.985013, mean_q: 11.531442\n",
      " 37865/100000: episode: 433, duration: 7.022s, episode steps:  40, steps per second:   6, episode reward:  1.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.175 [0.000, 4.000],  loss: 5.080473, mae: 6.856051, mean_q: 11.288902\n",
      " 37913/100000: episode: 434, duration: 8.398s, episode steps:  48, steps per second:   6, episode reward:  1.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.021 [0.000, 4.000],  loss: 4.963981, mae: 6.983840, mean_q: 11.527196\n",
      " 37948/100000: episode: 435, duration: 6.169s, episode steps:  35, steps per second:   6, episode reward:  1.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.086 [0.000, 4.000],  loss: 5.525094, mae: 6.877785, mean_q: 11.419501\n",
      " 38086/100000: episode: 436, duration: 23.786s, episode steps: 138, steps per second:   6, episode reward:  1.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.203 [0.000, 4.000],  loss: 5.281821, mae: 6.683378, mean_q: 11.086595\n",
      " 38153/100000: episode: 437, duration: 11.622s, episode steps:  67, steps per second:   6, episode reward:  1.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.134 [0.000, 4.000],  loss: 5.484067, mae: 6.525955, mean_q: 10.909766\n",
      " 38314/100000: episode: 438, duration: 27.681s, episode steps: 161, steps per second:   6, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.130 [0.000, 4.000],  loss: 5.020576, mae: 6.784932, mean_q: 11.252937\n",
      " 38395/100000: episode: 439, duration: 13.990s, episode steps:  81, steps per second:   6, episode reward:  1.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.148 [0.000, 4.000],  loss: 5.556712, mae: 7.083718, mean_q: 11.596850\n",
      " 38454/100000: episode: 440, duration: 10.350s, episode steps:  59, steps per second:   6, episode reward:  1.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.475 [0.000, 4.000],  loss: 4.810231, mae: 6.858248, mean_q: 11.414242\n",
      " 38556/100000: episode: 441, duration: 17.698s, episode steps: 102, steps per second:   6, episode reward:  1.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.314 [0.000, 4.000],  loss: 5.724848, mae: 6.655682, mean_q: 11.085413\n",
      " 38625/100000: episode: 442, duration: 12.040s, episode steps:  69, steps per second:   6, episode reward:  2.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.290 [0.000, 4.000],  loss: 5.265058, mae: 6.228938, mean_q: 10.383469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 38851/100000: episode: 443, duration: 38.997s, episode steps: 226, steps per second:   6, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.235 [0.000, 4.000],  loss: 5.099897, mae: 6.304586, mean_q: 10.495872\n",
      " 39031/100000: episode: 444, duration: 30.963s, episode steps: 180, steps per second:   6, episode reward:  4.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.211 [0.000, 4.000],  loss: 4.696270, mae: 6.563058, mean_q: 10.884044\n",
      " 39079/100000: episode: 445, duration: 8.383s, episode steps:  48, steps per second:   6, episode reward:  1.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.917 [0.000, 4.000],  loss: 4.703920, mae: 6.642570, mean_q: 10.862515\n",
      " 39128/100000: episode: 446, duration: 8.553s, episode steps:  49, steps per second:   6, episode reward:  2.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 1.612 [0.000, 4.000],  loss: 4.206356, mae: 6.322611, mean_q: 10.471395\n",
      " 39224/100000: episode: 447, duration: 16.606s, episode steps:  96, steps per second:   6, episode reward:  1.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.406 [0.000, 4.000],  loss: 4.592914, mae: 6.450603, mean_q: 10.592187\n",
      " 39363/100000: episode: 448, duration: 24.039s, episode steps: 139, steps per second:   6, episode reward:  1.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.281 [0.000, 4.000],  loss: 4.252223, mae: 6.511251, mean_q: 10.844551\n",
      " 39401/100000: episode: 449, duration: 6.688s, episode steps:  38, steps per second:   6, episode reward:  1.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.158 [0.000, 4.000],  loss: 4.386947, mae: 6.398280, mean_q: 10.720836\n",
      " 39438/100000: episode: 450, duration: 6.504s, episode steps:  37, steps per second:   6, episode reward:  1.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.162 [0.000, 4.000],  loss: 5.381791, mae: 6.202231, mean_q: 10.278381\n",
      " 39496/100000: episode: 451, duration: 10.095s, episode steps:  58, steps per second:   6, episode reward:  1.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.345 [0.000, 4.000],  loss: 4.834060, mae: 6.206704, mean_q: 10.231936\n",
      " 39551/100000: episode: 452, duration: 9.589s, episode steps:  55, steps per second:   6, episode reward:  1.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.927 [0.000, 4.000],  loss: 4.278294, mae: 6.133389, mean_q: 10.145535\n",
      " 39596/100000: episode: 453, duration: 7.907s, episode steps:  45, steps per second:   6, episode reward:  1.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.711 [0.000, 4.000],  loss: 4.305762, mae: 6.254223, mean_q: 10.342032\n",
      " 39830/100000: episode: 454, duration: 40.400s, episode steps: 234, steps per second:   6, episode reward:  3.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.380 [0.000, 4.000],  loss: 4.467254, mae: 6.306950, mean_q: 10.381110\n",
      " 39907/100000: episode: 455, duration: 13.367s, episode steps:  77, steps per second:   6, episode reward:  1.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.558 [0.000, 4.000],  loss: 4.360771, mae: 6.408393, mean_q: 10.658535\n",
      " 39953/100000: episode: 456, duration: 8.034s, episode steps:  46, steps per second:   6, episode reward:  2.000, mean reward:  0.043 [ 0.000,  1.000], mean action: 2.065 [0.000, 4.000],  loss: 3.524594, mae: 6.172601, mean_q: 10.305118\n",
      " 40100/100000: episode: 457, duration: 25.455s, episode steps: 147, steps per second:   6, episode reward:  2.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.082 [0.000, 4.000],  loss: 5.160572, mae: 6.704890, mean_q: 11.117709\n",
      " 40182/100000: episode: 458, duration: 14.248s, episode steps:  82, steps per second:   6, episode reward:  1.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.707 [0.000, 4.000],  loss: 4.823780, mae: 6.947085, mean_q: 11.561449\n",
      " 40268/100000: episode: 459, duration: 14.925s, episode steps:  86, steps per second:   6, episode reward:  1.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.337 [0.000, 4.000],  loss: 5.514511, mae: 6.849147, mean_q: 11.510119\n",
      " 40352/100000: episode: 460, duration: 14.613s, episode steps:  84, steps per second:   6, episode reward:  1.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.714 [0.000, 4.000],  loss: 5.131141, mae: 6.816801, mean_q: 11.515566\n",
      " 40520/100000: episode: 461, duration: 28.973s, episode steps: 168, steps per second:   6, episode reward:  3.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.042 [0.000, 4.000],  loss: 5.196322, mae: 7.298684, mean_q: 12.331559\n",
      " 40592/100000: episode: 462, duration: 12.488s, episode steps:  72, steps per second:   6, episode reward:  2.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.181 [0.000, 4.000],  loss: 5.083932, mae: 7.611732, mean_q: 12.787220\n",
      " 40661/100000: episode: 463, duration: 12.009s, episode steps:  69, steps per second:   6, episode reward:  1.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.304 [0.000, 4.000],  loss: 5.478049, mae: 7.449493, mean_q: 12.557121\n",
      " 40769/100000: episode: 464, duration: 18.697s, episode steps: 108, steps per second:   6, episode reward:  1.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.426 [0.000, 4.000],  loss: 5.505306, mae: 7.432383, mean_q: 12.515910\n",
      " 40901/100000: episode: 465, duration: 22.863s, episode steps: 132, steps per second:   6, episode reward:  1.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.462 [0.000, 4.000],  loss: 6.021658, mae: 7.960592, mean_q: 13.322865\n",
      " 41125/100000: episode: 466, duration: 38.598s, episode steps: 224, steps per second:   6, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 2.344 [0.000, 4.000],  loss: 5.894400, mae: 8.190164, mean_q: 13.702291\n",
      " 41238/100000: episode: 467, duration: 19.556s, episode steps: 113, steps per second:   6, episode reward:  2.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.257 [0.000, 4.000],  loss: 5.313072, mae: 8.063235, mean_q: 13.517139\n",
      " 41443/100000: episode: 468, duration: 35.244s, episode steps: 205, steps per second:   6, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.044 [0.000, 4.000],  loss: 5.206705, mae: 8.128824, mean_q: 13.497280\n",
      " 41487/100000: episode: 469, duration: 7.684s, episode steps:  44, steps per second:   6, episode reward:  2.000, mean reward:  0.045 [ 0.000,  1.000], mean action: 2.023 [0.000, 4.000],  loss: 5.181482, mae: 8.242661, mean_q: 13.721433\n",
      " 41550/100000: episode: 470, duration: 10.931s, episode steps:  63, steps per second:   6, episode reward:  1.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.556 [0.000, 4.000],  loss: 6.052581, mae: 8.190271, mean_q: 13.587194\n",
      " 41762/100000: episode: 471, duration: 36.422s, episode steps: 212, steps per second:   6, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.458 [0.000, 4.000],  loss: 5.677688, mae: 8.449769, mean_q: 14.017863\n",
      " 41808/100000: episode: 472, duration: 8.079s, episode steps:  46, steps per second:   6, episode reward:  1.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.348 [0.000, 4.000],  loss: 6.106119, mae: 8.834842, mean_q: 14.646444\n",
      " 41934/100000: episode: 473, duration: 21.728s, episode steps: 126, steps per second:   6, episode reward:  1.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.381 [0.000, 4.000],  loss: 5.352851, mae: 8.391249, mean_q: 13.954757\n",
      " 42022/100000: episode: 474, duration: 15.238s, episode steps:  88, steps per second:   6, episode reward:  1.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.352 [0.000, 4.000],  loss: 5.529625, mae: 9.216687, mean_q: 15.183728\n",
      " 42084/100000: episode: 475, duration: 10.805s, episode steps:  62, steps per second:   6, episode reward:  1.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.952 [0.000, 4.000],  loss: 6.354243, mae: 9.237598, mean_q: 15.097926\n",
      " 42154/100000: episode: 476, duration: 12.177s, episode steps:  70, steps per second:   6, episode reward:  1.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.257 [0.000, 4.000],  loss: 5.799409, mae: 8.893780, mean_q: 14.393773\n",
      " 42211/100000: episode: 477, duration: 9.939s, episode steps:  57, steps per second:   6, episode reward:  1.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.193 [0.000, 4.000],  loss: 6.253124, mae: 9.284340, mean_q: 15.043185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 42250/100000: episode: 478, duration: 6.836s, episode steps:  39, steps per second:   6, episode reward:  1.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.513 [0.000, 4.000],  loss: 6.294784, mae: 9.233840, mean_q: 15.015364\n",
      " 42298/100000: episode: 479, duration: 8.402s, episode steps:  48, steps per second:   6, episode reward:  1.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.104 [0.000, 4.000],  loss: 6.865736, mae: 9.239221, mean_q: 14.990410\n",
      " 42365/100000: episode: 480, duration: 11.662s, episode steps:  67, steps per second:   6, episode reward:  2.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.418 [0.000, 4.000],  loss: 7.704737, mae: 9.350788, mean_q: 15.062372\n",
      " 42496/100000: episode: 481, duration: 22.661s, episode steps: 131, steps per second:   6, episode reward:  3.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.046 [0.000, 4.000],  loss: 6.339449, mae: 8.732029, mean_q: 14.183603\n",
      " 42549/100000: episode: 482, duration: 9.258s, episode steps:  53, steps per second:   6, episode reward:  2.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 2.358 [0.000, 4.000],  loss: 6.990871, mae: 8.370626, mean_q: 13.596269\n",
      " 42594/100000: episode: 483, duration: 7.875s, episode steps:  45, steps per second:   6, episode reward:  1.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.022 [0.000, 4.000],  loss: 5.687624, mae: 8.281667, mean_q: 13.424596\n",
      " 42744/100000: episode: 484, duration: 25.976s, episode steps: 150, steps per second:   6, episode reward:  2.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.167 [0.000, 4.000],  loss: 5.873664, mae: 7.993781, mean_q: 12.950448\n",
      " 42783/100000: episode: 485, duration: 6.893s, episode steps:  39, steps per second:   6, episode reward:  1.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.308 [0.000, 4.000],  loss: 5.831731, mae: 7.597090, mean_q: 12.296361\n",
      " 42863/100000: episode: 486, duration: 13.986s, episode steps:  80, steps per second:   6, episode reward:  1.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.375 [0.000, 4.000],  loss: 6.710758, mae: 8.018045, mean_q: 12.950543\n",
      " 42926/100000: episode: 487, duration: 10.996s, episode steps:  63, steps per second:   6, episode reward:  2.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 1.794 [0.000, 4.000],  loss: 7.642300, mae: 7.411323, mean_q: 11.993279\n",
      " 42991/100000: episode: 488, duration: 11.340s, episode steps:  65, steps per second:   6, episode reward:  1.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.938 [0.000, 4.000],  loss: 6.037742, mae: 7.104257, mean_q: 11.507442\n",
      " 43034/100000: episode: 489, duration: 7.551s, episode steps:  43, steps per second:   6, episode reward:  1.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.884 [0.000, 4.000],  loss: 5.893806, mae: 6.931962, mean_q: 11.276765\n",
      " 43087/100000: episode: 490, duration: 9.273s, episode steps:  53, steps per second:   6, episode reward:  1.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.000 [0.000, 4.000],  loss: 5.948476, mae: 6.833415, mean_q: 11.147577\n",
      " 43166/100000: episode: 491, duration: 13.755s, episode steps:  79, steps per second:   6, episode reward:  1.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.127 [0.000, 4.000],  loss: 6.343679, mae: 6.513564, mean_q: 10.553680\n",
      " 43221/100000: episode: 492, duration: 9.630s, episode steps:  55, steps per second:   6, episode reward:  1.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.455 [0.000, 4.000],  loss: 5.956041, mae: 6.690329, mean_q: 10.910972\n",
      " 43532/100000: episode: 493, duration: 53.719s, episode steps: 311, steps per second:   6, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.347 [0.000, 4.000],  loss: 5.230905, mae: 6.801298, mean_q: 11.235004\n",
      " 43575/100000: episode: 494, duration: 7.530s, episode steps:  43, steps per second:   6, episode reward:  1.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.186 [0.000, 4.000],  loss: 5.025257, mae: 7.174017, mean_q: 11.981314\n",
      " 43659/100000: episode: 495, duration: 14.570s, episode steps:  84, steps per second:   6, episode reward:  2.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.024 [0.000, 4.000],  loss: 6.257473, mae: 7.242761, mean_q: 12.043254\n",
      " 43698/100000: episode: 496, duration: 6.842s, episode steps:  39, steps per second:   6, episode reward:  1.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.077 [0.000, 4.000],  loss: 4.821704, mae: 6.730489, mean_q: 11.240102\n",
      " 43749/100000: episode: 497, duration: 8.912s, episode steps:  51, steps per second:   6, episode reward:  1.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.216 [0.000, 4.000],  loss: 5.129174, mae: 6.746540, mean_q: 11.255149\n",
      " 43781/100000: episode: 498, duration: 5.666s, episode steps:  32, steps per second:   6, episode reward:  1.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.719 [0.000, 4.000],  loss: 5.275636, mae: 6.705761, mean_q: 11.142492\n",
      " 43834/100000: episode: 499, duration: 9.274s, episode steps:  53, steps per second:   6, episode reward:  1.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.774 [0.000, 4.000],  loss: 5.170813, mae: 6.838947, mean_q: 11.343834\n",
      " 43886/100000: episode: 500, duration: 9.086s, episode steps:  52, steps per second:   6, episode reward:  1.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.750 [0.000, 4.000],  loss: 6.452601, mae: 6.771696, mean_q: 11.144301\n",
      " 44060/100000: episode: 501, duration: 30.001s, episode steps: 174, steps per second:   6, episode reward:  2.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.448 [0.000, 4.000],  loss: 5.005250, mae: 6.466243, mean_q: 10.832765\n",
      " 44106/100000: episode: 502, duration: 8.067s, episode steps:  46, steps per second:   6, episode reward:  3.000, mean reward:  0.065 [ 0.000,  1.000], mean action: 2.130 [0.000, 4.000],  loss: 5.020984, mae: 7.016780, mean_q: 11.762500\n",
      " 44179/100000: episode: 503, duration: 12.693s, episode steps:  73, steps per second:   6, episode reward:  3.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 1.986 [0.000, 4.000],  loss: 5.281314, mae: 6.506049, mean_q: 10.827993\n",
      " 44265/100000: episode: 504, duration: 14.938s, episode steps:  86, steps per second:   6, episode reward:  1.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.372 [0.000, 4.000],  loss: 4.586645, mae: 6.230805, mean_q: 10.483171\n",
      " 44324/100000: episode: 505, duration: 10.281s, episode steps:  59, steps per second:   6, episode reward:  2.000, mean reward:  0.034 [ 0.000,  1.000], mean action: 2.136 [0.000, 4.000],  loss: 4.725608, mae: 6.210799, mean_q: 10.346794\n",
      " 44371/100000: episode: 506, duration: 8.240s, episode steps:  47, steps per second:   6, episode reward:  2.000, mean reward:  0.043 [ 0.000,  1.000], mean action: 2.298 [0.000, 4.000],  loss: 5.465533, mae: 6.073453, mean_q: 10.025900\n",
      " 44436/100000: episode: 507, duration: 11.318s, episode steps:  65, steps per second:   6, episode reward:  1.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.046 [0.000, 4.000],  loss: 4.870298, mae: 5.745915, mean_q: 9.541913\n",
      " 44568/100000: episode: 508, duration: 22.840s, episode steps: 132, steps per second:   6, episode reward:  2.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.144 [0.000, 4.000],  loss: 4.771251, mae: 6.009617, mean_q: 9.840600\n",
      " 44615/100000: episode: 509, duration: 8.238s, episode steps:  47, steps per second:   6, episode reward:  1.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.638 [0.000, 4.000],  loss: 4.893745, mae: 6.395599, mean_q: 10.490427\n",
      " 44843/100000: episode: 510, duration: 39.268s, episode steps: 228, steps per second:   6, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.320 [0.000, 4.000],  loss: 4.765303, mae: 6.540067, mean_q: 10.834267\n",
      " 44928/100000: episode: 511, duration: 14.742s, episode steps:  85, steps per second:   6, episode reward:  1.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.941 [0.000, 4.000],  loss: 4.797367, mae: 7.025411, mean_q: 11.841021\n",
      " 45014/100000: episode: 512, duration: 14.889s, episode steps:  86, steps per second:   6, episode reward:  2.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.233 [0.000, 4.000],  loss: 5.048912, mae: 7.189493, mean_q: 12.015922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 45051/100000: episode: 513, duration: 6.520s, episode steps:  37, steps per second:   6, episode reward:  1.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.216 [0.000, 4.000],  loss: 4.585954, mae: 6.855047, mean_q: 11.406037\n",
      " 45106/100000: episode: 514, duration: 9.589s, episode steps:  55, steps per second:   6, episode reward:  1.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.055 [0.000, 4.000],  loss: 5.430320, mae: 7.110699, mean_q: 11.773421\n",
      " 45156/100000: episode: 515, duration: 8.758s, episode steps:  50, steps per second:   6, episode reward:  1.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.160 [0.000, 4.000],  loss: 5.948578, mae: 7.386378, mean_q: 12.169890\n",
      " 45216/100000: episode: 516, duration: 10.450s, episode steps:  60, steps per second:   6, episode reward:  1.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.983 [0.000, 4.000],  loss: 5.648504, mae: 7.448042, mean_q: 12.260904\n",
      " 45294/100000: episode: 517, duration: 13.586s, episode steps:  78, steps per second:   6, episode reward:  1.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.256 [0.000, 4.000],  loss: 6.268864, mae: 7.455528, mean_q: 12.247168\n",
      " 45497/100000: episode: 518, duration: 35.009s, episode steps: 203, steps per second:   6, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 2.251 [0.000, 4.000],  loss: 5.587098, mae: 7.889760, mean_q: 13.186468\n",
      " 45570/100000: episode: 519, duration: 12.714s, episode steps:  73, steps per second:   6, episode reward:  1.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.342 [0.000, 4.000],  loss: 5.902534, mae: 7.804386, mean_q: 13.097337\n",
      " 45683/100000: episode: 520, duration: 19.549s, episode steps: 113, steps per second:   6, episode reward:  1.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.425 [0.000, 4.000],  loss: 5.291368, mae: 8.106119, mean_q: 13.712501\n",
      " 45773/100000: episode: 521, duration: 15.650s, episode steps:  90, steps per second:   6, episode reward:  2.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.656 [0.000, 4.000],  loss: 5.728050, mae: 8.605380, mean_q: 14.433928\n",
      " 45884/100000: episode: 522, duration: 19.235s, episode steps: 111, steps per second:   6, episode reward:  1.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.441 [0.000, 4.000],  loss: 6.231047, mae: 8.212279, mean_q: 13.716833\n",
      " 45991/100000: episode: 523, duration: 18.512s, episode steps: 107, steps per second:   6, episode reward:  1.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.103 [0.000, 4.000],  loss: 5.947605, mae: 8.055210, mean_q: 13.467801\n",
      " 46035/100000: episode: 524, duration: 7.736s, episode steps:  44, steps per second:   6, episode reward:  1.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.114 [0.000, 4.000],  loss: 7.633796, mae: 8.360100, mean_q: 14.071218\n",
      " 46111/100000: episode: 525, duration: 13.172s, episode steps:  76, steps per second:   6, episode reward:  2.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.224 [0.000, 4.000],  loss: 7.494374, mae: 8.542513, mean_q: 14.346595\n",
      " 46146/100000: episode: 526, duration: 6.176s, episode steps:  35, steps per second:   6, episode reward:  1.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 1.800 [0.000, 4.000],  loss: 6.742139, mae: 8.066644, mean_q: 13.667630\n",
      " 46380/100000: episode: 527, duration: 40.304s, episode steps: 234, steps per second:   6, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.009 [0.000, 4.000],  loss: 6.767766, mae: 7.981246, mean_q: 13.406853\n",
      " 46434/100000: episode: 528, duration: 9.430s, episode steps:  54, steps per second:   6, episode reward:  1.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.000 [0.000, 4.000],  loss: 6.649225, mae: 7.488332, mean_q: 12.418078\n",
      " 46586/100000: episode: 529, duration: 26.251s, episode steps: 152, steps per second:   6, episode reward:  2.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.112 [0.000, 4.000],  loss: 5.776184, mae: 7.213542, mean_q: 11.969357\n",
      " 46727/100000: episode: 530, duration: 24.351s, episode steps: 141, steps per second:   6, episode reward:  1.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.312 [0.000, 4.000],  loss: 5.408463, mae: 6.683709, mean_q: 11.052495\n",
      " 46771/100000: episode: 531, duration: 7.688s, episode steps:  44, steps per second:   6, episode reward:  1.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.273 [0.000, 4.000],  loss: 5.267091, mae: 6.741432, mean_q: 11.089648\n",
      " 46818/100000: episode: 532, duration: 8.211s, episode steps:  47, steps per second:   6, episode reward:  1.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.043 [0.000, 4.000],  loss: 4.780187, mae: 6.738648, mean_q: 11.145464\n",
      " 46875/100000: episode: 533, duration: 9.946s, episode steps:  57, steps per second:   6, episode reward:  2.000, mean reward:  0.035 [ 0.000,  1.000], mean action: 2.140 [0.000, 4.000],  loss: 5.812243, mae: 6.310564, mean_q: 10.345468\n",
      " 46968/100000: episode: 534, duration: 16.084s, episode steps:  93, steps per second:   6, episode reward:  1.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.140 [0.000, 4.000],  loss: 4.587983, mae: 6.270516, mean_q: 10.318708\n",
      " 47034/100000: episode: 535, duration: 11.512s, episode steps:  66, steps per second:   6, episode reward:  3.000, mean reward:  0.045 [ 0.000,  1.000], mean action: 1.879 [0.000, 4.000],  loss: 4.667108, mae: 6.181808, mean_q: 10.186492\n",
      " 47278/100000: episode: 536, duration: 41.974s, episode steps: 244, steps per second:   6, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.352 [0.000, 4.000],  loss: 4.596779, mae: 6.378019, mean_q: 10.503090\n",
      " 47328/100000: episode: 537, duration: 8.736s, episode steps:  50, steps per second:   6, episode reward:  1.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.860 [0.000, 4.000],  loss: 4.897388, mae: 6.552032, mean_q: 10.975586\n",
      " 47400/100000: episode: 538, duration: 12.485s, episode steps:  72, steps per second:   6, episode reward:  2.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.250 [0.000, 4.000],  loss: 5.191298, mae: 6.737494, mean_q: 11.099114\n",
      " 47457/100000: episode: 539, duration: 9.928s, episode steps:  57, steps per second:   6, episode reward:  1.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.018 [0.000, 4.000],  loss: 5.631889, mae: 6.688467, mean_q: 11.051202\n",
      " 47604/100000: episode: 540, duration: 25.394s, episode steps: 147, steps per second:   6, episode reward:  2.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.803 [0.000, 4.000],  loss: 5.089144, mae: 6.821983, mean_q: 11.312847\n",
      " 47688/100000: episode: 541, duration: 14.573s, episode steps:  84, steps per second:   6, episode reward:  2.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.274 [0.000, 4.000],  loss: 5.761410, mae: 7.044349, mean_q: 11.623725\n",
      " 47743/100000: episode: 542, duration: 9.618s, episode steps:  55, steps per second:   6, episode reward:  1.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.236 [0.000, 4.000],  loss: 4.811567, mae: 6.548320, mean_q: 10.812785\n",
      " 47855/100000: episode: 543, duration: 19.368s, episode steps: 112, steps per second:   6, episode reward:  1.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.571 [0.000, 4.000],  loss: 5.490827, mae: 6.627011, mean_q: 11.023479\n",
      " 47926/100000: episode: 544, duration: 12.325s, episode steps:  71, steps per second:   6, episode reward:  3.000, mean reward:  0.042 [ 0.000,  1.000], mean action: 2.169 [0.000, 4.000],  loss: 5.168357, mae: 6.501166, mean_q: 10.805120\n",
      " 47987/100000: episode: 545, duration: 10.640s, episode steps:  61, steps per second:   6, episode reward:  1.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.279 [0.000, 4.000],  loss: 4.505684, mae: 5.998224, mean_q: 10.010763\n",
      " 48090/100000: episode: 546, duration: 17.814s, episode steps: 103, steps per second:   6, episode reward:  1.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.689 [0.000, 4.000],  loss: 4.449147, mae: 6.086496, mean_q: 10.263478\n",
      " 48141/100000: episode: 547, duration: 8.906s, episode steps:  51, steps per second:   6, episode reward:  1.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.961 [0.000, 4.000],  loss: 5.259845, mae: 6.395653, mean_q: 10.842641\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 48255/100000: episode: 548, duration: 19.733s, episode steps: 114, steps per second:   6, episode reward:  1.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.439 [0.000, 4.000],  loss: 4.982746, mae: 5.963404, mean_q: 10.053222\n",
      " 48303/100000: episode: 549, duration: 8.430s, episode steps:  48, steps per second:   6, episode reward:  1.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.896 [0.000, 4.000],  loss: 5.575634, mae: 5.726823, mean_q: 9.695603\n",
      " 48394/100000: episode: 550, duration: 15.783s, episode steps:  91, steps per second:   6, episode reward:  1.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.242 [0.000, 4.000],  loss: 4.853025, mae: 5.772946, mean_q: 9.846055\n",
      " 48433/100000: episode: 551, duration: 6.863s, episode steps:  39, steps per second:   6, episode reward:  1.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.256 [0.000, 4.000],  loss: 4.857255, mae: 6.132282, mean_q: 10.541663\n",
      " 48575/100000: episode: 552, duration: 24.549s, episode steps: 142, steps per second:   6, episode reward:  1.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.401 [0.000, 4.000],  loss: 4.736720, mae: 5.785749, mean_q: 9.990082\n",
      " 48634/100000: episode: 553, duration: 10.286s, episode steps:  59, steps per second:   6, episode reward:  1.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.525 [0.000, 4.000],  loss: 4.878654, mae: 5.823994, mean_q: 10.152058\n",
      " 48700/100000: episode: 554, duration: 11.493s, episode steps:  66, steps per second:   6, episode reward:  1.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.303 [0.000, 4.000],  loss: 5.250819, mae: 5.730038, mean_q: 10.040548\n",
      " 48738/100000: episode: 555, duration: 6.695s, episode steps:  38, steps per second:   6, episode reward:  1.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.737 [0.000, 4.000],  loss: 5.430609, mae: 5.944163, mean_q: 10.275551\n",
      " 48783/100000: episode: 556, duration: 7.870s, episode steps:  45, steps per second:   6, episode reward:  1.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.133 [0.000, 4.000],  loss: 4.265555, mae: 5.775908, mean_q: 10.118858\n",
      " 48853/100000: episode: 557, duration: 12.196s, episode steps:  70, steps per second:   6, episode reward:  2.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.157 [0.000, 4.000],  loss: 4.657657, mae: 5.877831, mean_q: 10.227792\n",
      " 48908/100000: episode: 558, duration: 9.627s, episode steps:  55, steps per second:   6, episode reward:  1.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.836 [0.000, 4.000],  loss: 5.403222, mae: 6.223368, mean_q: 10.649201\n",
      " 49040/100000: episode: 559, duration: 22.886s, episode steps: 132, steps per second:   6, episode reward:  2.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.508 [0.000, 4.000],  loss: 5.192333, mae: 6.371539, mean_q: 10.930337\n",
      " 49186/100000: episode: 560, duration: 25.179s, episode steps: 146, steps per second:   6, episode reward:  2.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.322 [0.000, 4.000],  loss: 5.291487, mae: 6.560917, mean_q: 11.235820\n",
      " 49254/100000: episode: 561, duration: 11.869s, episode steps:  68, steps per second:   6, episode reward:  2.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.044 [0.000, 4.000],  loss: 5.458441, mae: 6.892315, mean_q: 11.798762\n",
      " 49456/100000: episode: 562, duration: 34.846s, episode steps: 202, steps per second:   6, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 2.153 [0.000, 4.000],  loss: 5.385767, mae: 7.125348, mean_q: 12.083704\n",
      " 49503/100000: episode: 563, duration: 8.226s, episode steps:  47, steps per second:   6, episode reward:  1.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.170 [0.000, 4.000],  loss: 5.737137, mae: 7.579471, mean_q: 12.629204\n",
      " 49786/100000: episode: 564, duration: 48.747s, episode steps: 283, steps per second:   6, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.481 [0.000, 4.000],  loss: 5.503167, mae: 7.951534, mean_q: 13.339871\n",
      " 49974/100000: episode: 565, duration: 32.431s, episode steps: 188, steps per second:   6, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 2.351 [0.000, 4.000],  loss: 5.864450, mae: 9.116122, mean_q: 15.279424\n",
      " 50036/100000: episode: 566, duration: 10.798s, episode steps:  62, steps per second:   6, episode reward:  2.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 2.113 [0.000, 4.000],  loss: 5.261026, mae: 8.843407, mean_q: 14.740140\n",
      " 50140/100000: episode: 567, duration: 17.974s, episode steps: 104, steps per second:   6, episode reward:  2.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.250 [0.000, 4.000],  loss: 5.213142, mae: 8.936460, mean_q: 14.948821\n",
      " 50237/100000: episode: 568, duration: 16.782s, episode steps:  97, steps per second:   6, episode reward:  1.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.485 [0.000, 4.000],  loss: 5.666448, mae: 8.994032, mean_q: 14.969939\n",
      " 50360/100000: episode: 569, duration: 21.282s, episode steps: 123, steps per second:   6, episode reward:  2.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.797 [0.000, 4.000],  loss: 5.133217, mae: 8.984255, mean_q: 15.033374\n",
      " 50430/100000: episode: 570, duration: 12.154s, episode steps:  70, steps per second:   6, episode reward:  2.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.229 [0.000, 4.000],  loss: 4.676877, mae: 8.464495, mean_q: 14.211143\n",
      " 50635/100000: episode: 571, duration: 35.272s, episode steps: 205, steps per second:   6, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.112 [0.000, 4.000],  loss: 5.529479, mae: 8.268286, mean_q: 13.838172\n",
      " 50675/100000: episode: 572, duration: 7.022s, episode steps:  40, steps per second:   6, episode reward:  2.000, mean reward:  0.050 [ 0.000,  1.000], mean action: 2.250 [0.000, 4.000],  loss: 5.402970, mae: 7.794346, mean_q: 12.843335\n",
      " 50737/100000: episode: 573, duration: 10.771s, episode steps:  62, steps per second:   6, episode reward:  2.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 1.823 [0.000, 4.000],  loss: 4.716679, mae: 7.455130, mean_q: 12.309264\n",
      " 50830/100000: episode: 574, duration: 16.109s, episode steps:  93, steps per second:   6, episode reward:  2.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.323 [0.000, 4.000],  loss: 4.556736, mae: 6.854258, mean_q: 11.370202\n",
      " 50873/100000: episode: 575, duration: 7.533s, episode steps:  43, steps per second:   6, episode reward:  2.000, mean reward:  0.047 [ 0.000,  1.000], mean action: 2.395 [0.000, 4.000],  loss: 4.685849, mae: 6.576332, mean_q: 10.999227\n",
      " 50966/100000: episode: 576, duration: 16.121s, episode steps:  93, steps per second:   6, episode reward:  1.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.333 [0.000, 4.000],  loss: 4.535216, mae: 6.508391, mean_q: 10.827525\n",
      " 51007/100000: episode: 577, duration: 7.195s, episode steps:  41, steps per second:   6, episode reward:  2.000, mean reward:  0.049 [ 0.000,  1.000], mean action: 2.146 [0.000, 4.000],  loss: 4.609681, mae: 6.524101, mean_q: 10.937257\n",
      " 51091/100000: episode: 578, duration: 14.608s, episode steps:  84, steps per second:   6, episode reward:  2.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.155 [0.000, 4.000],  loss: 5.153679, mae: 6.326013, mean_q: 10.554464\n",
      " 51226/100000: episode: 579, duration: 23.322s, episode steps: 135, steps per second:   6, episode reward:  1.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.467 [0.000, 4.000],  loss: 4.459505, mae: 6.289055, mean_q: 10.498223\n",
      " 51273/100000: episode: 580, duration: 8.241s, episode steps:  47, steps per second:   6, episode reward:  1.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.511 [0.000, 4.000],  loss: 4.481987, mae: 6.519731, mean_q: 10.981753\n",
      " 51358/100000: episode: 581, duration: 14.784s, episode steps:  85, steps per second:   6, episode reward:  1.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.400 [0.000, 4.000],  loss: 4.893215, mae: 6.168036, mean_q: 10.309943\n",
      " 51582/100000: episode: 582, duration: 38.626s, episode steps: 224, steps per second:   6, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 2.156 [0.000, 4.000],  loss: 4.323290, mae: 6.384650, mean_q: 10.848162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 51636/100000: episode: 583, duration: 9.477s, episode steps:  54, steps per second:   6, episode reward:  1.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.463 [0.000, 4.000],  loss: 4.373816, mae: 6.574643, mean_q: 11.105409\n",
      " 51686/100000: episode: 584, duration: 8.751s, episode steps:  50, steps per second:   6, episode reward:  1.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.040 [0.000, 4.000],  loss: 4.617615, mae: 6.763862, mean_q: 11.403211\n",
      " 51725/100000: episode: 585, duration: 6.863s, episode steps:  39, steps per second:   6, episode reward:  1.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.821 [0.000, 4.000],  loss: 4.798488, mae: 6.993961, mean_q: 11.864976\n",
      " 51830/100000: episode: 586, duration: 18.249s, episode steps: 105, steps per second:   6, episode reward:  2.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.905 [0.000, 4.000],  loss: 4.560119, mae: 6.747497, mean_q: 11.473760\n",
      " 51902/100000: episode: 587, duration: 12.541s, episode steps:  72, steps per second:   6, episode reward:  1.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.236 [0.000, 4.000],  loss: 4.581450, mae: 6.738112, mean_q: 11.374180\n",
      " 51996/100000: episode: 588, duration: 16.295s, episode steps:  94, steps per second:   6, episode reward:  1.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.340 [0.000, 4.000],  loss: 4.706151, mae: 6.553770, mean_q: 10.984629\n",
      " 52118/100000: episode: 589, duration: 21.138s, episode steps: 122, steps per second:   6, episode reward:  2.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.221 [0.000, 4.000],  loss: 4.602591, mae: 6.869664, mean_q: 11.481400\n",
      " 52246/100000: episode: 590, duration: 22.174s, episode steps: 128, steps per second:   6, episode reward:  1.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.234 [0.000, 4.000],  loss: 4.930088, mae: 6.744874, mean_q: 11.257011\n",
      " 52287/100000: episode: 591, duration: 7.186s, episode steps:  41, steps per second:   6, episode reward:  1.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.244 [0.000, 4.000],  loss: 5.075908, mae: 6.894960, mean_q: 11.477208\n",
      " 52495/100000: episode: 592, duration: 35.900s, episode steps: 208, steps per second:   6, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.332 [0.000, 4.000],  loss: 5.106298, mae: 6.797902, mean_q: 11.264897\n",
      " 52554/100000: episode: 593, duration: 10.263s, episode steps:  59, steps per second:   6, episode reward:  1.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.339 [0.000, 4.000],  loss: 5.140431, mae: 6.851919, mean_q: 11.450735\n",
      " 52633/100000: episode: 594, duration: 13.732s, episode steps:  79, steps per second:   6, episode reward:  2.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.038 [0.000, 4.000],  loss: 4.569760, mae: 6.587062, mean_q: 11.003709\n",
      " 52694/100000: episode: 595, duration: 10.656s, episode steps:  61, steps per second:   6, episode reward:  2.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 1.902 [0.000, 4.000],  loss: 4.966805, mae: 6.509537, mean_q: 10.803055\n",
      " 52742/100000: episode: 596, duration: 8.377s, episode steps:  48, steps per second:   6, episode reward:  1.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.250 [0.000, 4.000],  loss: 3.757475, mae: 6.361284, mean_q: 10.565171\n",
      " 52942/100000: episode: 597, duration: 34.423s, episode steps: 200, steps per second:   6, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.185 [0.000, 4.000],  loss: 4.839987, mae: 6.810246, mean_q: 11.327819\n",
      " 52984/100000: episode: 598, duration: 7.365s, episode steps:  42, steps per second:   6, episode reward:  1.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.357 [0.000, 4.000],  loss: 5.103577, mae: 7.027668, mean_q: 11.732977\n",
      " 53246/100000: episode: 599, duration: 45.120s, episode steps: 262, steps per second:   6, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 2.153 [0.000, 4.000],  loss: 5.213319, mae: 7.296906, mean_q: 12.295552\n",
      " 53309/100000: episode: 600, duration: 10.994s, episode steps:  63, steps per second:   6, episode reward:  2.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 2.587 [0.000, 4.000],  loss: 4.678835, mae: 7.598591, mean_q: 12.890100\n",
      " 53346/100000: episode: 601, duration: 6.515s, episode steps:  37, steps per second:   6, episode reward:  2.000, mean reward:  0.054 [ 0.000,  1.000], mean action: 1.946 [0.000, 4.000],  loss: 5.401564, mae: 7.725009, mean_q: 13.034355\n",
      " 53408/100000: episode: 602, duration: 10.799s, episode steps:  62, steps per second:   6, episode reward:  2.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 1.726 [0.000, 4.000],  loss: 4.994090, mae: 7.689740, mean_q: 13.007528\n",
      " 53464/100000: episode: 603, duration: 9.758s, episode steps:  56, steps per second:   6, episode reward:  1.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.679 [0.000, 4.000],  loss: 5.330835, mae: 7.381757, mean_q: 12.386584\n",
      " 53556/100000: episode: 604, duration: 15.956s, episode steps:  92, steps per second:   6, episode reward:  1.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.663 [0.000, 4.000],  loss: 5.606669, mae: 7.132057, mean_q: 11.997595\n",
      " 53696/100000: episode: 605, duration: 24.178s, episode steps: 140, steps per second:   6, episode reward:  1.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.564 [0.000, 4.000],  loss: 5.487936, mae: 7.425567, mean_q: 12.690641\n",
      " 53772/100000: episode: 606, duration: 13.224s, episode steps:  76, steps per second:   6, episode reward:  1.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.092 [0.000, 4.000],  loss: 5.586503, mae: 7.797598, mean_q: 13.351790\n",
      " 53835/100000: episode: 607, duration: 10.988s, episode steps:  63, steps per second:   6, episode reward:  2.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 2.159 [0.000, 4.000],  loss: 5.462585, mae: 7.884516, mean_q: 13.438477\n",
      " 53902/100000: episode: 608, duration: 11.660s, episode steps:  67, steps per second:   6, episode reward:  1.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.149 [0.000, 4.000],  loss: 5.192742, mae: 7.463191, mean_q: 12.797483\n",
      " 53962/100000: episode: 609, duration: 10.450s, episode steps:  60, steps per second:   6, episode reward:  2.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 2.133 [0.000, 4.000],  loss: 5.025176, mae: 7.173541, mean_q: 12.173206\n",
      " 54160/100000: episode: 610, duration: 34.173s, episode steps: 198, steps per second:   6, episode reward:  3.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.187 [0.000, 4.000],  loss: 5.475120, mae: 6.834449, mean_q: 11.580709\n",
      " 54211/100000: episode: 611, duration: 8.920s, episode steps:  51, steps per second:   6, episode reward:  2.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 2.059 [0.000, 4.000],  loss: 5.038363, mae: 5.753477, mean_q: 9.693578\n",
      " 54261/100000: episode: 612, duration: 8.752s, episode steps:  50, steps per second:   6, episode reward:  1.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.220 [0.000, 4.000],  loss: 5.005900, mae: 5.686122, mean_q: 9.592652\n",
      " 54321/100000: episode: 613, duration: 10.505s, episode steps:  60, steps per second:   6, episode reward:  1.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.333 [0.000, 4.000],  loss: 4.926987, mae: 5.652599, mean_q: 9.565339\n",
      " 54628/100000: episode: 614, duration: 52.896s, episode steps: 307, steps per second:   6, episode reward:  1.000, mean reward:  0.003 [ 0.000,  1.000], mean action: 2.261 [0.000, 4.000],  loss: 4.281534, mae: 5.641775, mean_q: 9.578941\n",
      " 54714/100000: episode: 615, duration: 14.933s, episode steps:  86, steps per second:   6, episode reward:  1.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.105 [0.000, 4.000],  loss: 3.735666, mae: 5.634653, mean_q: 9.451394\n",
      " 54773/100000: episode: 616, duration: 10.295s, episode steps:  59, steps per second:   6, episode reward:  2.000, mean reward:  0.034 [ 0.000,  1.000], mean action: 2.169 [0.000, 4.000],  loss: 3.946048, mae: 5.676050, mean_q: 9.547234\n",
      " 54826/100000: episode: 617, duration: 9.284s, episode steps:  53, steps per second:   6, episode reward:  2.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 2.019 [0.000, 4.000],  loss: 3.892929, mae: 5.359531, mean_q: 9.007919\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 54895/100000: episode: 618, duration: 11.996s, episode steps:  69, steps per second:   6, episode reward:  1.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.072 [0.000, 4.000],  loss: 3.644506, mae: 5.394797, mean_q: 9.088399\n",
      " 54952/100000: episode: 619, duration: 9.939s, episode steps:  57, steps per second:   6, episode reward:  1.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.807 [0.000, 4.000],  loss: 3.356549, mae: 5.345754, mean_q: 9.019026\n",
      " 55020/100000: episode: 620, duration: 11.859s, episode steps:  68, steps per second:   6, episode reward:  2.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.088 [0.000, 4.000],  loss: 4.055529, mae: 5.651812, mean_q: 9.426076\n",
      " 55089/100000: episode: 621, duration: 12.028s, episode steps:  69, steps per second:   6, episode reward:  2.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.043 [0.000, 4.000],  loss: 3.710574, mae: 5.259604, mean_q: 8.787171\n",
      " 55358/100000: episode: 622, duration: 46.306s, episode steps: 269, steps per second:   6, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.368 [0.000, 4.000],  loss: 3.801583, mae: 5.642140, mean_q: 9.406967\n",
      " 55400/100000: episode: 623, duration: 7.397s, episode steps:  42, steps per second:   6, episode reward:  1.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.000 [0.000, 4.000],  loss: 3.427546, mae: 6.116645, mean_q: 10.298605\n",
      " 55477/100000: episode: 624, duration: 13.383s, episode steps:  77, steps per second:   6, episode reward:  1.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.532 [0.000, 4.000],  loss: 3.868834, mae: 6.327222, mean_q: 10.660131\n",
      " 55565/100000: episode: 625, duration: 15.280s, episode steps:  88, steps per second:   6, episode reward:  2.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.284 [0.000, 4.000],  loss: 4.499014, mae: 6.139422, mean_q: 10.352744\n",
      " 55610/100000: episode: 626, duration: 7.878s, episode steps:  45, steps per second:   6, episode reward:  1.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.111 [0.000, 4.000],  loss: 4.209039, mae: 5.493438, mean_q: 9.236287\n",
      " 55654/100000: episode: 627, duration: 7.725s, episode steps:  44, steps per second:   6, episode reward:  1.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.023 [0.000, 4.000],  loss: 4.219240, mae: 5.660154, mean_q: 9.514810\n",
      " 55747/100000: episode: 628, duration: 16.123s, episode steps:  93, steps per second:   6, episode reward:  1.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.839 [0.000, 4.000],  loss: 4.372447, mae: 5.866824, mean_q: 9.856157\n",
      " 55810/100000: episode: 629, duration: 10.995s, episode steps:  63, steps per second:   6, episode reward:  1.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.063 [0.000, 4.000],  loss: 4.802885, mae: 6.157009, mean_q: 10.380978\n",
      " 55913/100000: episode: 630, duration: 17.873s, episode steps: 103, steps per second:   6, episode reward:  2.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.175 [0.000, 4.000],  loss: 4.046394, mae: 6.133542, mean_q: 10.372070\n",
      " 55987/100000: episode: 631, duration: 12.858s, episode steps:  74, steps per second:   6, episode reward:  2.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.095 [0.000, 4.000],  loss: 4.194098, mae: 5.856737, mean_q: 9.968472\n",
      " 56094/100000: episode: 632, duration: 18.528s, episode steps: 107, steps per second:   6, episode reward:  1.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.243 [0.000, 4.000],  loss: 5.045186, mae: 6.088329, mean_q: 10.282287\n",
      " 56148/100000: episode: 633, duration: 9.415s, episode steps:  54, steps per second:   6, episode reward:  1.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.778 [0.000, 4.000],  loss: 6.192115, mae: 7.001556, mean_q: 11.838807\n",
      " 56189/100000: episode: 634, duration: 7.224s, episode steps:  41, steps per second:   6, episode reward:  1.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.098 [0.000, 4.000],  loss: 4.911545, mae: 6.997741, mean_q: 11.815615\n",
      " 56248/100000: episode: 635, duration: 10.291s, episode steps:  59, steps per second:   6, episode reward:  2.000, mean reward:  0.034 [ 0.000,  1.000], mean action: 2.119 [0.000, 4.000],  loss: 6.310175, mae: 6.786772, mean_q: 11.400731\n",
      " 56311/100000: episode: 636, duration: 11.004s, episode steps:  63, steps per second:   6, episode reward:  1.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.127 [0.000, 4.000],  loss: 5.809196, mae: 6.388061, mean_q: 10.681262\n",
      " 56372/100000: episode: 637, duration: 10.658s, episode steps:  61, steps per second:   6, episode reward:  2.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 1.967 [0.000, 4.000],  loss: 6.169639, mae: 6.468235, mean_q: 10.717764\n",
      " 56435/100000: episode: 638, duration: 10.969s, episode steps:  63, steps per second:   6, episode reward:  2.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 2.333 [0.000, 4.000],  loss: 5.678627, mae: 6.174073, mean_q: 10.280573\n",
      " 56546/100000: episode: 639, duration: 19.275s, episode steps: 111, steps per second:   6, episode reward:  2.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.333 [0.000, 4.000],  loss: 5.895293, mae: 6.162498, mean_q: 10.292685\n",
      " 56660/100000: episode: 640, duration: 19.795s, episode steps: 114, steps per second:   6, episode reward:  1.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.956 [0.000, 4.000],  loss: 5.264426, mae: 6.492679, mean_q: 10.856858\n",
      " 56743/100000: episode: 641, duration: 14.430s, episode steps:  83, steps per second:   6, episode reward:  2.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.964 [0.000, 4.000],  loss: 4.959248, mae: 6.226426, mean_q: 10.354033\n",
      " 56804/100000: episode: 642, duration: 10.648s, episode steps:  61, steps per second:   6, episode reward:  2.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 2.344 [0.000, 4.000],  loss: 3.903771, mae: 6.298356, mean_q: 10.550561\n",
      " 56883/100000: episode: 643, duration: 13.765s, episode steps:  79, steps per second:   6, episode reward:  3.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 2.051 [0.000, 4.000],  loss: 4.664156, mae: 5.987411, mean_q: 9.992985\n",
      " 56939/100000: episode: 644, duration: 9.812s, episode steps:  56, steps per second:   6, episode reward:  2.000, mean reward:  0.036 [ 0.000,  1.000], mean action: 1.786 [0.000, 4.000],  loss: 4.326962, mae: 6.166112, mean_q: 10.292604\n",
      " 57035/100000: episode: 645, duration: 16.664s, episode steps:  96, steps per second:   6, episode reward:  1.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.260 [0.000, 4.000],  loss: 4.720841, mae: 6.300996, mean_q: 10.498183\n",
      " 57073/100000: episode: 646, duration: 6.709s, episode steps:  38, steps per second:   6, episode reward:  2.000, mean reward:  0.053 [ 0.000,  1.000], mean action: 2.553 [0.000, 4.000],  loss: 4.039191, mae: 6.298697, mean_q: 10.438802\n",
      " 57219/100000: episode: 647, duration: 25.235s, episode steps: 146, steps per second:   6, episode reward:  2.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.438 [0.000, 4.000],  loss: 4.799195, mae: 6.100134, mean_q: 10.175241\n",
      " 57291/100000: episode: 648, duration: 12.546s, episode steps:  72, steps per second:   6, episode reward:  1.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.319 [0.000, 4.000],  loss: 4.044805, mae: 6.422456, mean_q: 10.767849\n",
      " 57327/100000: episode: 649, duration: 6.330s, episode steps:  36, steps per second:   6, episode reward:  1.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.083 [0.000, 4.000],  loss: 5.736541, mae: 6.765853, mean_q: 11.118562\n",
      " 57386/100000: episode: 650, duration: 10.304s, episode steps:  59, steps per second:   6, episode reward:  1.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.644 [0.000, 4.000],  loss: 4.686233, mae: 6.620980, mean_q: 11.067350\n",
      " 57424/100000: episode: 651, duration: 6.693s, episode steps:  38, steps per second:   6, episode reward:  2.000, mean reward:  0.053 [ 0.000,  1.000], mean action: 2.342 [0.000, 4.000],  loss: 4.590458, mae: 6.267462, mean_q: 10.515769\n",
      " 57546/100000: episode: 652, duration: 21.147s, episode steps: 122, steps per second:   6, episode reward:  1.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.672 [0.000, 4.000],  loss: 5.060808, mae: 6.633525, mean_q: 11.125196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 57686/100000: episode: 653, duration: 24.257s, episode steps: 140, steps per second:   6, episode reward:  2.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.079 [0.000, 4.000],  loss: 4.890100, mae: 6.856921, mean_q: 11.513121\n",
      " 57733/100000: episode: 654, duration: 8.251s, episode steps:  47, steps per second:   6, episode reward:  1.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.915 [0.000, 4.000],  loss: 5.261283, mae: 7.504519, mean_q: 12.538408\n",
      " 57843/100000: episode: 655, duration: 19.038s, episode steps: 110, steps per second:   6, episode reward:  1.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.336 [0.000, 4.000],  loss: 5.954219, mae: 7.667253, mean_q: 12.858974\n",
      " 57930/100000: episode: 656, duration: 15.111s, episode steps:  87, steps per second:   6, episode reward:  1.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.322 [0.000, 4.000],  loss: 6.533198, mae: 8.079665, mean_q: 13.653781\n",
      " 57972/100000: episode: 657, duration: 7.399s, episode steps:  42, steps per second:   6, episode reward:  2.000, mean reward:  0.048 [ 0.000,  1.000], mean action: 2.095 [0.000, 4.000],  loss: 8.982054, mae: 8.281282, mean_q: 13.971039\n",
      " 58055/100000: episode: 658, duration: 14.431s, episode steps:  83, steps per second:   6, episode reward:  1.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.386 [0.000, 4.000],  loss: 7.548085, mae: 8.029617, mean_q: 13.599994\n",
      " 58099/100000: episode: 659, duration: 7.697s, episode steps:  44, steps per second:   6, episode reward:  1.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.068 [0.000, 4.000],  loss: 7.795932, mae: 8.161345, mean_q: 13.911298\n",
      " 58153/100000: episode: 660, duration: 9.424s, episode steps:  54, steps per second:   6, episode reward:  1.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.833 [0.000, 4.000],  loss: 6.780445, mae: 8.007452, mean_q: 13.636898\n",
      " 58192/100000: episode: 661, duration: 6.891s, episode steps:  39, steps per second:   6, episode reward:  1.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.385 [0.000, 4.000],  loss: 6.587689, mae: 8.011409, mean_q: 13.755469\n",
      " 58344/100000: episode: 662, duration: 26.277s, episode steps: 152, steps per second:   6, episode reward:  2.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.467 [0.000, 4.000],  loss: 6.635436, mae: 7.814149, mean_q: 13.393047\n",
      " 58525/100000: episode: 663, duration: 31.218s, episode steps: 181, steps per second:   6, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.309 [0.000, 4.000],  loss: 6.200700, mae: 8.384799, mean_q: 14.238313\n",
      " 58600/100000: episode: 664, duration: 13.044s, episode steps:  75, steps per second:   6, episode reward:  1.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.667 [0.000, 4.000],  loss: 6.352116, mae: 8.797707, mean_q: 14.801203\n",
      " 58694/100000: episode: 665, duration: 16.386s, episode steps:  94, steps per second:   6, episode reward:  1.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.330 [0.000, 4.000],  loss: 6.693630, mae: 8.489379, mean_q: 14.347468\n",
      " 58752/100000: episode: 666, duration: 10.122s, episode steps:  58, steps per second:   6, episode reward:  1.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.293 [0.000, 4.000],  loss: 7.217943, mae: 8.290304, mean_q: 13.987704\n",
      " 58922/100000: episode: 667, duration: 29.326s, episode steps: 170, steps per second:   6, episode reward:  2.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.341 [0.000, 4.000],  loss: 6.647599, mae: 8.251780, mean_q: 13.881827\n",
      " 59074/100000: episode: 668, duration: 26.183s, episode steps: 152, steps per second:   6, episode reward:  3.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.033 [0.000, 4.000],  loss: 5.488616, mae: 8.139914, mean_q: 13.562696\n",
      " 59251/100000: episode: 669, duration: 30.502s, episode steps: 177, steps per second:   6, episode reward:  2.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.130 [0.000, 4.000],  loss: 5.654098, mae: 8.036404, mean_q: 13.299112\n",
      " 59330/100000: episode: 670, duration: 13.694s, episode steps:  79, steps per second:   6, episode reward:  1.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.519 [0.000, 4.000],  loss: 4.833312, mae: 8.058135, mean_q: 13.282550\n",
      " 59399/100000: episode: 671, duration: 11.999s, episode steps:  69, steps per second:   6, episode reward:  1.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.493 [0.000, 4.000],  loss: 5.417136, mae: 8.185369, mean_q: 13.473637\n",
      " 59448/100000: episode: 672, duration: 8.529s, episode steps:  49, steps per second:   6, episode reward:  2.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 2.224 [0.000, 4.000],  loss: 4.824919, mae: 7.823667, mean_q: 12.985584\n",
      " 59771/100000: episode: 673, duration: 55.486s, episode steps: 323, steps per second:   6, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.257 [0.000, 4.000],  loss: 4.301196, mae: 7.275382, mean_q: 12.107385\n",
      " 59828/100000: episode: 674, duration: 9.915s, episode steps:  57, steps per second:   6, episode reward:  2.000, mean reward:  0.035 [ 0.000,  1.000], mean action: 1.842 [0.000, 4.000],  loss: 3.923245, mae: 7.161060, mean_q: 11.900451\n",
      " 59891/100000: episode: 675, duration: 10.979s, episode steps:  63, steps per second:   6, episode reward:  2.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 2.206 [0.000, 4.000],  loss: 3.736042, mae: 6.777379, mean_q: 11.298663\n",
      " 59973/100000: episode: 676, duration: 14.221s, episode steps:  82, steps per second:   6, episode reward:  2.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.256 [0.000, 4.000],  loss: 3.102686, mae: 6.488644, mean_q: 10.930887\n",
      " 60067/100000: episode: 677, duration: 16.306s, episode steps:  94, steps per second:   6, episode reward:  1.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.521 [0.000, 4.000],  loss: 3.608283, mae: 6.735128, mean_q: 11.307796\n",
      " 60194/100000: episode: 678, duration: 21.923s, episode steps: 127, steps per second:   6, episode reward:  1.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.402 [0.000, 4.000],  loss: 3.992505, mae: 6.905638, mean_q: 11.613283\n",
      " 60267/100000: episode: 679, duration: 12.698s, episode steps:  73, steps per second:   6, episode reward:  1.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.041 [0.000, 4.000],  loss: 4.211878, mae: 7.191335, mean_q: 12.147003\n",
      " 60350/100000: episode: 680, duration: 14.406s, episode steps:  83, steps per second:   6, episode reward:  1.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.313 [0.000, 4.000],  loss: 4.403123, mae: 6.919753, mean_q: 11.730515\n",
      " 60567/100000: episode: 681, duration: 37.355s, episode steps: 217, steps per second:   6, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.346 [0.000, 4.000],  loss: 3.875601, mae: 7.039681, mean_q: 11.860979\n",
      " 60804/100000: episode: 682, duration: 40.868s, episode steps: 237, steps per second:   6, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 2.274 [0.000, 4.000],  loss: 4.514278, mae: 7.376362, mean_q: 12.569416\n",
      " 60904/100000: episode: 683, duration: 17.310s, episode steps: 100, steps per second:   6, episode reward:  1.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.110 [0.000, 4.000],  loss: 5.144152, mae: 8.243392, mean_q: 13.970165\n",
      " 61038/100000: episode: 684, duration: 23.119s, episode steps: 134, steps per second:   6, episode reward:  2.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.291 [0.000, 4.000],  loss: 4.757563, mae: 8.111140, mean_q: 13.801138\n",
      " 61109/100000: episode: 685, duration: 12.352s, episode steps:  71, steps per second:   6, episode reward:  2.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.085 [0.000, 4.000],  loss: 4.558674, mae: 7.994967, mean_q: 13.522180\n",
      " 61172/100000: episode: 686, duration: 10.961s, episode steps:  63, steps per second:   6, episode reward:  1.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.905 [0.000, 4.000],  loss: 4.091755, mae: 7.591792, mean_q: 12.914054\n",
      " 61227/100000: episode: 687, duration: 9.588s, episode steps:  55, steps per second:   6, episode reward:  1.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.291 [0.000, 4.000],  loss: 4.159523, mae: 7.205323, mean_q: 12.281045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 61401/100000: episode: 688, duration: 29.981s, episode steps: 174, steps per second:   6, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.339 [0.000, 4.000],  loss: 4.461294, mae: 7.552257, mean_q: 12.732701\n",
      " 61445/100000: episode: 689, duration: 7.719s, episode steps:  44, steps per second:   6, episode reward:  2.000, mean reward:  0.045 [ 0.000,  1.000], mean action: 1.977 [0.000, 4.000],  loss: 5.433994, mae: 8.038675, mean_q: 13.591266\n",
      " 61487/100000: episode: 690, duration: 7.365s, episode steps:  42, steps per second:   6, episode reward:  1.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.881 [0.000, 4.000],  loss: 5.831982, mae: 7.899106, mean_q: 13.335277\n",
      " 61698/100000: episode: 691, duration: 36.403s, episode steps: 211, steps per second:   6, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 2.227 [0.000, 4.000],  loss: 5.733128, mae: 7.804675, mean_q: 12.914020\n",
      " 61818/100000: episode: 692, duration: 20.766s, episode steps: 120, steps per second:   6, episode reward:  1.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.308 [0.000, 4.000],  loss: 6.133029, mae: 7.767378, mean_q: 12.803411\n",
      " 61887/100000: episode: 693, duration: 12.010s, episode steps:  69, steps per second:   6, episode reward:  1.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.275 [0.000, 4.000],  loss: 5.491876, mae: 7.437270, mean_q: 12.299507\n",
      " 61926/100000: episode: 694, duration: 6.879s, episode steps:  39, steps per second:   6, episode reward:  1.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.410 [0.000, 4.000],  loss: 5.882332, mae: 7.831816, mean_q: 13.018546\n",
      " 61967/100000: episode: 695, duration: 7.234s, episode steps:  41, steps per second:   6, episode reward:  1.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.024 [0.000, 4.000],  loss: 6.436558, mae: 7.761125, mean_q: 12.791976\n",
      " 62021/100000: episode: 696, duration: 9.441s, episode steps:  54, steps per second:   6, episode reward:  3.000, mean reward:  0.056 [ 0.000,  1.000], mean action: 2.315 [0.000, 4.000],  loss: 6.313941, mae: 7.617353, mean_q: 12.672166\n",
      " 62179/100000: episode: 697, duration: 27.369s, episode steps: 158, steps per second:   6, episode reward:  2.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.013 [0.000, 4.000],  loss: 6.246211, mae: 7.445381, mean_q: 12.407932\n",
      " 62285/100000: episode: 698, duration: 18.383s, episode steps: 106, steps per second:   6, episode reward:  1.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.538 [0.000, 4.000],  loss: 5.845903, mae: 7.741279, mean_q: 12.876038\n",
      " 62360/100000: episode: 699, duration: 13.073s, episode steps:  75, steps per second:   6, episode reward:  3.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 2.000 [0.000, 4.000],  loss: 5.920875, mae: 7.466304, mean_q: 12.470236\n",
      " 62416/100000: episode: 700, duration: 9.776s, episode steps:  56, steps per second:   6, episode reward:  2.000, mean reward:  0.036 [ 0.000,  1.000], mean action: 2.321 [0.000, 4.000],  loss: 5.993831, mae: 6.622859, mean_q: 10.996411\n",
      " 62465/100000: episode: 701, duration: 8.591s, episode steps:  49, steps per second:   6, episode reward:  1.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.592 [0.000, 4.000],  loss: 5.190669, mae: 6.536626, mean_q: 10.880866\n",
      " 62510/100000: episode: 702, duration: 7.910s, episode steps:  45, steps per second:   6, episode reward:  1.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.444 [0.000, 4.000],  loss: 5.198739, mae: 6.655672, mean_q: 11.068939\n",
      " 62569/100000: episode: 703, duration: 10.312s, episode steps:  59, steps per second:   6, episode reward:  1.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.220 [0.000, 4.000],  loss: 4.362105, mae: 6.060329, mean_q: 10.130970\n",
      " 62606/100000: episode: 704, duration: 6.526s, episode steps:  37, steps per second:   6, episode reward:  1.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.162 [0.000, 4.000],  loss: 5.075747, mae: 5.917980, mean_q: 9.976934\n",
      " 62682/100000: episode: 705, duration: 13.268s, episode steps:  76, steps per second:   6, episode reward:  2.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.737 [0.000, 4.000],  loss: 4.742726, mae: 5.446984, mean_q: 9.101913\n",
      " 62730/100000: episode: 706, duration: 8.427s, episode steps:  48, steps per second:   6, episode reward:  1.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.208 [0.000, 4.000],  loss: 4.932959, mae: 4.994024, mean_q: 8.318378\n",
      " 62770/100000: episode: 707, duration: 7.049s, episode steps:  40, steps per second:   6, episode reward:  1.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.300 [0.000, 4.000],  loss: 4.539132, mae: 5.203843, mean_q: 8.689981\n",
      " 62860/100000: episode: 708, duration: 15.686s, episode steps:  90, steps per second:   6, episode reward:  2.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.989 [0.000, 4.000],  loss: 4.744946, mae: 5.080156, mean_q: 8.381474\n",
      " 62904/100000: episode: 709, duration: 7.747s, episode steps:  44, steps per second:   6, episode reward:  1.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.091 [0.000, 4.000],  loss: 4.421469, mae: 4.770087, mean_q: 7.850592\n",
      " 63120/100000: episode: 710, duration: 37.289s, episode steps: 216, steps per second:   6, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 2.306 [0.000, 4.000],  loss: 4.579037, mae: 5.050498, mean_q: 8.359982\n",
      " 63189/100000: episode: 711, duration: 12.095s, episode steps:  69, steps per second:   6, episode reward:  2.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.261 [0.000, 4.000],  loss: 5.171829, mae: 5.147010, mean_q: 8.667582\n",
      " 63254/100000: episode: 712, duration: 11.335s, episode steps:  65, steps per second:   6, episode reward:  1.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.246 [0.000, 4.000],  loss: 4.522476, mae: 5.176374, mean_q: 8.688762\n",
      " 63303/100000: episode: 713, duration: 8.612s, episode steps:  49, steps per second:   6, episode reward:  1.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.714 [0.000, 4.000],  loss: 4.292553, mae: 4.998440, mean_q: 8.354722\n",
      " 63353/100000: episode: 714, duration: 8.765s, episode steps:  50, steps per second:   6, episode reward:  1.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.800 [0.000, 4.000],  loss: 5.003292, mae: 5.530818, mean_q: 9.199368\n",
      " 63401/100000: episode: 715, duration: 8.410s, episode steps:  48, steps per second:   6, episode reward:  1.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.104 [0.000, 4.000],  loss: 4.508644, mae: 5.593610, mean_q: 9.361169\n",
      " 63569/100000: episode: 716, duration: 29.082s, episode steps: 168, steps per second:   6, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.345 [0.000, 4.000],  loss: 4.651773, mae: 5.735957, mean_q: 9.524315\n",
      " 63661/100000: episode: 717, duration: 16.001s, episode steps:  92, steps per second:   6, episode reward:  1.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.141 [0.000, 4.000],  loss: 4.887413, mae: 6.334076, mean_q: 10.412671\n",
      " 63695/100000: episode: 718, duration: 6.017s, episode steps:  34, steps per second:   6, episode reward:  1.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.294 [0.000, 4.000],  loss: 5.346779, mae: 6.840840, mean_q: 11.240539\n",
      " 63737/100000: episode: 719, duration: 7.378s, episode steps:  42, steps per second:   6, episode reward:  1.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.095 [0.000, 4.000],  loss: 5.520411, mae: 6.632763, mean_q: 10.969808\n",
      " 64019/100000: episode: 720, duration: 48.571s, episode steps: 282, steps per second:   6, episode reward:  3.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.457 [0.000, 4.000],  loss: 5.710494, mae: 7.199082, mean_q: 11.949505\n",
      " 64101/100000: episode: 721, duration: 14.266s, episode steps:  82, steps per second:   6, episode reward:  2.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.902 [0.000, 4.000],  loss: 4.886207, mae: 7.201376, mean_q: 12.053892\n",
      " 64169/100000: episode: 722, duration: 11.854s, episode steps:  68, steps per second:   6, episode reward:  1.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.956 [0.000, 4.000],  loss: 4.502996, mae: 7.210388, mean_q: 12.056703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 64286/100000: episode: 723, duration: 20.268s, episode steps: 117, steps per second:   6, episode reward:  1.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.265 [0.000, 4.000],  loss: 4.956530, mae: 7.072650, mean_q: 11.758455\n",
      " 64340/100000: episode: 724, duration: 9.451s, episode steps:  54, steps per second:   6, episode reward:  3.000, mean reward:  0.056 [ 0.000,  1.000], mean action: 2.037 [0.000, 4.000],  loss: 4.863966, mae: 7.096488, mean_q: 11.824705\n",
      " 64425/100000: episode: 725, duration: 14.754s, episode steps:  85, steps per second:   6, episode reward:  1.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.400 [0.000, 4.000],  loss: 5.468111, mae: 7.232872, mean_q: 12.095207\n",
      " 64471/100000: episode: 726, duration: 8.052s, episode steps:  46, steps per second:   6, episode reward:  1.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.261 [0.000, 4.000],  loss: 5.667532, mae: 7.291343, mean_q: 12.324327\n",
      " 64512/100000: episode: 727, duration: 7.213s, episode steps:  41, steps per second:   6, episode reward:  2.000, mean reward:  0.049 [ 0.000,  1.000], mean action: 2.195 [0.000, 4.000],  loss: 6.322036, mae: 7.339981, mean_q: 12.199392\n",
      " 64577/100000: episode: 728, duration: 11.341s, episode steps:  65, steps per second:   6, episode reward:  1.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.646 [0.000, 4.000],  loss: 5.463583, mae: 6.908871, mean_q: 11.593385\n",
      " 64624/100000: episode: 729, duration: 8.256s, episode steps:  47, steps per second:   6, episode reward:  1.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.191 [0.000, 4.000],  loss: 6.093916, mae: 6.892050, mean_q: 11.607231\n",
      " 64712/100000: episode: 730, duration: 15.304s, episode steps:  88, steps per second:   6, episode reward:  2.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.239 [0.000, 4.000],  loss: 5.222088, mae: 6.445132, mean_q: 10.879572\n",
      " 64769/100000: episode: 731, duration: 9.974s, episode steps:  57, steps per second:   6, episode reward:  2.000, mean reward:  0.035 [ 0.000,  1.000], mean action: 1.860 [0.000, 4.000],  loss: 4.556262, mae: 6.318186, mean_q: 10.649901\n",
      " 64916/100000: episode: 732, duration: 25.444s, episode steps: 147, steps per second:   6, episode reward:  1.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.265 [0.000, 4.000],  loss: 4.503575, mae: 6.039281, mean_q: 10.061364\n",
      " 65004/100000: episode: 733, duration: 15.300s, episode steps:  88, steps per second:   6, episode reward:  1.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.330 [0.000, 4.000],  loss: 5.083827, mae: 6.017051, mean_q: 10.103302\n",
      " 65051/100000: episode: 734, duration: 8.433s, episode steps:  47, steps per second:   6, episode reward:  1.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.106 [0.000, 4.000],  loss: 4.498283, mae: 6.131711, mean_q: 10.373446\n",
      " 65108/100000: episode: 735, duration: 9.981s, episode steps:  57, steps per second:   6, episode reward:  2.000, mean reward:  0.035 [ 0.000,  1.000], mean action: 2.614 [0.000, 4.000],  loss: 4.344122, mae: 5.970483, mean_q: 10.070333\n",
      " 65178/100000: episode: 736, duration: 12.205s, episode steps:  70, steps per second:   6, episode reward:  2.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.086 [0.000, 4.000],  loss: 5.185679, mae: 6.107201, mean_q: 10.233338\n",
      " 65381/100000: episode: 737, duration: 35.060s, episode steps: 203, steps per second:   6, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 2.241 [0.000, 4.000],  loss: 4.807284, mae: 6.350346, mean_q: 10.687314\n",
      " 65543/100000: episode: 738, duration: 28.010s, episode steps: 162, steps per second:   6, episode reward:  2.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.235 [0.000, 4.000],  loss: 4.552288, mae: 6.648098, mean_q: 11.232541\n",
      " 65662/100000: episode: 739, duration: 20.608s, episode steps: 119, steps per second:   6, episode reward:  3.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.353 [0.000, 4.000],  loss: 4.412810, mae: 6.444313, mean_q: 10.958310\n",
      " 65716/100000: episode: 740, duration: 9.446s, episode steps:  54, steps per second:   6, episode reward:  1.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.148 [0.000, 4.000],  loss: 4.961356, mae: 6.418331, mean_q: 10.997982\n",
      " 65765/100000: episode: 741, duration: 8.566s, episode steps:  49, steps per second:   6, episode reward:  2.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 2.184 [0.000, 4.000],  loss: 4.813038, mae: 6.525017, mean_q: 11.165023\n",
      " 65933/100000: episode: 742, duration: 29.069s, episode steps: 168, steps per second:   6, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.446 [0.000, 4.000],  loss: 4.669193, mae: 6.692781, mean_q: 11.440877\n",
      " 65985/100000: episode: 743, duration: 9.217s, episode steps:  52, steps per second:   6, episode reward:  1.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.654 [0.000, 4.000],  loss: 4.905819, mae: 7.071718, mean_q: 12.008378\n",
      " 66060/100000: episode: 744, duration: 13.053s, episode steps:  75, steps per second:   6, episode reward:  1.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.573 [0.000, 4.000],  loss: 4.397862, mae: 6.758760, mean_q: 11.558195\n",
      " 66111/100000: episode: 745, duration: 8.927s, episode steps:  51, steps per second:   6, episode reward:  1.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.980 [0.000, 4.000],  loss: 4.628727, mae: 6.818827, mean_q: 11.641723\n",
      " 66157/100000: episode: 746, duration: 8.057s, episode steps:  46, steps per second:   6, episode reward:  1.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.087 [0.000, 4.000],  loss: 4.524457, mae: 6.915290, mean_q: 11.797184\n",
      " 66199/100000: episode: 747, duration: 7.390s, episode steps:  42, steps per second:   6, episode reward:  1.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.952 [0.000, 4.000],  loss: 4.314698, mae: 6.612718, mean_q: 11.285213\n",
      " 66243/100000: episode: 748, duration: 7.747s, episode steps:  44, steps per second:   6, episode reward:  1.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.455 [0.000, 4.000],  loss: 4.972631, mae: 6.535081, mean_q: 11.202443\n",
      " 66284/100000: episode: 749, duration: 7.208s, episode steps:  41, steps per second:   6, episode reward:  2.000, mean reward:  0.049 [ 0.000,  1.000], mean action: 2.220 [0.000, 4.000],  loss: 4.483391, mae: 6.210887, mean_q: 10.659623\n",
      " 66419/100000: episode: 750, duration: 23.364s, episode steps: 135, steps per second:   6, episode reward:  1.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.667 [0.000, 4.000],  loss: 4.647076, mae: 6.030352, mean_q: 10.317518\n",
      " 66476/100000: episode: 751, duration: 9.972s, episode steps:  57, steps per second:   6, episode reward:  1.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.053 [0.000, 4.000],  loss: 4.889506, mae: 6.369102, mean_q: 10.913706\n",
      " 66550/100000: episode: 752, duration: 12.912s, episode steps:  74, steps per second:   6, episode reward:  1.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.527 [0.000, 4.000],  loss: 5.141133, mae: 6.438635, mean_q: 11.076816\n",
      " 66662/100000: episode: 753, duration: 19.444s, episode steps: 112, steps per second:   6, episode reward:  1.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.634 [0.000, 4.000],  loss: 5.039834, mae: 6.773342, mean_q: 11.657686\n",
      " 66742/100000: episode: 754, duration: 13.901s, episode steps:  80, steps per second:   6, episode reward:  1.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.513 [0.000, 4.000],  loss: 5.243959, mae: 7.143616, mean_q: 12.307973\n",
      " 66805/100000: episode: 755, duration: 10.974s, episode steps:  63, steps per second:   6, episode reward:  1.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.397 [0.000, 4.000],  loss: 6.186551, mae: 7.332232, mean_q: 12.610629\n",
      " 66843/100000: episode: 756, duration: 6.698s, episode steps:  38, steps per second:   6, episode reward:  1.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.342 [0.000, 4.000],  loss: 6.027781, mae: 7.327475, mean_q: 12.655347\n",
      " 66921/100000: episode: 757, duration: 13.595s, episode steps:  78, steps per second:   6, episode reward:  1.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.192 [0.000, 4.000],  loss: 5.530083, mae: 6.800453, mean_q: 11.683002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 66979/100000: episode: 758, duration: 10.165s, episode steps:  58, steps per second:   6, episode reward:  2.000, mean reward:  0.034 [ 0.000,  1.000], mean action: 1.914 [0.000, 4.000],  loss: 5.997587, mae: 6.874319, mean_q: 11.803789\n",
      " 67065/100000: episode: 759, duration: 15.013s, episode steps:  86, steps per second:   6, episode reward:  2.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.267 [0.000, 4.000],  loss: 6.190030, mae: 6.729432, mean_q: 11.441424\n",
      " 67127/100000: episode: 760, duration: 10.836s, episode steps:  62, steps per second:   6, episode reward:  1.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.323 [0.000, 4.000],  loss: 6.448819, mae: 6.778102, mean_q: 11.604216\n",
      " 67197/100000: episode: 761, duration: 12.203s, episode steps:  70, steps per second:   6, episode reward:  2.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.443 [0.000, 4.000],  loss: 5.709737, mae: 6.632299, mean_q: 11.409390\n",
      " 67257/100000: episode: 762, duration: 10.504s, episode steps:  60, steps per second:   6, episode reward:  1.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.600 [0.000, 4.000],  loss: 5.332777, mae: 6.655438, mean_q: 11.493623\n",
      " 67352/100000: episode: 763, duration: 16.482s, episode steps:  95, steps per second:   6, episode reward:  1.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.158 [0.000, 4.000],  loss: 6.232853, mae: 6.753342, mean_q: 11.669119\n",
      " 67410/100000: episode: 764, duration: 10.180s, episode steps:  58, steps per second:   6, episode reward:  1.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.276 [0.000, 4.000],  loss: 5.374441, mae: 6.586401, mean_q: 11.233306\n",
      " 67449/100000: episode: 765, duration: 6.875s, episode steps:  39, steps per second:   6, episode reward:  1.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.333 [0.000, 4.000],  loss: 5.444172, mae: 6.624631, mean_q: 11.340096\n",
      " 67492/100000: episode: 766, duration: 7.577s, episode steps:  43, steps per second:   6, episode reward:  1.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.302 [0.000, 4.000],  loss: 6.205099, mae: 6.622282, mean_q: 11.314461\n",
      " 67540/100000: episode: 767, duration: 8.440s, episode steps:  48, steps per second:   6, episode reward:  1.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.833 [0.000, 4.000],  loss: 5.238997, mae: 6.432939, mean_q: 10.964600\n",
      " 67591/100000: episode: 768, duration: 8.968s, episode steps:  51, steps per second:   6, episode reward:  1.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.314 [0.000, 4.000],  loss: 5.564493, mae: 6.378263, mean_q: 10.884887\n",
      " 67742/100000: episode: 769, duration: 26.233s, episode steps: 151, steps per second:   6, episode reward:  2.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.417 [0.000, 4.000],  loss: 5.582067, mae: 6.080026, mean_q: 10.346917\n",
      " 67799/100000: episode: 770, duration: 9.978s, episode steps:  57, steps per second:   6, episode reward:  2.000, mean reward:  0.035 [ 0.000,  1.000], mean action: 2.158 [0.000, 4.000],  loss: 4.679743, mae: 5.915781, mean_q: 10.007410\n",
      " 68012/100000: episode: 771, duration: 36.797s, episode steps: 213, steps per second:   6, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.624 [0.000, 4.000],  loss: 4.669256, mae: 6.000644, mean_q: 10.220912\n",
      " 68059/100000: episode: 772, duration: 8.223s, episode steps:  47, steps per second:   6, episode reward:  1.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.957 [0.000, 4.000],  loss: 4.427241, mae: 6.224067, mean_q: 10.728123\n",
      " 68181/100000: episode: 773, duration: 21.144s, episode steps: 122, steps per second:   6, episode reward:  2.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.385 [0.000, 4.000],  loss: 4.630208, mae: 6.338336, mean_q: 10.902383\n",
      " 68300/100000: episode: 774, duration: 20.602s, episode steps: 119, steps per second:   6, episode reward:  1.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.647 [0.000, 4.000],  loss: 5.125226, mae: 6.505937, mean_q: 11.183725\n",
      " 68344/100000: episode: 775, duration: 7.770s, episode steps:  44, steps per second:   6, episode reward:  1.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.409 [0.000, 4.000],  loss: 4.452724, mae: 6.422453, mean_q: 11.122109\n",
      " 68381/100000: episode: 776, duration: 6.520s, episode steps:  37, steps per second:   6, episode reward:  1.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.081 [0.000, 4.000],  loss: 4.611848, mae: 6.452024, mean_q: 11.121531\n",
      " 68431/100000: episode: 777, duration: 8.745s, episode steps:  50, steps per second:   6, episode reward:  2.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 2.500 [0.000, 4.000],  loss: 4.697241, mae: 6.190303, mean_q: 10.710714\n",
      " 68605/100000: episode: 778, duration: 30.056s, episode steps: 174, steps per second:   6, episode reward:  3.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.132 [0.000, 4.000],  loss: 4.007422, mae: 6.212732, mean_q: 10.728112\n",
      " 68650/100000: episode: 779, duration: 7.902s, episode steps:  45, steps per second:   6, episode reward:  1.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.956 [0.000, 4.000],  loss: 3.461325, mae: 6.048802, mean_q: 10.460006\n",
      " 68703/100000: episode: 780, duration: 9.258s, episode steps:  53, steps per second:   6, episode reward:  2.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 2.302 [0.000, 4.000],  loss: 3.948326, mae: 6.106832, mean_q: 10.466551\n",
      " 68792/100000: episode: 781, duration: 15.449s, episode steps:  89, steps per second:   6, episode reward:  2.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.191 [0.000, 4.000],  loss: 4.220370, mae: 6.015929, mean_q: 10.274477\n",
      " 68846/100000: episode: 782, duration: 9.433s, episode steps:  54, steps per second:   6, episode reward:  1.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.426 [0.000, 4.000],  loss: 4.331553, mae: 6.145804, mean_q: 10.532764\n",
      " 68892/100000: episode: 783, duration: 8.065s, episode steps:  46, steps per second:   6, episode reward:  1.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.130 [0.000, 4.000],  loss: 4.357263, mae: 6.056164, mean_q: 10.466484\n",
      " 68930/100000: episode: 784, duration: 6.697s, episode steps:  38, steps per second:   6, episode reward:  1.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.579 [0.000, 4.000],  loss: 4.586502, mae: 6.312114, mean_q: 10.786843\n",
      " 68971/100000: episode: 785, duration: 7.236s, episode steps:  41, steps per second:   6, episode reward:  2.000, mean reward:  0.049 [ 0.000,  1.000], mean action: 2.195 [0.000, 4.000],  loss: 4.177247, mae: 5.717715, mean_q: 9.778350\n",
      " 69011/100000: episode: 786, duration: 7.071s, episode steps:  40, steps per second:   6, episode reward:  1.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.400 [0.000, 4.000],  loss: 4.518651, mae: 5.672384, mean_q: 9.561079\n",
      " 69052/100000: episode: 787, duration: 7.237s, episode steps:  41, steps per second:   6, episode reward:  1.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.146 [0.000, 4.000],  loss: 4.761965, mae: 5.860429, mean_q: 9.889454\n",
      " 69146/100000: episode: 788, duration: 16.366s, episode steps:  94, steps per second:   6, episode reward:  2.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.277 [0.000, 4.000],  loss: 4.616141, mae: 5.832625, mean_q: 9.889920\n",
      " 69249/100000: episode: 789, duration: 17.907s, episode steps: 103, steps per second:   6, episode reward:  1.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.078 [0.000, 4.000],  loss: 4.649334, mae: 5.767828, mean_q: 9.732171\n",
      " 69342/100000: episode: 790, duration: 16.162s, episode steps:  93, steps per second:   6, episode reward:  1.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.258 [0.000, 4.000],  loss: 4.897030, mae: 5.628471, mean_q: 9.318506\n",
      " 69483/100000: episode: 791, duration: 24.407s, episode steps: 141, steps per second:   6, episode reward:  1.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.177 [0.000, 4.000],  loss: 5.111687, mae: 5.787792, mean_q: 9.595287\n",
      " 69740/100000: episode: 792, duration: 44.401s, episode steps: 257, steps per second:   6, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.358 [0.000, 4.000],  loss: 4.731819, mae: 6.408648, mean_q: 10.707491\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 69798/100000: episode: 793, duration: 10.132s, episode steps:  58, steps per second:   6, episode reward:  2.000, mean reward:  0.034 [ 0.000,  1.000], mean action: 2.241 [0.000, 4.000],  loss: 4.910751, mae: 6.954809, mean_q: 11.664351\n",
      " 69868/100000: episode: 794, duration: 12.182s, episode steps:  70, steps per second:   6, episode reward:  1.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.143 [0.000, 4.000],  loss: 5.012158, mae: 6.728694, mean_q: 11.168017\n",
      " 69905/100000: episode: 795, duration: 6.547s, episode steps:  37, steps per second:   6, episode reward:  1.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.270 [0.000, 4.000],  loss: 5.046446, mae: 7.163979, mean_q: 11.983988\n",
      " 69970/100000: episode: 796, duration: 11.345s, episode steps:  65, steps per second:   6, episode reward:  1.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.385 [0.000, 4.000],  loss: 5.082349, mae: 6.903931, mean_q: 11.409124\n",
      " 70020/100000: episode: 797, duration: 8.759s, episode steps:  50, steps per second:   6, episode reward:  1.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.320 [0.000, 4.000],  loss: 4.732124, mae: 7.150418, mean_q: 11.958352\n",
      " 70099/100000: episode: 798, duration: 13.725s, episode steps:  79, steps per second:   6, episode reward:  1.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.519 [0.000, 4.000],  loss: 4.286960, mae: 7.057827, mean_q: 11.800728\n",
      " 70542/100000: episode: 799, duration: 76.171s, episode steps: 443, steps per second:   6, episode reward:  1.000, mean reward:  0.002 [ 0.000,  1.000], mean action: 2.494 [0.000, 4.000],  loss: 5.297805, mae: 8.108985, mean_q: 13.714087\n",
      " 70588/100000: episode: 800, duration: 8.052s, episode steps:  46, steps per second:   6, episode reward:  1.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.826 [0.000, 4.000],  loss: 5.451868, mae: 9.110516, mean_q: 15.416071\n",
      " 70722/100000: episode: 801, duration: 23.159s, episode steps: 134, steps per second:   6, episode reward:  1.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.306 [0.000, 4.000],  loss: 5.549619, mae: 8.321693, mean_q: 14.122846\n",
      " 70807/100000: episode: 802, duration: 14.740s, episode steps:  85, steps per second:   6, episode reward:  1.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.482 [0.000, 4.000],  loss: 5.105034, mae: 8.205882, mean_q: 13.967493\n",
      " 71057/100000: episode: 803, duration: 43.003s, episode steps: 250, steps per second:   6, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.372 [0.000, 4.000],  loss: 5.619721, mae: 8.353003, mean_q: 14.256597\n",
      " 71098/100000: episode: 804, duration: 7.195s, episode steps:  41, steps per second:   6, episode reward:  1.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.415 [0.000, 4.000],  loss: 4.887969, mae: 8.136351, mean_q: 13.936351\n",
      " 71210/100000: episode: 805, duration: 19.352s, episode steps: 112, steps per second:   6, episode reward:  1.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.036 [0.000, 4.000],  loss: 4.957729, mae: 8.052916, mean_q: 13.782225\n",
      " 71255/100000: episode: 806, duration: 7.855s, episode steps:  45, steps per second:   6, episode reward:  1.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.311 [0.000, 4.000],  loss: 5.321749, mae: 7.772198, mean_q: 13.272998\n",
      " 71469/100000: episode: 807, duration: 36.884s, episode steps: 214, steps per second:   6, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.322 [0.000, 4.000],  loss: 5.362866, mae: 6.944113, mean_q: 11.742845\n",
      " 71515/100000: episode: 808, duration: 8.036s, episode steps:  46, steps per second:   6, episode reward:  1.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.152 [0.000, 4.000],  loss: 5.256009, mae: 6.554164, mean_q: 11.096385\n",
      " 71608/100000: episode: 809, duration: 16.127s, episode steps:  93, steps per second:   6, episode reward:  1.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.505 [0.000, 4.000],  loss: 5.000614, mae: 6.170004, mean_q: 10.392109\n",
      " 71643/100000: episode: 810, duration: 6.186s, episode steps:  35, steps per second:   6, episode reward:  1.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.343 [0.000, 4.000],  loss: 5.231345, mae: 6.810444, mean_q: 11.454896\n",
      " 71745/100000: episode: 811, duration: 17.651s, episode steps: 102, steps per second:   6, episode reward:  1.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.990 [0.000, 4.000],  loss: 5.543480, mae: 6.717301, mean_q: 11.251964\n",
      " 71844/100000: episode: 812, duration: 17.179s, episode steps:  99, steps per second:   6, episode reward:  1.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.626 [0.000, 4.000],  loss: 5.236019, mae: 6.726592, mean_q: 11.263572\n",
      " 71977/100000: episode: 813, duration: 22.987s, episode steps: 133, steps per second:   6, episode reward:  2.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.444 [0.000, 4.000],  loss: 5.645845, mae: 6.986235, mean_q: 11.752812\n",
      " 72086/100000: episode: 814, duration: 18.954s, episode steps: 109, steps per second:   6, episode reward:  1.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.587 [0.000, 4.000],  loss: 6.564216, mae: 6.877695, mean_q: 11.444792\n",
      " 72132/100000: episode: 815, duration: 8.052s, episode steps:  46, steps per second:   6, episode reward:  1.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.043 [0.000, 4.000],  loss: 5.387919, mae: 7.032188, mean_q: 11.869602\n",
      " 72189/100000: episode: 816, duration: 9.980s, episode steps:  57, steps per second:   6, episode reward:  2.000, mean reward:  0.035 [ 0.000,  1.000], mean action: 2.491 [0.000, 4.000],  loss: 4.688254, mae: 6.918165, mean_q: 11.740646\n",
      " 72304/100000: episode: 817, duration: 19.936s, episode steps: 115, steps per second:   6, episode reward:  1.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.191 [0.000, 4.000],  loss: 5.399250, mae: 6.880600, mean_q: 11.700023\n",
      " 72493/100000: episode: 818, duration: 32.664s, episode steps: 189, steps per second:   6, episode reward:  2.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.312 [0.000, 4.000],  loss: 5.622921, mae: 7.246890, mean_q: 12.312096\n",
      " 72613/100000: episode: 819, duration: 20.753s, episode steps: 120, steps per second:   6, episode reward:  1.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.367 [0.000, 4.000],  loss: 4.896141, mae: 6.752423, mean_q: 11.544230\n",
      " 72677/100000: episode: 820, duration: 11.149s, episode steps:  64, steps per second:   6, episode reward:  1.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.547 [0.000, 4.000],  loss: 5.242158, mae: 6.696089, mean_q: 11.542247\n",
      " 72752/100000: episode: 821, duration: 13.032s, episode steps:  75, steps per second:   6, episode reward:  1.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.547 [0.000, 4.000],  loss: 5.745190, mae: 6.608968, mean_q: 11.420673\n",
      " 72800/100000: episode: 822, duration: 8.414s, episode steps:  48, steps per second:   6, episode reward:  1.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.562 [0.000, 4.000],  loss: 5.528416, mae: 6.564302, mean_q: 11.343155\n",
      " 72843/100000: episode: 823, duration: 7.555s, episode steps:  43, steps per second:   6, episode reward:  1.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.884 [0.000, 4.000],  loss: 6.382067, mae: 6.529358, mean_q: 11.228745\n",
      " 73023/100000: episode: 824, duration: 31.139s, episode steps: 180, steps per second:   6, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.394 [0.000, 4.000],  loss: 5.862409, mae: 6.672612, mean_q: 11.546529\n",
      " 73160/100000: episode: 825, duration: 23.730s, episode steps: 137, steps per second:   6, episode reward:  1.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.358 [0.000, 4.000],  loss: 5.151000, mae: 6.607701, mean_q: 11.564552\n",
      " 73234/100000: episode: 826, duration: 12.883s, episode steps:  74, steps per second:   6, episode reward:  1.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.270 [0.000, 4.000],  loss: 5.222385, mae: 6.981064, mean_q: 12.384454\n",
      " 73294/100000: episode: 827, duration: 10.458s, episode steps:  60, steps per second:   6, episode reward:  1.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.767 [0.000, 4.000],  loss: 5.917621, mae: 7.114297, mean_q: 12.525558\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 73347/100000: episode: 828, duration: 9.245s, episode steps:  53, steps per second:   6, episode reward:  1.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.528 [0.000, 4.000],  loss: 6.510339, mae: 7.152411, mean_q: 12.652527\n",
      " 73455/100000: episode: 829, duration: 18.710s, episode steps: 108, steps per second:   6, episode reward:  1.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.361 [0.000, 4.000],  loss: 6.986250, mae: 7.062313, mean_q: 12.469654\n",
      " 73500/100000: episode: 830, duration: 7.908s, episode steps:  45, steps per second:   6, episode reward:  1.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.111 [0.000, 4.000],  loss: 6.810327, mae: 7.430865, mean_q: 13.085510\n",
      " 73612/100000: episode: 831, duration: 19.439s, episode steps: 112, steps per second:   6, episode reward:  1.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.473 [0.000, 4.000],  loss: 6.282714, mae: 7.435585, mean_q: 13.067787\n",
      " 73656/100000: episode: 832, duration: 7.715s, episode steps:  44, steps per second:   6, episode reward:  2.000, mean reward:  0.045 [ 0.000,  1.000], mean action: 1.750 [0.000, 4.000],  loss: 6.308236, mae: 7.577056, mean_q: 13.472874\n",
      " 73738/100000: episode: 833, duration: 14.261s, episode steps:  82, steps per second:   6, episode reward:  1.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.012 [0.000, 4.000],  loss: 6.144946, mae: 7.325432, mean_q: 12.905842\n",
      " 73830/100000: episode: 834, duration: 15.948s, episode steps:  92, steps per second:   6, episode reward:  2.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.924 [0.000, 4.000],  loss: 6.050716, mae: 7.248511, mean_q: 12.654741\n",
      " 73919/100000: episode: 835, duration: 15.477s, episode steps:  89, steps per second:   6, episode reward:  2.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.596 [0.000, 4.000],  loss: 5.391680, mae: 7.320509, mean_q: 12.803405\n",
      " 73981/100000: episode: 836, duration: 10.815s, episode steps:  62, steps per second:   6, episode reward:  1.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.548 [0.000, 4.000],  loss: 5.413947, mae: 7.056158, mean_q: 12.326262\n",
      " 74119/100000: episode: 837, duration: 23.972s, episode steps: 138, steps per second:   6, episode reward:  2.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.377 [0.000, 4.000],  loss: 6.224431, mae: 6.990457, mean_q: 12.085946\n",
      " 74234/100000: episode: 838, duration: 19.945s, episode steps: 115, steps per second:   6, episode reward:  1.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.478 [0.000, 4.000],  loss: 5.705401, mae: 6.818005, mean_q: 11.684499\n",
      " 74313/100000: episode: 839, duration: 13.742s, episode steps:  79, steps per second:   6, episode reward:  2.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.886 [0.000, 4.000],  loss: 5.654920, mae: 6.942859, mean_q: 11.889590\n",
      " 74387/100000: episode: 840, duration: 12.917s, episode steps:  74, steps per second:   6, episode reward:  2.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.919 [0.000, 4.000],  loss: 5.220274, mae: 6.869893, mean_q: 11.773120\n",
      " 74432/100000: episode: 841, duration: 7.883s, episode steps:  45, steps per second:   6, episode reward:  1.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.000 [0.000, 4.000],  loss: 4.181185, mae: 6.303571, mean_q: 10.745534\n",
      " 74493/100000: episode: 842, duration: 10.636s, episode steps:  61, steps per second:   6, episode reward:  1.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.984 [0.000, 4.000],  loss: 4.740181, mae: 6.378113, mean_q: 10.818450\n",
      " 74543/100000: episode: 843, duration: 8.785s, episode steps:  50, steps per second:   6, episode reward:  1.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.480 [0.000, 4.000],  loss: 5.579439, mae: 6.366394, mean_q: 10.786726\n",
      " 74700/100000: episode: 844, duration: 27.149s, episode steps: 157, steps per second:   6, episode reward:  2.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.217 [0.000, 4.000],  loss: 4.739554, mae: 6.214536, mean_q: 10.487745\n",
      " 74742/100000: episode: 845, duration: 7.367s, episode steps:  42, steps per second:   6, episode reward:  1.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.738 [0.000, 4.000],  loss: 3.919837, mae: 6.010248, mean_q: 10.216691\n",
      " 74801/100000: episode: 846, duration: 10.338s, episode steps:  59, steps per second:   6, episode reward:  1.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.068 [0.000, 4.000],  loss: 5.535552, mae: 6.258559, mean_q: 10.656191\n",
      " 74872/100000: episode: 847, duration: 12.389s, episode steps:  71, steps per second:   6, episode reward:  1.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.577 [0.000, 4.000],  loss: 4.956814, mae: 6.288412, mean_q: 10.731327\n",
      " 74929/100000: episode: 848, duration: 9.970s, episode steps:  57, steps per second:   6, episode reward:  2.000, mean reward:  0.035 [ 0.000,  1.000], mean action: 2.421 [0.000, 4.000],  loss: 5.079821, mae: 6.614865, mean_q: 11.305881\n",
      " 75108/100000: episode: 849, duration: 30.895s, episode steps: 179, steps per second:   6, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.972 [0.000, 4.000],  loss: 4.841131, mae: 6.802227, mean_q: 11.453109\n",
      " 75180/100000: episode: 850, duration: 12.544s, episode steps:  72, steps per second:   6, episode reward:  2.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.292 [0.000, 4.000],  loss: 5.000219, mae: 7.035315, mean_q: 11.798294\n",
      " 75232/100000: episode: 851, duration: 9.134s, episode steps:  52, steps per second:   6, episode reward:  2.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 2.250 [0.000, 4.000],  loss: 4.191829, mae: 6.351108, mean_q: 10.579874\n",
      " 75275/100000: episode: 852, duration: 7.579s, episode steps:  43, steps per second:   6, episode reward:  1.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.395 [0.000, 4.000],  loss: 5.430579, mae: 6.723106, mean_q: 11.198727\n",
      " 75461/100000: episode: 853, duration: 32.150s, episode steps: 186, steps per second:   6, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 2.403 [0.000, 4.000],  loss: 5.471292, mae: 7.175308, mean_q: 12.070459\n",
      " 75500/100000: episode: 854, duration: 6.876s, episode steps:  39, steps per second:   6, episode reward:  1.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.897 [0.000, 4.000],  loss: 5.186648, mae: 7.480012, mean_q: 12.613902\n",
      " 75546/100000: episode: 855, duration: 8.076s, episode steps:  46, steps per second:   6, episode reward:  1.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.283 [0.000, 4.000],  loss: 4.777472, mae: 7.728965, mean_q: 13.085345\n",
      " 75605/100000: episode: 856, duration: 10.298s, episode steps:  59, steps per second:   6, episode reward:  1.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.119 [0.000, 4.000],  loss: 4.989460, mae: 7.730501, mean_q: 13.039772\n",
      " 75717/100000: episode: 857, duration: 19.441s, episode steps: 112, steps per second:   6, episode reward:  2.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.223 [0.000, 4.000],  loss: 4.857723, mae: 7.685824, mean_q: 12.967708\n",
      " 75770/100000: episode: 858, duration: 9.319s, episode steps:  53, steps per second:   6, episode reward:  1.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.113 [0.000, 4.000],  loss: 5.751823, mae: 7.363361, mean_q: 12.364182\n",
      " 75833/100000: episode: 859, duration: 11.016s, episode steps:  63, steps per second:   6, episode reward:  1.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.286 [0.000, 4.000],  loss: 5.585692, mae: 7.517851, mean_q: 12.659259\n",
      " 75886/100000: episode: 860, duration: 9.293s, episode steps:  53, steps per second:   6, episode reward:  2.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 2.302 [0.000, 4.000],  loss: 5.146645, mae: 7.332447, mean_q: 12.352821\n",
      " 75922/100000: episode: 861, duration: 6.383s, episode steps:  36, steps per second:   6, episode reward:  1.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.139 [0.000, 4.000],  loss: 5.846885, mae: 7.410736, mean_q: 12.440312\n",
      " 75975/100000: episode: 862, duration: 9.277s, episode steps:  53, steps per second:   6, episode reward:  1.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.208 [0.000, 4.000],  loss: 5.599098, mae: 7.139443, mean_q: 12.011884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 76022/100000: episode: 863, duration: 8.257s, episode steps:  47, steps per second:   6, episode reward:  1.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.830 [0.000, 4.000],  loss: 5.886426, mae: 6.957636, mean_q: 11.679954\n",
      " 76080/100000: episode: 864, duration: 10.181s, episode steps:  58, steps per second:   6, episode reward:  2.000, mean reward:  0.034 [ 0.000,  1.000], mean action: 2.621 [0.000, 4.000],  loss: 5.267107, mae: 6.500436, mean_q: 10.968993\n",
      " 76124/100000: episode: 865, duration: 7.744s, episode steps:  44, steps per second:   6, episode reward:  1.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.932 [0.000, 4.000],  loss: 5.824551, mae: 6.457438, mean_q: 10.934913\n",
      " 76225/100000: episode: 866, duration: 17.570s, episode steps: 101, steps per second:   6, episode reward:  1.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.416 [0.000, 4.000],  loss: 5.710565, mae: 6.335122, mean_q: 10.828985\n",
      " 76520/100000: episode: 867, duration: 50.995s, episode steps: 295, steps per second:   6, episode reward:  1.000, mean reward:  0.003 [ 0.000,  1.000], mean action: 2.200 [0.000, 4.000],  loss: 5.046040, mae: 5.801903, mean_q: 9.898772\n",
      " 76673/100000: episode: 868, duration: 26.480s, episode steps: 153, steps per second:   6, episode reward:  2.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.373 [0.000, 4.000],  loss: 4.399971, mae: 5.961230, mean_q: 10.175047\n",
      " 76785/100000: episode: 869, duration: 19.413s, episode steps: 112, steps per second:   6, episode reward:  1.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.625 [0.000, 4.000],  loss: 4.369187, mae: 6.254976, mean_q: 10.642532\n",
      " 76829/100000: episode: 870, duration: 7.741s, episode steps:  44, steps per second:   6, episode reward:  1.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.841 [0.000, 4.000],  loss: 5.253480, mae: 6.472709, mean_q: 11.021107\n",
      " 76869/100000: episode: 871, duration: 7.030s, episode steps:  40, steps per second:   6, episode reward:  1.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.950 [0.000, 4.000],  loss: 5.103770, mae: 6.155024, mean_q: 10.537639\n",
      " 76953/100000: episode: 872, duration: 14.608s, episode steps:  84, steps per second:   6, episode reward:  1.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.464 [0.000, 4.000],  loss: 4.612461, mae: 6.390514, mean_q: 10.960281\n",
      " 77036/100000: episode: 873, duration: 14.380s, episode steps:  83, steps per second:   6, episode reward:  1.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.795 [0.000, 4.000],  loss: 5.257106, mae: 6.490712, mean_q: 11.074390\n",
      " 77079/100000: episode: 874, duration: 7.572s, episode steps:  43, steps per second:   6, episode reward:  1.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.907 [0.000, 4.000],  loss: 5.637646, mae: 6.472238, mean_q: 11.053870\n",
      " 77120/100000: episode: 875, duration: 7.213s, episode steps:  41, steps per second:   6, episode reward:  1.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.000 [0.000, 4.000],  loss: 5.169445, mae: 6.369478, mean_q: 10.933382\n",
      " 77183/100000: episode: 876, duration: 10.970s, episode steps:  63, steps per second:   6, episode reward:  3.000, mean reward:  0.048 [ 0.000,  1.000], mean action: 2.238 [0.000, 4.000],  loss: 4.824071, mae: 6.328359, mean_q: 10.807483\n",
      " 77274/100000: episode: 877, duration: 15.793s, episode steps:  91, steps per second:   6, episode reward:  1.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.297 [0.000, 4.000],  loss: 5.306380, mae: 6.272808, mean_q: 10.626626\n",
      " 77377/100000: episode: 878, duration: 17.932s, episode steps: 103, steps per second:   6, episode reward:  2.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.369 [0.000, 4.000],  loss: 5.404717, mae: 6.580131, mean_q: 11.234317\n",
      " 77428/100000: episode: 879, duration: 8.940s, episode steps:  51, steps per second:   6, episode reward:  1.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.353 [0.000, 4.000],  loss: 7.109628, mae: 7.172516, mean_q: 12.195476\n",
      " 77479/100000: episode: 880, duration: 8.961s, episode steps:  51, steps per second:   6, episode reward:  3.000, mean reward:  0.059 [ 0.000,  1.000], mean action: 2.353 [0.000, 4.000],  loss: 5.459106, mae: 6.792861, mean_q: 11.627383\n",
      " 77525/100000: episode: 881, duration: 8.093s, episode steps:  46, steps per second:   6, episode reward:  1.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.609 [0.000, 4.000],  loss: 5.332922, mae: 6.428401, mean_q: 11.050173\n",
      " 77587/100000: episode: 882, duration: 10.849s, episode steps:  62, steps per second:   6, episode reward:  2.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 2.339 [0.000, 4.000],  loss: 6.816939, mae: 5.972713, mean_q: 10.255676\n",
      " 77632/100000: episode: 883, duration: 7.918s, episode steps:  45, steps per second:   6, episode reward:  1.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.156 [0.000, 4.000],  loss: 5.877822, mae: 5.867099, mean_q: 10.143507\n",
      " 77677/100000: episode: 884, duration: 7.926s, episode steps:  45, steps per second:   6, episode reward:  1.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.156 [0.000, 4.000],  loss: 6.123627, mae: 5.769877, mean_q: 9.962861\n",
      " 77725/100000: episode: 885, duration: 8.455s, episode steps:  48, steps per second:   6, episode reward:  1.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.667 [0.000, 4.000],  loss: 5.739417, mae: 5.998680, mean_q: 10.390954\n",
      " 77800/100000: episode: 886, duration: 13.125s, episode steps:  75, steps per second:   6, episode reward:  2.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.920 [0.000, 4.000],  loss: 5.382772, mae: 5.847723, mean_q: 10.072349\n",
      " 77842/100000: episode: 887, duration: 7.377s, episode steps:  42, steps per second:   6, episode reward:  2.000, mean reward:  0.048 [ 0.000,  1.000], mean action: 2.095 [0.000, 4.000],  loss: 6.393201, mae: 5.528364, mean_q: 9.472216\n",
      " 77913/100000: episode: 888, duration: 12.434s, episode steps:  71, steps per second:   6, episode reward:  1.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.239 [0.000, 4.000],  loss: 6.069079, mae: 5.461494, mean_q: 9.296407\n",
      " 78029/100000: episode: 889, duration: 20.162s, episode steps: 116, steps per second:   6, episode reward:  2.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.052 [0.000, 4.000],  loss: 5.098823, mae: 5.604364, mean_q: 9.541039\n",
      " 78264/100000: episode: 890, duration: 40.641s, episode steps: 235, steps per second:   6, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 2.472 [0.000, 4.000],  loss: 5.908171, mae: 6.573343, mean_q: 11.150227\n",
      " 78498/100000: episode: 891, duration: 40.358s, episode steps: 234, steps per second:   6, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.449 [0.000, 4.000],  loss: 5.372500, mae: 6.593072, mean_q: 11.257623\n",
      " 78532/100000: episode: 892, duration: 6.006s, episode steps:  34, steps per second:   6, episode reward:  1.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.618 [0.000, 4.000],  loss: 4.707999, mae: 6.925067, mean_q: 11.847697\n",
      " 78577/100000: episode: 893, duration: 7.906s, episode steps:  45, steps per second:   6, episode reward:  1.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.200 [0.000, 4.000],  loss: 4.152792, mae: 6.895426, mean_q: 11.769060\n",
      " 78740/100000: episode: 894, duration: 28.170s, episode steps: 163, steps per second:   6, episode reward:  2.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.521 [0.000, 4.000],  loss: 5.409426, mae: 7.062852, mean_q: 12.070661\n",
      " 78788/100000: episode: 895, duration: 8.405s, episode steps:  48, steps per second:   6, episode reward:  1.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.250 [0.000, 4.000],  loss: 4.682977, mae: 7.327905, mean_q: 12.692261\n",
      " 78847/100000: episode: 896, duration: 10.265s, episode steps:  59, steps per second:   6, episode reward:  1.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.390 [0.000, 4.000],  loss: 4.563535, mae: 7.265856, mean_q: 12.610266\n",
      " 78891/100000: episode: 897, duration: 7.703s, episode steps:  44, steps per second:   6, episode reward:  1.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.409 [0.000, 4.000],  loss: 5.352056, mae: 7.222383, mean_q: 12.464593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 79131/100000: episode: 898, duration: 41.288s, episode steps: 240, steps per second:   6, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 2.538 [0.000, 4.000],  loss: 4.835907, mae: 7.339449, mean_q: 12.952716\n",
      " 79198/100000: episode: 899, duration: 11.678s, episode steps:  67, steps per second:   6, episode reward:  1.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.925 [0.000, 4.000],  loss: 5.881157, mae: 7.502548, mean_q: 13.489013\n",
      " 79253/100000: episode: 900, duration: 9.627s, episode steps:  55, steps per second:   6, episode reward:  1.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.455 [0.000, 4.000],  loss: 5.924403, mae: 7.251412, mean_q: 12.899413\n",
      " 79310/100000: episode: 901, duration: 9.979s, episode steps:  57, steps per second:   6, episode reward:  1.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.351 [0.000, 4.000],  loss: 5.521254, mae: 7.250319, mean_q: 13.047626\n",
      " 79355/100000: episode: 902, duration: 7.902s, episode steps:  45, steps per second:   6, episode reward:  1.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.356 [0.000, 4.000],  loss: 5.726891, mae: 7.742610, mean_q: 13.847846\n",
      " 79444/100000: episode: 903, duration: 15.485s, episode steps:  89, steps per second:   6, episode reward:  3.000, mean reward:  0.034 [ 0.000,  1.000], mean action: 2.382 [0.000, 4.000],  loss: 5.277709, mae: 7.439069, mean_q: 13.199711\n",
      " 79535/100000: episode: 904, duration: 15.808s, episode steps:  91, steps per second:   6, episode reward:  2.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.604 [0.000, 4.000],  loss: 6.102201, mae: 7.303535, mean_q: 12.888602\n",
      " 79600/100000: episode: 905, duration: 11.344s, episode steps:  65, steps per second:   6, episode reward:  1.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.308 [0.000, 4.000],  loss: 6.140404, mae: 7.221729, mean_q: 12.857137\n",
      " 79703/100000: episode: 906, duration: 17.887s, episode steps: 103, steps per second:   6, episode reward:  1.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.408 [0.000, 4.000],  loss: 5.412601, mae: 7.163899, mean_q: 12.634249\n",
      " 79789/100000: episode: 907, duration: 14.982s, episode steps:  86, steps per second:   6, episode reward:  1.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.291 [0.000, 4.000],  loss: 5.640683, mae: 7.081493, mean_q: 12.360220\n",
      " 79843/100000: episode: 908, duration: 9.447s, episode steps:  54, steps per second:   6, episode reward:  2.000, mean reward:  0.037 [ 0.000,  1.000], mean action: 1.833 [0.000, 4.000],  loss: 6.273606, mae: 7.262362, mean_q: 12.763209\n",
      " 79892/100000: episode: 909, duration: 8.578s, episode steps:  49, steps per second:   6, episode reward:  2.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 1.939 [0.000, 4.000],  loss: 5.253926, mae: 7.167717, mean_q: 12.596133\n",
      " 79954/100000: episode: 910, duration: 10.818s, episode steps:  62, steps per second:   6, episode reward:  1.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.016 [0.000, 4.000],  loss: 5.895646, mae: 7.120014, mean_q: 12.470067\n",
      " 80184/100000: episode: 911, duration: 39.747s, episode steps: 230, steps per second:   6, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.378 [0.000, 4.000],  loss: 5.411077, mae: 6.802472, mean_q: 11.658509\n",
      " 80224/100000: episode: 912, duration: 7.026s, episode steps:  40, steps per second:   6, episode reward:  1.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.250 [0.000, 4.000],  loss: 5.769650, mae: 6.730040, mean_q: 11.459375\n",
      " 80280/100000: episode: 913, duration: 9.777s, episode steps:  56, steps per second:   6, episode reward:  1.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.054 [0.000, 4.000],  loss: 4.921277, mae: 6.724012, mean_q: 11.536172\n",
      " 80355/100000: episode: 914, duration: 13.033s, episode steps:  75, steps per second:   6, episode reward:  2.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.360 [0.000, 4.000],  loss: 4.689076, mae: 6.431366, mean_q: 11.012258\n",
      " 80416/100000: episode: 915, duration: 10.638s, episode steps:  61, steps per second:   6, episode reward:  1.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.311 [0.000, 4.000],  loss: 5.011809, mae: 6.309140, mean_q: 10.821404\n",
      " 80520/100000: episode: 916, duration: 18.091s, episode steps: 104, steps per second:   6, episode reward:  2.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.173 [0.000, 4.000],  loss: 4.766359, mae: 6.187107, mean_q: 10.625828\n",
      " 80571/100000: episode: 917, duration: 8.932s, episode steps:  51, steps per second:   6, episode reward:  1.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.412 [0.000, 4.000],  loss: 5.825520, mae: 6.591947, mean_q: 11.159235\n",
      " 80771/100000: episode: 918, duration: 34.571s, episode steps: 200, steps per second:   6, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 2.650 [0.000, 4.000],  loss: 4.778161, mae: 6.356674, mean_q: 10.905854\n",
      " 80853/100000: episode: 919, duration: 14.244s, episode steps:  82, steps per second:   6, episode reward:  2.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.329 [0.000, 4.000],  loss: 4.549034, mae: 6.169451, mean_q: 10.664867\n",
      " 80902/100000: episode: 920, duration: 8.634s, episode steps:  49, steps per second:   6, episode reward:  1.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.204 [0.000, 4.000],  loss: 4.406362, mae: 6.116595, mean_q: 10.578535\n",
      " 80964/100000: episode: 921, duration: 10.815s, episode steps:  62, steps per second:   6, episode reward:  1.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.387 [0.000, 4.000],  loss: 3.849446, mae: 6.010059, mean_q: 10.421414\n",
      " 81015/100000: episode: 922, duration: 8.899s, episode steps:  51, steps per second:   6, episode reward:  2.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 2.039 [0.000, 4.000],  loss: 3.575938, mae: 6.047327, mean_q: 10.422966\n",
      " 81066/100000: episode: 923, duration: 8.930s, episode steps:  51, steps per second:   6, episode reward:  1.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.431 [0.000, 4.000],  loss: 4.508524, mae: 5.709477, mean_q: 9.828053\n",
      " 81181/100000: episode: 924, duration: 19.943s, episode steps: 115, steps per second:   6, episode reward:  1.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.165 [0.000, 4.000],  loss: 4.416756, mae: 5.557857, mean_q: 9.422904\n",
      " 81234/100000: episode: 925, duration: 9.261s, episode steps:  53, steps per second:   6, episode reward:  2.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 2.377 [0.000, 4.000],  loss: 4.338985, mae: 5.525636, mean_q: 9.418889\n",
      " 81291/100000: episode: 926, duration: 9.956s, episode steps:  57, steps per second:   6, episode reward:  2.000, mean reward:  0.035 [ 0.000,  1.000], mean action: 2.018 [0.000, 4.000],  loss: 4.238860, mae: 5.543457, mean_q: 9.503036\n",
      " 81355/100000: episode: 927, duration: 11.161s, episode steps:  64, steps per second:   6, episode reward:  1.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.922 [0.000, 4.000],  loss: 3.388349, mae: 5.509794, mean_q: 9.452105\n",
      " 81469/100000: episode: 928, duration: 19.770s, episode steps: 114, steps per second:   6, episode reward:  2.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.456 [0.000, 4.000],  loss: 3.916876, mae: 5.506115, mean_q: 9.487267\n",
      " 81529/100000: episode: 929, duration: 10.462s, episode steps:  60, steps per second:   6, episode reward:  1.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.317 [0.000, 4.000],  loss: 3.931604, mae: 5.232989, mean_q: 9.118397\n",
      " 81566/100000: episode: 930, duration: 6.530s, episode steps:  37, steps per second:   6, episode reward:  1.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.946 [0.000, 4.000],  loss: 3.592377, mae: 5.122405, mean_q: 8.977701\n",
      " 81649/100000: episode: 931, duration: 14.499s, episode steps:  83, steps per second:   6, episode reward:  1.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.494 [0.000, 4.000],  loss: 3.713093, mae: 5.261030, mean_q: 9.232019\n",
      " 81716/100000: episode: 932, duration: 11.736s, episode steps:  67, steps per second:   6, episode reward:  1.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.940 [0.000, 4.000],  loss: 4.642795, mae: 5.085412, mean_q: 8.696717\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 81761/100000: episode: 933, duration: 7.951s, episode steps:  45, steps per second:   6, episode reward:  1.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.467 [0.000, 4.000],  loss: 3.813923, mae: 5.474324, mean_q: 9.325624\n",
      " 81809/100000: episode: 934, duration: 8.432s, episode steps:  48, steps per second:   6, episode reward:  1.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.271 [0.000, 4.000],  loss: 4.484770, mae: 5.335425, mean_q: 9.044568\n",
      " 81937/100000: episode: 935, duration: 22.184s, episode steps: 128, steps per second:   6, episode reward:  2.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.312 [0.000, 4.000],  loss: 4.182367, mae: 5.404627, mean_q: 9.146534\n",
      " 82042/100000: episode: 936, duration: 18.241s, episode steps: 105, steps per second:   6, episode reward:  2.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.019 [0.000, 4.000],  loss: 3.826770, mae: 5.514097, mean_q: 9.442077\n",
      " 82129/100000: episode: 937, duration: 15.159s, episode steps:  87, steps per second:   6, episode reward:  2.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.253 [0.000, 4.000],  loss: 4.826379, mae: 5.525619, mean_q: 9.384106\n",
      " 82218/100000: episode: 938, duration: 15.546s, episode steps:  89, steps per second:   6, episode reward:  1.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.202 [0.000, 4.000],  loss: 4.070913, mae: 5.163011, mean_q: 8.813807\n",
      " 82293/100000: episode: 939, duration: 13.170s, episode steps:  75, steps per second:   6, episode reward:  2.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.067 [0.000, 4.000],  loss: 4.489552, mae: 5.469868, mean_q: 9.281665\n",
      " 82593/100000: episode: 940, duration: 51.916s, episode steps: 300, steps per second:   6, episode reward:  4.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.363 [0.000, 4.000],  loss: 3.621773, mae: 5.460432, mean_q: 9.252303\n",
      " 82643/100000: episode: 941, duration: 8.778s, episode steps:  50, steps per second:   6, episode reward:  1.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.960 [0.000, 4.000],  loss: 3.385272, mae: 5.263222, mean_q: 9.001249\n",
      " 82694/100000: episode: 942, duration: 8.903s, episode steps:  51, steps per second:   6, episode reward:  2.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 2.549 [0.000, 4.000],  loss: 3.556248, mae: 5.181744, mean_q: 8.895758\n",
      " 82783/100000: episode: 943, duration: 15.473s, episode steps:  89, steps per second:   6, episode reward:  2.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.955 [0.000, 4.000],  loss: 4.027518, mae: 5.006092, mean_q: 8.527928\n",
      " 82832/100000: episode: 944, duration: 8.583s, episode steps:  49, steps per second:   6, episode reward:  1.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.898 [0.000, 4.000],  loss: 3.233245, mae: 4.951440, mean_q: 8.455804\n",
      " 82926/100000: episode: 945, duration: 16.336s, episode steps:  94, steps per second:   6, episode reward:  2.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.383 [0.000, 4.000],  loss: 3.863877, mae: 5.221027, mean_q: 8.864828\n",
      " 83088/100000: episode: 946, duration: 28.044s, episode steps: 162, steps per second:   6, episode reward:  2.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.043 [0.000, 4.000],  loss: 3.757216, mae: 5.242092, mean_q: 9.015544\n",
      " 83147/100000: episode: 947, duration: 10.291s, episode steps:  59, steps per second:   6, episode reward:  1.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.831 [0.000, 4.000],  loss: 4.353211, mae: 5.904605, mean_q: 10.142615\n",
      " 83225/100000: episode: 948, duration: 13.585s, episode steps:  78, steps per second:   6, episode reward:  1.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.436 [0.000, 4.000],  loss: 4.847382, mae: 5.953617, mean_q: 10.165053\n",
      " 83373/100000: episode: 949, duration: 25.856s, episode steps: 148, steps per second:   6, episode reward:  1.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.993 [0.000, 4.000],  loss: 4.813356, mae: 6.126645, mean_q: 10.482445\n",
      " 83421/100000: episode: 950, duration: 8.499s, episode steps:  48, steps per second:   6, episode reward:  1.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.104 [0.000, 4.000],  loss: 5.163957, mae: 6.362021, mean_q: 10.824982\n",
      " 83472/100000: episode: 951, duration: 8.959s, episode steps:  51, steps per second:   6, episode reward:  1.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.216 [0.000, 4.000],  loss: 4.829068, mae: 6.393947, mean_q: 10.852337\n",
      " 83538/100000: episode: 952, duration: 11.555s, episode steps:  66, steps per second:   6, episode reward:  1.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.045 [0.000, 4.000],  loss: 5.426784, mae: 6.471564, mean_q: 10.846638\n",
      " 83593/100000: episode: 953, duration: 9.637s, episode steps:  55, steps per second:   6, episode reward:  1.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.945 [0.000, 4.000],  loss: 5.159129, mae: 6.461607, mean_q: 10.788068\n",
      " 83655/100000: episode: 954, duration: 10.850s, episode steps:  62, steps per second:   6, episode reward:  2.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 2.048 [0.000, 4.000],  loss: 5.487376, mae: 6.307966, mean_q: 10.569922\n",
      " 83754/100000: episode: 955, duration: 17.236s, episode steps:  99, steps per second:   6, episode reward:  1.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.354 [0.000, 4.000],  loss: 5.059885, mae: 6.442440, mean_q: 10.697451\n",
      " 83821/100000: episode: 956, duration: 11.738s, episode steps:  67, steps per second:   6, episode reward:  1.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.985 [0.000, 4.000],  loss: 5.393085, mae: 6.487088, mean_q: 10.830003\n",
      " 83963/100000: episode: 957, duration: 24.572s, episode steps: 142, steps per second:   6, episode reward:  2.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.873 [0.000, 4.000],  loss: 4.534739, mae: 6.920735, mean_q: 11.514496\n",
      " 84022/100000: episode: 958, duration: 10.319s, episode steps:  59, steps per second:   6, episode reward:  1.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.136 [0.000, 4.000],  loss: 4.894089, mae: 6.548245, mean_q: 10.776560\n",
      " 84073/100000: episode: 959, duration: 8.927s, episode steps:  51, steps per second:   6, episode reward:  1.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.216 [0.000, 4.000],  loss: 4.837905, mae: 6.338038, mean_q: 10.514277\n",
      " 84128/100000: episode: 960, duration: 9.656s, episode steps:  55, steps per second:   6, episode reward:  1.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.109 [0.000, 4.000],  loss: 4.073146, mae: 6.043954, mean_q: 9.944179\n",
      " 84169/100000: episode: 961, duration: 7.224s, episode steps:  41, steps per second:   6, episode reward:  1.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.829 [0.000, 4.000],  loss: 5.321520, mae: 6.278113, mean_q: 10.212611\n",
      " 84263/100000: episode: 962, duration: 16.368s, episode steps:  94, steps per second:   6, episode reward:  2.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.213 [0.000, 4.000],  loss: 4.526133, mae: 5.960832, mean_q: 9.841686\n",
      " 84313/100000: episode: 963, duration: 8.772s, episode steps:  50, steps per second:   6, episode reward:  1.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.500 [0.000, 4.000],  loss: 3.577919, mae: 5.862692, mean_q: 9.683898\n",
      " 84361/100000: episode: 964, duration: 8.435s, episode steps:  48, steps per second:   6, episode reward:  1.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.938 [0.000, 4.000],  loss: 5.429404, mae: 5.930946, mean_q: 9.728870\n",
      " 84481/100000: episode: 965, duration: 20.832s, episode steps: 120, steps per second:   6, episode reward:  1.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.108 [0.000, 4.000],  loss: 5.065308, mae: 6.277243, mean_q: 10.307269\n",
      " 84525/100000: episode: 966, duration: 7.759s, episode steps:  44, steps per second:   6, episode reward:  1.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.318 [0.000, 4.000],  loss: 4.432003, mae: 6.193802, mean_q: 10.322806\n",
      " 84712/100000: episode: 967, duration: 32.295s, episode steps: 187, steps per second:   6, episode reward:  2.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.406 [0.000, 4.000],  loss: 5.456254, mae: 6.417578, mean_q: 10.702164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 84753/100000: episode: 968, duration: 7.220s, episode steps:  41, steps per second:   6, episode reward:  1.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.171 [0.000, 4.000],  loss: 6.085099, mae: 6.643352, mean_q: 11.163088\n",
      " 84822/100000: episode: 969, duration: 12.031s, episode steps:  69, steps per second:   6, episode reward:  1.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.957 [0.000, 4.000],  loss: 5.009725, mae: 6.701506, mean_q: 11.292746\n",
      " 84879/100000: episode: 970, duration: 9.987s, episode steps:  57, steps per second:   6, episode reward:  2.000, mean reward:  0.035 [ 0.000,  1.000], mean action: 2.737 [0.000, 4.000],  loss: 6.320929, mae: 6.295943, mean_q: 10.669348\n",
      " 84957/100000: episode: 971, duration: 13.592s, episode steps:  78, steps per second:   6, episode reward:  1.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.282 [0.000, 4.000],  loss: 5.416831, mae: 6.145088, mean_q: 10.496780\n",
      " 85060/100000: episode: 972, duration: 17.861s, episode steps: 103, steps per second:   6, episode reward:  1.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.175 [0.000, 4.000],  loss: 6.501733, mae: 6.594421, mean_q: 11.242699\n",
      " 85127/100000: episode: 973, duration: 11.701s, episode steps:  67, steps per second:   6, episode reward:  1.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.284 [0.000, 4.000],  loss: 6.150844, mae: 6.930496, mean_q: 11.881337\n",
      " 85185/100000: episode: 974, duration: 10.159s, episode steps:  58, steps per second:   6, episode reward:  2.000, mean reward:  0.034 [ 0.000,  1.000], mean action: 2.517 [0.000, 4.000],  loss: 6.839321, mae: 6.814875, mean_q: 11.753150\n",
      " 85253/100000: episode: 975, duration: 11.851s, episode steps:  68, steps per second:   6, episode reward:  2.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.279 [0.000, 4.000],  loss: 5.720822, mae: 6.750234, mean_q: 11.709231\n",
      " 85312/100000: episode: 976, duration: 10.377s, episode steps:  59, steps per second:   6, episode reward:  1.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.271 [0.000, 4.000],  loss: 5.919629, mae: 6.443879, mean_q: 11.170882\n",
      " 85417/100000: episode: 977, duration: 18.256s, episode steps: 105, steps per second:   6, episode reward:  1.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.638 [0.000, 4.000],  loss: 6.329300, mae: 6.546840, mean_q: 11.430928\n",
      " 85456/100000: episode: 978, duration: 6.906s, episode steps:  39, steps per second:   6, episode reward:  1.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.128 [0.000, 4.000],  loss: 5.657845, mae: 6.729992, mean_q: 11.823621\n",
      " 85511/100000: episode: 979, duration: 9.638s, episode steps:  55, steps per second:   6, episode reward:  2.000, mean reward:  0.036 [ 0.000,  1.000], mean action: 2.236 [0.000, 4.000],  loss: 5.401499, mae: 6.479272, mean_q: 11.378964\n",
      " 85575/100000: episode: 980, duration: 11.193s, episode steps:  64, steps per second:   6, episode reward:  1.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.016 [0.000, 4.000],  loss: 6.509601, mae: 6.320782, mean_q: 11.016248\n",
      " 85780/100000: episode: 981, duration: 35.492s, episode steps: 205, steps per second:   6, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 2.078 [0.000, 4.000],  loss: 5.667621, mae: 6.527838, mean_q: 11.379549\n",
      " 85835/100000: episode: 982, duration: 9.641s, episode steps:  55, steps per second:   6, episode reward:  1.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.418 [0.000, 4.000],  loss: 6.176674, mae: 6.779516, mean_q: 11.787163\n",
      " 85931/100000: episode: 983, duration: 16.686s, episode steps:  96, steps per second:   6, episode reward:  2.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.146 [0.000, 4.000],  loss: 5.251919, mae: 6.753939, mean_q: 11.681469\n",
      " 85991/100000: episode: 984, duration: 10.497s, episode steps:  60, steps per second:   6, episode reward:  1.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.333 [0.000, 4.000],  loss: 6.333367, mae: 6.625398, mean_q: 11.412906\n",
      " 86099/100000: episode: 985, duration: 18.752s, episode steps: 108, steps per second:   6, episode reward:  2.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.435 [0.000, 4.000],  loss: 4.852411, mae: 6.338041, mean_q: 10.927544\n",
      " 86148/100000: episode: 986, duration: 8.600s, episode steps:  49, steps per second:   6, episode reward:  2.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 2.020 [0.000, 4.000],  loss: 5.018254, mae: 6.495542, mean_q: 11.284911\n",
      " 86220/100000: episode: 987, duration: 12.530s, episode steps:  72, steps per second:   6, episode reward:  1.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.931 [0.000, 4.000],  loss: 4.588146, mae: 6.611472, mean_q: 11.350651\n",
      " 86353/100000: episode: 988, duration: 23.069s, episode steps: 133, steps per second:   6, episode reward:  2.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.203 [0.000, 4.000],  loss: 4.734797, mae: 6.788581, mean_q: 11.625964\n",
      " 86428/100000: episode: 989, duration: 13.061s, episode steps:  75, steps per second:   6, episode reward:  1.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.827 [0.000, 4.000],  loss: 4.560631, mae: 6.772649, mean_q: 11.489009\n",
      " 86481/100000: episode: 990, duration: 9.268s, episode steps:  53, steps per second:   6, episode reward:  1.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.264 [0.000, 4.000],  loss: 4.223431, mae: 6.916676, mean_q: 11.771028\n",
      " 86532/100000: episode: 991, duration: 8.946s, episode steps:  51, steps per second:   6, episode reward:  2.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 2.255 [0.000, 4.000],  loss: 4.972040, mae: 6.839112, mean_q: 11.632713\n",
      " 86622/100000: episode: 992, duration: 15.623s, episode steps:  90, steps per second:   6, episode reward:  1.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.422 [0.000, 4.000],  loss: 4.609404, mae: 6.823297, mean_q: 11.607576\n",
      " 86693/100000: episode: 993, duration: 12.360s, episode steps:  71, steps per second:   6, episode reward:  2.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.141 [0.000, 4.000],  loss: 5.038525, mae: 6.809524, mean_q: 11.670546\n",
      " 86740/100000: episode: 994, duration: 8.243s, episode steps:  47, steps per second:   6, episode reward:  2.000, mean reward:  0.043 [ 0.000,  1.000], mean action: 1.957 [0.000, 4.000],  loss: 4.331663, mae: 6.197736, mean_q: 10.614798\n",
      " 86796/100000: episode: 995, duration: 9.784s, episode steps:  56, steps per second:   6, episode reward:  1.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.196 [0.000, 4.000],  loss: 5.027523, mae: 6.085412, mean_q: 10.378976\n",
      " 87025/100000: episode: 996, duration: 39.580s, episode steps: 229, steps per second:   6, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 2.498 [0.000, 4.000],  loss: 4.682174, mae: 6.215091, mean_q: 10.650220\n",
      " 87062/100000: episode: 997, duration: 6.531s, episode steps:  37, steps per second:   6, episode reward:  1.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.216 [0.000, 4.000],  loss: 5.261247, mae: 6.521919, mean_q: 11.277497\n",
      " 87156/100000: episode: 998, duration: 16.333s, episode steps:  94, steps per second:   6, episode reward:  2.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.489 [0.000, 4.000],  loss: 4.432388, mae: 6.320669, mean_q: 10.851317\n",
      " 87316/100000: episode: 999, duration: 27.662s, episode steps: 160, steps per second:   6, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.469 [0.000, 4.000],  loss: 4.307421, mae: 6.284257, mean_q: 10.964518\n",
      " 87370/100000: episode: 1000, duration: 9.462s, episode steps:  54, steps per second:   6, episode reward:  1.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.296 [0.000, 4.000],  loss: 4.036334, mae: 6.150042, mean_q: 10.878491\n",
      " 87478/100000: episode: 1001, duration: 18.705s, episode steps: 108, steps per second:   6, episode reward:  1.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.065 [0.000, 4.000],  loss: 4.505198, mae: 5.841262, mean_q: 10.262544\n",
      " 87716/100000: episode: 1002, duration: 41.065s, episode steps: 238, steps per second:   6, episode reward:  3.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.256 [0.000, 4.000],  loss: 3.642608, mae: 5.714661, mean_q: 10.055186\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 87818/100000: episode: 1003, duration: 17.652s, episode steps: 102, steps per second:   6, episode reward:  2.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.137 [0.000, 4.000],  loss: 3.660694, mae: 5.898588, mean_q: 10.328044\n",
      " 87873/100000: episode: 1004, duration: 9.608s, episode steps:  55, steps per second:   6, episode reward:  1.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.127 [0.000, 4.000],  loss: 3.759126, mae: 6.013584, mean_q: 10.548348\n",
      " 88049/100000: episode: 1005, duration: 30.361s, episode steps: 176, steps per second:   6, episode reward:  3.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.239 [0.000, 4.000],  loss: 3.405863, mae: 5.680010, mean_q: 9.964133\n",
      " 88154/100000: episode: 1006, duration: 18.180s, episode steps: 105, steps per second:   6, episode reward:  1.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.352 [0.000, 4.000],  loss: 3.762881, mae: 5.722743, mean_q: 10.050332\n",
      " 88199/100000: episode: 1007, duration: 7.882s, episode steps:  45, steps per second:   6, episode reward:  2.000, mean reward:  0.044 [ 0.000,  1.000], mean action: 2.000 [0.000, 4.000],  loss: 3.857515, mae: 6.008276, mean_q: 10.367482\n",
      " 88235/100000: episode: 1008, duration: 6.350s, episode steps:  36, steps per second:   6, episode reward:  1.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.167 [0.000, 4.000],  loss: 3.855738, mae: 6.046183, mean_q: 10.454250\n",
      " 88282/100000: episode: 1009, duration: 8.234s, episode steps:  47, steps per second:   6, episode reward:  1.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.064 [0.000, 4.000],  loss: 3.392399, mae: 5.819965, mean_q: 10.072168\n",
      " 88449/100000: episode: 1010, duration: 28.854s, episode steps: 167, steps per second:   6, episode reward:  2.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.371 [0.000, 4.000],  loss: 3.825677, mae: 5.931121, mean_q: 10.242885\n",
      " 88566/100000: episode: 1011, duration: 20.242s, episode steps: 117, steps per second:   6, episode reward:  2.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.453 [0.000, 4.000],  loss: 4.144878, mae: 6.556385, mean_q: 11.364182\n",
      " 88687/100000: episode: 1012, duration: 20.955s, episode steps: 121, steps per second:   6, episode reward:  1.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.372 [0.000, 4.000],  loss: 4.035639, mae: 6.534938, mean_q: 11.357071\n",
      " 88757/100000: episode: 1013, duration: 12.239s, episode steps:  70, steps per second:   6, episode reward:  2.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.129 [0.000, 4.000],  loss: 4.361528, mae: 6.571868, mean_q: 11.471260\n",
      " 89056/100000: episode: 1014, duration: 51.529s, episode steps: 299, steps per second:   6, episode reward:  1.000, mean reward:  0.003 [ 0.000,  1.000], mean action: 2.562 [0.000, 4.000],  loss: 4.154788, mae: 6.517966, mean_q: 11.513773\n",
      " 89152/100000: episode: 1015, duration: 16.617s, episode steps:  96, steps per second:   6, episode reward:  1.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.542 [0.000, 4.000],  loss: 4.139883, mae: 6.325230, mean_q: 11.288074\n",
      " 89212/100000: episode: 1016, duration: 10.475s, episode steps:  60, steps per second:   6, episode reward:  2.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 1.700 [0.000, 4.000],  loss: 3.722390, mae: 6.240652, mean_q: 11.214655\n",
      " 89273/100000: episode: 1017, duration: 10.623s, episode steps:  61, steps per second:   6, episode reward:  1.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.623 [0.000, 4.000],  loss: 3.924102, mae: 6.153697, mean_q: 11.042769\n",
      " 89325/100000: episode: 1018, duration: 9.069s, episode steps:  52, steps per second:   6, episode reward:  2.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 2.058 [0.000, 4.000],  loss: 4.050282, mae: 6.386912, mean_q: 11.428329\n",
      " 89429/100000: episode: 1019, duration: 18.011s, episode steps: 104, steps per second:   6, episode reward:  2.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.038 [0.000, 4.000],  loss: 4.136842, mae: 6.027846, mean_q: 10.799589\n",
      " 89658/100000: episode: 1020, duration: 39.439s, episode steps: 229, steps per second:   6, episode reward:  4.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.926 [0.000, 4.000],  loss: 3.864857, mae: 5.231678, mean_q: 9.329616\n",
      " 89704/100000: episode: 1021, duration: 8.061s, episode steps:  46, steps per second:   6, episode reward:  1.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.761 [0.000, 4.000],  loss: 2.991170, mae: 4.936173, mean_q: 8.830077\n",
      " 89753/100000: episode: 1022, duration: 8.621s, episode steps:  49, steps per second:   6, episode reward:  1.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.918 [0.000, 4.000],  loss: 3.064392, mae: 4.795146, mean_q: 8.561068\n",
      " 89810/100000: episode: 1023, duration: 9.948s, episode steps:  57, steps per second:   6, episode reward:  1.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.772 [0.000, 4.000],  loss: 2.825535, mae: 4.758250, mean_q: 8.490493\n",
      " 89865/100000: episode: 1024, duration: 9.608s, episode steps:  55, steps per second:   6, episode reward:  2.000, mean reward:  0.036 [ 0.000,  1.000], mean action: 2.200 [0.000, 4.000],  loss: 2.586229, mae: 4.314684, mean_q: 7.683430\n",
      " 89955/100000: episode: 1025, duration: 15.615s, episode steps:  90, steps per second:   6, episode reward:  2.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.044 [0.000, 4.000],  loss: 3.099597, mae: 4.422337, mean_q: 7.781578\n",
      " 90006/100000: episode: 1026, duration: 8.928s, episode steps:  51, steps per second:   6, episode reward:  2.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 2.294 [0.000, 4.000],  loss: 2.857340, mae: 4.364453, mean_q: 7.654928\n",
      " 90050/100000: episode: 1027, duration: 7.748s, episode steps:  44, steps per second:   6, episode reward:  2.000, mean reward:  0.045 [ 0.000,  1.000], mean action: 2.136 [0.000, 4.000],  loss: 3.627788, mae: 4.310279, mean_q: 7.367456\n",
      " 90118/100000: episode: 1028, duration: 11.866s, episode steps:  68, steps per second:   6, episode reward:  1.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.676 [0.000, 4.000],  loss: 3.286530, mae: 4.064804, mean_q: 7.004394\n",
      " 90228/100000: episode: 1029, duration: 19.059s, episode steps: 110, steps per second:   6, episode reward:  2.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.209 [0.000, 4.000],  loss: 3.168083, mae: 4.140313, mean_q: 7.100558\n",
      " 90262/100000: episode: 1030, duration: 6.011s, episode steps:  34, steps per second:   6, episode reward:  2.000, mean reward:  0.059 [ 0.000,  1.000], mean action: 2.706 [0.000, 4.000],  loss: 2.530799, mae: 4.429101, mean_q: 7.561566\n",
      " 90347/100000: episode: 1031, duration: 14.803s, episode steps:  85, steps per second:   6, episode reward:  1.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.282 [0.000, 4.000],  loss: 2.846788, mae: 4.604510, mean_q: 7.802865\n",
      " 90486/100000: episode: 1032, duration: 24.102s, episode steps: 139, steps per second:   6, episode reward:  1.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.187 [0.000, 4.000],  loss: 3.445853, mae: 4.819062, mean_q: 8.188130\n",
      " 90530/100000: episode: 1033, duration: 7.736s, episode steps:  44, steps per second:   6, episode reward:  1.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.136 [0.000, 4.000],  loss: 4.290345, mae: 5.100963, mean_q: 8.629649\n",
      " 90621/100000: episode: 1034, duration: 15.854s, episode steps:  91, steps per second:   6, episode reward:  2.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.055 [0.000, 4.000],  loss: 4.222939, mae: 5.486530, mean_q: 9.249146\n",
      " 90705/100000: episode: 1035, duration: 14.645s, episode steps:  84, steps per second:   6, episode reward:  1.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.000 [0.000, 4.000],  loss: 4.786582, mae: 5.282011, mean_q: 8.814323\n",
      " 90743/100000: episode: 1036, duration: 6.727s, episode steps:  38, steps per second:   6, episode reward:  1.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.789 [0.000, 4.000],  loss: 3.533889, mae: 5.116072, mean_q: 8.600818\n",
      " 90787/100000: episode: 1037, duration: 7.763s, episode steps:  44, steps per second:   6, episode reward:  1.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.068 [0.000, 4.000],  loss: 5.433921, mae: 5.646404, mean_q: 9.415874\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 90831/100000: episode: 1038, duration: 7.755s, episode steps:  44, steps per second:   6, episode reward:  1.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.477 [0.000, 4.000],  loss: 4.525116, mae: 5.657946, mean_q: 9.512589\n",
      " 90942/100000: episode: 1039, duration: 19.271s, episode steps: 111, steps per second:   6, episode reward:  2.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.874 [0.000, 4.000],  loss: 4.892728, mae: 5.818607, mean_q: 9.794332\n",
      " 91012/100000: episode: 1040, duration: 12.241s, episode steps:  70, steps per second:   6, episode reward:  2.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.729 [0.000, 4.000],  loss: 4.955667, mae: 5.776595, mean_q: 9.779156\n",
      " 91052/100000: episode: 1041, duration: 7.048s, episode steps:  40, steps per second:   6, episode reward:  1.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.150 [0.000, 4.000],  loss: 5.026218, mae: 5.840978, mean_q: 9.811907\n",
      " 91246/100000: episode: 1042, duration: 33.552s, episode steps: 194, steps per second:   6, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 2.258 [0.000, 4.000],  loss: 4.624865, mae: 5.625317, mean_q: 9.556376\n",
      " 91367/100000: episode: 1043, duration: 20.932s, episode steps: 121, steps per second:   6, episode reward:  3.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.380 [0.000, 4.000],  loss: 4.653708, mae: 5.494691, mean_q: 9.349197\n",
      " 91461/100000: episode: 1044, duration: 16.320s, episode steps:  94, steps per second:   6, episode reward:  1.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.330 [0.000, 4.000],  loss: 4.419868, mae: 5.418556, mean_q: 9.212417\n",
      " 91524/100000: episode: 1045, duration: 11.017s, episode steps:  63, steps per second:   6, episode reward:  2.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 2.190 [0.000, 4.000],  loss: 4.097157, mae: 5.251281, mean_q: 9.093635\n",
      " 91611/100000: episode: 1046, duration: 15.138s, episode steps:  87, steps per second:   6, episode reward:  1.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.966 [0.000, 4.000],  loss: 3.935892, mae: 5.131050, mean_q: 8.891964\n",
      " 91676/100000: episode: 1047, duration: 11.335s, episode steps:  65, steps per second:   6, episode reward:  2.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.338 [0.000, 4.000],  loss: 4.200424, mae: 5.467483, mean_q: 9.433846\n",
      " 91713/100000: episode: 1048, duration: 6.541s, episode steps:  37, steps per second:   6, episode reward:  1.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.108 [0.000, 4.000],  loss: 4.159473, mae: 5.159650, mean_q: 8.818221\n",
      " 91798/100000: episode: 1049, duration: 14.805s, episode steps:  85, steps per second:   6, episode reward:  1.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.929 [0.000, 4.000],  loss: 4.081499, mae: 5.261333, mean_q: 9.017152\n",
      " 91954/100000: episode: 1050, duration: 26.931s, episode steps: 156, steps per second:   6, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.340 [0.000, 4.000],  loss: 3.957552, mae: 5.707038, mean_q: 9.797346\n",
      " 92011/100000: episode: 1051, duration: 9.970s, episode steps:  57, steps per second:   6, episode reward:  2.000, mean reward:  0.035 [ 0.000,  1.000], mean action: 2.333 [0.000, 4.000],  loss: 4.200916, mae: 5.792140, mean_q: 9.942798\n",
      " 92217/100000: episode: 1052, duration: 35.626s, episode steps: 206, steps per second:   6, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.481 [0.000, 4.000],  loss: 3.928675, mae: 5.987299, mean_q: 10.258419\n",
      " 92255/100000: episode: 1053, duration: 6.698s, episode steps:  38, steps per second:   6, episode reward:  1.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.184 [0.000, 4.000],  loss: 3.568270, mae: 6.156896, mean_q: 10.644473\n",
      " 92544/100000: episode: 1054, duration: 49.812s, episode steps: 289, steps per second:   6, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.166 [0.000, 4.000],  loss: 4.011242, mae: 6.489382, mean_q: 11.129224\n",
      " 92599/100000: episode: 1055, duration: 9.590s, episode steps:  55, steps per second:   6, episode reward:  2.000, mean reward:  0.036 [ 0.000,  1.000], mean action: 2.109 [0.000, 4.000],  loss: 4.135840, mae: 6.894814, mean_q: 11.871174\n",
      " 92849/100000: episode: 1056, duration: 43.062s, episode steps: 250, steps per second:   6, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.300 [0.000, 4.000],  loss: 3.485813, mae: 6.762196, mean_q: 11.802126\n",
      " 92922/100000: episode: 1057, duration: 12.693s, episode steps:  73, steps per second:   6, episode reward:  1.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.959 [0.000, 4.000],  loss: 3.084229, mae: 6.331072, mean_q: 11.135546\n",
      " 92972/100000: episode: 1058, duration: 8.744s, episode steps:  50, steps per second:   6, episode reward:  1.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.240 [0.000, 4.000],  loss: 3.199075, mae: 6.418638, mean_q: 11.314938\n",
      " 93048/100000: episode: 1059, duration: 13.219s, episode steps:  76, steps per second:   6, episode reward:  1.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.039 [0.000, 4.000],  loss: 3.572756, mae: 6.485556, mean_q: 11.389748\n",
      " 93110/100000: episode: 1060, duration: 10.815s, episode steps:  62, steps per second:   6, episode reward:  1.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.145 [0.000, 4.000],  loss: 4.500636, mae: 6.667411, mean_q: 11.643943\n",
      " 93143/100000: episode: 1061, duration: 5.874s, episode steps:  33, steps per second:   6, episode reward:  1.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 1.879 [0.000, 4.000],  loss: 4.387488, mae: 6.736624, mean_q: 11.811167\n",
      " 93306/100000: episode: 1062, duration: 28.180s, episode steps: 163, steps per second:   6, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.172 [0.000, 4.000],  loss: 4.612752, mae: 6.488247, mean_q: 11.280296\n",
      " 93644/100000: episode: 1063, duration: 58.225s, episode steps: 338, steps per second:   6, episode reward:  1.000, mean reward:  0.003 [ 0.000,  1.000], mean action: 2.290 [0.000, 4.000],  loss: 4.729345, mae: 7.142753, mean_q: 12.444350\n",
      " 93710/100000: episode: 1064, duration: 11.517s, episode steps:  66, steps per second:   6, episode reward:  1.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.318 [0.000, 4.000],  loss: 4.972863, mae: 7.497313, mean_q: 13.005019\n",
      " 93756/100000: episode: 1065, duration: 8.094s, episode steps:  46, steps per second:   6, episode reward:  1.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.217 [0.000, 4.000],  loss: 4.724867, mae: 7.712772, mean_q: 13.351618\n",
      " 93959/100000: episode: 1066, duration: 35.092s, episode steps: 203, steps per second:   6, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 2.246 [0.000, 4.000],  loss: 5.092865, mae: 7.660773, mean_q: 13.286077\n",
      " 94023/100000: episode: 1067, duration: 11.134s, episode steps:  64, steps per second:   6, episode reward:  2.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.688 [0.000, 4.000],  loss: 5.313007, mae: 8.205133, mean_q: 14.176226\n",
      " 94169/100000: episode: 1068, duration: 25.167s, episode steps: 146, steps per second:   6, episode reward:  3.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.116 [0.000, 4.000],  loss: 5.115764, mae: 8.182634, mean_q: 14.121218\n",
      " 94374/100000: episode: 1069, duration: 35.398s, episode steps: 205, steps per second:   6, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 2.380 [0.000, 4.000],  loss: 4.652014, mae: 7.746976, mean_q: 13.476508\n",
      " 94478/100000: episode: 1070, duration: 18.019s, episode steps: 104, steps per second:   6, episode reward:  2.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.260 [0.000, 4.000],  loss: 4.347750, mae: 7.418634, mean_q: 12.861897\n",
      " 94670/100000: episode: 1071, duration: 33.133s, episode steps: 192, steps per second:   6, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.240 [0.000, 4.000],  loss: 4.175985, mae: 6.931267, mean_q: 12.028386\n",
      " 94722/100000: episode: 1072, duration: 9.098s, episode steps:  52, steps per second:   6, episode reward:  1.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.462 [0.000, 4.000],  loss: 4.454204, mae: 7.081733, mean_q: 12.338120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 94766/100000: episode: 1073, duration: 7.719s, episode steps:  44, steps per second:   6, episode reward:  2.000, mean reward:  0.045 [ 0.000,  1.000], mean action: 2.568 [0.000, 4.000],  loss: 4.255746, mae: 7.015725, mean_q: 12.233463\n",
      " 94810/100000: episode: 1074, duration: 7.723s, episode steps:  44, steps per second:   6, episode reward:  1.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.114 [0.000, 4.000],  loss: 4.107853, mae: 6.762183, mean_q: 11.738068\n",
      " 95002/100000: episode: 1075, duration: 33.134s, episode steps: 192, steps per second:   6, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.458 [0.000, 4.000],  loss: 3.891752, mae: 6.707613, mean_q: 11.678528\n",
      " 95053/100000: episode: 1076, duration: 8.922s, episode steps:  51, steps per second:   6, episode reward:  1.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.235 [0.000, 4.000],  loss: 3.568377, mae: 6.476455, mean_q: 11.281519\n",
      " 95111/100000: episode: 1077, duration: 10.122s, episode steps:  58, steps per second:   6, episode reward:  1.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.983 [0.000, 4.000],  loss: 4.223831, mae: 6.493454, mean_q: 11.248824\n",
      " 95274/100000: episode: 1078, duration: 28.201s, episode steps: 163, steps per second:   6, episode reward:  4.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.190 [0.000, 4.000],  loss: 3.971170, mae: 6.379584, mean_q: 11.157417\n",
      " 95577/100000: episode: 1079, duration: 52.166s, episode steps: 303, steps per second:   6, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.238 [0.000, 4.000],  loss: 3.976425, mae: 6.087859, mean_q: 10.596227\n",
      " 95646/100000: episode: 1080, duration: 12.013s, episode steps:  69, steps per second:   6, episode reward:  1.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.261 [0.000, 4.000],  loss: 4.258658, mae: 6.269186, mean_q: 10.869760\n",
      " 95742/100000: episode: 1081, duration: 16.660s, episode steps:  96, steps per second:   6, episode reward:  2.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.146 [0.000, 4.000],  loss: 3.749725, mae: 6.161424, mean_q: 10.691129\n",
      " 95778/100000: episode: 1082, duration: 6.354s, episode steps:  36, steps per second:   6, episode reward:  1.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.583 [0.000, 4.000],  loss: 4.255793, mae: 6.573024, mean_q: 11.344913\n",
      " 95887/100000: episode: 1083, duration: 18.852s, episode steps: 109, steps per second:   6, episode reward:  2.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.101 [0.000, 4.000],  loss: 3.601272, mae: 6.405037, mean_q: 11.067809\n",
      " 95994/100000: episode: 1084, duration: 18.612s, episode steps: 107, steps per second:   6, episode reward:  1.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.206 [0.000, 4.000],  loss: 4.221435, mae: 6.489802, mean_q: 11.024226\n",
      " 96049/100000: episode: 1085, duration: 9.612s, episode steps:  55, steps per second:   6, episode reward:  1.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.509 [0.000, 4.000],  loss: 4.232338, mae: 6.903601, mean_q: 11.785376\n",
      " 96099/100000: episode: 1086, duration: 8.735s, episode steps:  50, steps per second:   6, episode reward:  2.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 2.240 [0.000, 4.000],  loss: 4.060933, mae: 6.778937, mean_q: 11.733223\n",
      " 96155/100000: episode: 1087, duration: 9.789s, episode steps:  56, steps per second:   6, episode reward:  1.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.196 [0.000, 4.000],  loss: 3.824093, mae: 6.677173, mean_q: 11.490084\n",
      " 96245/100000: episode: 1088, duration: 15.659s, episode steps:  90, steps per second:   6, episode reward:  1.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.289 [0.000, 4.000],  loss: 4.367464, mae: 6.658749, mean_q: 11.461776\n",
      " 96307/100000: episode: 1089, duration: 10.870s, episode steps:  62, steps per second:   6, episode reward:  1.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.435 [0.000, 4.000],  loss: 4.054613, mae: 6.865851, mean_q: 11.944414\n",
      " 96353/100000: episode: 1090, duration: 8.087s, episode steps:  46, steps per second:   6, episode reward:  1.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.804 [0.000, 4.000],  loss: 4.345982, mae: 7.097167, mean_q: 12.248719\n",
      " 96451/100000: episode: 1091, duration: 17.029s, episode steps:  98, steps per second:   6, episode reward:  1.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.194 [0.000, 4.000],  loss: 4.669659, mae: 7.129720, mean_q: 12.260212\n",
      " 96496/100000: episode: 1092, duration: 7.914s, episode steps:  45, steps per second:   6, episode reward:  1.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.178 [0.000, 4.000],  loss: 6.480965, mae: 7.463808, mean_q: 12.893732\n",
      " 96545/100000: episode: 1093, duration: 8.607s, episode steps:  49, steps per second:   6, episode reward:  2.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 2.347 [0.000, 4.000],  loss: 5.141241, mae: 7.126947, mean_q: 12.243045\n",
      " 96614/100000: episode: 1094, duration: 12.037s, episode steps:  69, steps per second:   6, episode reward:  1.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.594 [0.000, 4.000],  loss: 5.388250, mae: 6.877174, mean_q: 11.907672\n",
      " 96667/100000: episode: 1095, duration: 9.316s, episode steps:  53, steps per second:   6, episode reward:  2.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 2.000 [0.000, 4.000],  loss: 5.959913, mae: 6.736611, mean_q: 11.551398\n",
      " 96736/100000: episode: 1096, duration: 12.052s, episode steps:  69, steps per second:   6, episode reward:  2.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.232 [0.000, 4.000],  loss: 5.478106, mae: 6.563592, mean_q: 11.335811\n",
      " 96891/100000: episode: 1097, duration: 26.888s, episode steps: 155, steps per second:   6, episode reward:  2.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.013 [0.000, 4.000],  loss: 5.106803, mae: 6.222506, mean_q: 10.728271\n",
      " 96987/100000: episode: 1098, duration: 16.712s, episode steps:  96, steps per second:   6, episode reward:  1.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.250 [0.000, 4.000],  loss: 4.569040, mae: 5.640604, mean_q: 9.821385\n",
      " 97068/100000: episode: 1099, duration: 14.147s, episode steps:  81, steps per second:   6, episode reward:  1.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.333 [0.000, 4.000],  loss: 4.877861, mae: 5.631207, mean_q: 9.739775\n",
      " 97193/100000: episode: 1100, duration: 21.721s, episode steps: 125, steps per second:   6, episode reward:  2.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.488 [0.000, 4.000],  loss: 5.728589, mae: 6.049046, mean_q: 10.427922\n",
      " 97238/100000: episode: 1101, duration: 7.912s, episode steps:  45, steps per second:   6, episode reward:  1.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.911 [0.000, 4.000],  loss: 5.619050, mae: 5.677331, mean_q: 9.816912\n",
      " 97401/100000: episode: 1102, duration: 28.260s, episode steps: 163, steps per second:   6, episode reward:  2.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.166 [0.000, 4.000],  loss: 5.244189, mae: 5.920204, mean_q: 10.119374\n",
      " 97511/100000: episode: 1103, duration: 19.070s, episode steps: 110, steps per second:   6, episode reward:  1.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.409 [0.000, 4.000],  loss: 5.012562, mae: 5.652351, mean_q: 9.514789\n",
      " 97563/100000: episode: 1104, duration: 9.137s, episode steps:  52, steps per second:   6, episode reward:  2.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 1.981 [0.000, 4.000],  loss: 4.247832, mae: 5.670212, mean_q: 9.622248\n",
      " 97604/100000: episode: 1105, duration: 7.232s, episode steps:  41, steps per second:   6, episode reward:  1.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.366 [0.000, 4.000],  loss: 5.173876, mae: 5.657696, mean_q: 9.531963\n",
      " 97668/100000: episode: 1106, duration: 11.215s, episode steps:  64, steps per second:   6, episode reward:  1.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.484 [0.000, 4.000],  loss: 4.312789, mae: 5.796771, mean_q: 9.739973\n",
      " 97893/100000: episode: 1107, duration: 38.846s, episode steps: 225, steps per second:   6, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.289 [0.000, 4.000],  loss: 5.104440, mae: 6.091048, mean_q: 10.176771\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 97954/100000: episode: 1108, duration: 10.667s, episode steps:  61, steps per second:   6, episode reward:  1.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.377 [0.000, 4.000],  loss: 4.903727, mae: 6.661449, mean_q: 11.174904\n",
      " 98106/100000: episode: 1109, duration: 26.289s, episode steps: 152, steps per second:   6, episode reward:  1.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.066 [0.000, 4.000],  loss: 4.529207, mae: 6.405660, mean_q: 10.777441\n",
      " 98249/100000: episode: 1110, duration: 24.738s, episode steps: 143, steps per second:   6, episode reward:  1.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.490 [0.000, 4.000],  loss: 4.647287, mae: 6.377871, mean_q: 10.709054\n",
      " 98314/100000: episode: 1111, duration: 11.310s, episode steps:  65, steps per second:   6, episode reward:  1.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.615 [0.000, 4.000],  loss: 4.580628, mae: 6.352193, mean_q: 10.823589\n",
      " 98375/100000: episode: 1112, duration: 10.654s, episode steps:  61, steps per second:   6, episode reward:  1.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.082 [0.000, 4.000],  loss: 4.938789, mae: 6.550223, mean_q: 11.156504\n",
      " 98627/100000: episode: 1113, duration: 43.457s, episode steps: 252, steps per second:   6, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.143 [0.000, 4.000],  loss: 4.292997, mae: 6.297761, mean_q: 10.779423\n",
      " 98703/100000: episode: 1114, duration: 13.255s, episode steps:  76, steps per second:   6, episode reward:  1.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.118 [0.000, 4.000],  loss: 4.136701, mae: 6.201855, mean_q: 10.674558\n",
      " 98793/100000: episode: 1115, duration: 15.673s, episode steps:  90, steps per second:   6, episode reward:  1.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.922 [0.000, 4.000],  loss: 3.636004, mae: 6.382909, mean_q: 11.084273\n",
      " 98835/100000: episode: 1116, duration: 7.356s, episode steps:  42, steps per second:   6, episode reward:  1.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.000 [0.000, 4.000],  loss: 4.576269, mae: 6.541307, mean_q: 11.423193\n",
      " 98921/100000: episode: 1117, duration: 14.967s, episode steps:  86, steps per second:   6, episode reward:  1.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.372 [0.000, 4.000],  loss: 4.122036, mae: 6.147163, mean_q: 10.759646\n",
      " 99173/100000: episode: 1118, duration: 43.439s, episode steps: 252, steps per second:   6, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.210 [0.000, 4.000],  loss: 4.103807, mae: 6.316908, mean_q: 10.938303\n",
      " 99271/100000: episode: 1119, duration: 16.988s, episode steps:  98, steps per second:   6, episode reward:  1.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.694 [0.000, 4.000],  loss: 4.274241, mae: 6.334827, mean_q: 10.847456\n",
      " 99369/100000: episode: 1120, duration: 17.000s, episode steps:  98, steps per second:   6, episode reward:  1.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.296 [0.000, 4.000],  loss: 4.348878, mae: 6.245829, mean_q: 10.829277\n",
      " 99426/100000: episode: 1121, duration: 9.998s, episode steps:  57, steps per second:   6, episode reward:  2.000, mean reward:  0.035 [ 0.000,  1.000], mean action: 1.860 [0.000, 4.000],  loss: 3.857930, mae: 6.178702, mean_q: 10.717096\n",
      " 99504/100000: episode: 1122, duration: 13.597s, episode steps:  78, steps per second:   6, episode reward:  2.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.000 [0.000, 4.000],  loss: 4.898377, mae: 6.412328, mean_q: 11.024086\n",
      " 99543/100000: episode: 1123, duration: 6.882s, episode steps:  39, steps per second:   6, episode reward:  1.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.333 [0.000, 4.000],  loss: 3.910033, mae: 6.410856, mean_q: 10.995107\n",
      " 99617/100000: episode: 1124, duration: 12.889s, episode steps:  74, steps per second:   6, episode reward:  2.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.405 [0.000, 4.000],  loss: 4.216781, mae: 6.102063, mean_q: 10.480126\n",
      " 99666/100000: episode: 1125, duration: 8.600s, episode steps:  49, steps per second:   6, episode reward:  1.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.245 [0.000, 4.000],  loss: 4.696032, mae: 5.860695, mean_q: 10.136766\n",
      " 99806/100000: episode: 1126, duration: 24.276s, episode steps: 140, steps per second:   6, episode reward:  2.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.329 [0.000, 4.000],  loss: 4.601049, mae: 5.914545, mean_q: 10.160558\n",
      " 99883/100000: episode: 1127, duration: 13.437s, episode steps:  77, steps per second:   6, episode reward:  1.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.039 [0.000, 4.000],  loss: 3.937235, mae: 5.871761, mean_q: 10.163393\n",
      "done, took 17218.393 seconds\n",
      "training complete\n",
      "Testing for 100 episodes ...\n",
      "Episode 1: reward: 0.000, steps: 65\n",
      "Episode 2: reward: 1.000, steps: 106\n",
      "Episode 3: reward: 1.000, steps: 117\n",
      "Episode 4: reward: 1.000, steps: 114\n",
      "Episode 5: reward: 1.000, steps: 91\n",
      "Episode 6: reward: 1.000, steps: 118\n",
      "Episode 7: reward: 1.000, steps: 135\n",
      "Episode 8: reward: 1.000, steps: 122\n",
      "Episode 9: reward: 1.000, steps: 101\n",
      "Episode 10: reward: 1.000, steps: 137\n",
      "Episode 11: reward: 1.000, steps: 59\n",
      "Episode 12: reward: 1.000, steps: 125\n",
      "Episode 13: reward: 1.000, steps: 74\n",
      "Episode 14: reward: 1.000, steps: 92\n",
      "Episode 15: reward: 1.000, steps: 152\n",
      "Episode 16: reward: 1.000, steps: 57\n",
      "Episode 17: reward: 1.000, steps: 147\n",
      "Episode 18: reward: 1.000, steps: 150\n",
      "Episode 19: reward: 1.000, steps: 202\n",
      "Episode 20: reward: 1.000, steps: 220\n",
      "Episode 21: reward: 1.000, steps: 61\n",
      "Episode 22: reward: 1.000, steps: 79\n",
      "Episode 23: reward: 1.000, steps: 83\n",
      "Episode 24: reward: 1.000, steps: 83\n",
      "Episode 25: reward: 1.000, steps: 148\n",
      "Episode 26: reward: 1.000, steps: 63\n",
      "Episode 27: reward: 1.000, steps: 74\n",
      "Episode 28: reward: 1.000, steps: 115\n",
      "Episode 29: reward: 1.000, steps: 85\n",
      "Episode 30: reward: 1.000, steps: 89\n",
      "Episode 31: reward: 1.000, steps: 70\n",
      "Episode 32: reward: 1.000, steps: 88\n",
      "Episode 33: reward: 1.000, steps: 311\n",
      "Episode 34: reward: 1.000, steps: 51\n",
      "Episode 35: reward: 1.000, steps: 75\n",
      "Episode 36: reward: 1.000, steps: 91\n",
      "Episode 37: reward: 1.000, steps: 109\n",
      "Episode 38: reward: 1.000, steps: 121\n",
      "Episode 39: reward: 1.000, steps: 60\n",
      "Episode 40: reward: 1.000, steps: 73\n",
      "Episode 41: reward: 1.000, steps: 212\n",
      "Episode 42: reward: 1.000, steps: 66\n",
      "Episode 43: reward: 1.000, steps: 47\n",
      "Episode 44: reward: 1.000, steps: 52\n",
      "Episode 45: reward: 1.000, steps: 65\n",
      "Episode 46: reward: 1.000, steps: 224\n",
      "Episode 47: reward: 1.000, steps: 73\n",
      "Episode 48: reward: 1.000, steps: 94\n",
      "Episode 49: reward: 1.000, steps: 47\n",
      "Episode 50: reward: 1.000, steps: 123\n",
      "Episode 51: reward: 1.000, steps: 110\n",
      "Episode 52: reward: 1.000, steps: 104\n",
      "Episode 53: reward: 1.000, steps: 89\n",
      "Episode 54: reward: 1.000, steps: 76\n",
      "Episode 55: reward: 1.000, steps: 70\n",
      "Episode 56: reward: 1.000, steps: 59\n",
      "Episode 57: reward: 1.000, steps: 85\n",
      "Episode 58: reward: 1.000, steps: 158\n",
      "Episode 59: reward: 1.000, steps: 151\n",
      "Episode 60: reward: 1.000, steps: 221\n",
      "Episode 61: reward: 1.000, steps: 101\n",
      "Episode 62: reward: 1.000, steps: 111\n",
      "Episode 63: reward: 1.000, steps: 103\n",
      "Episode 64: reward: 1.000, steps: 47\n",
      "Episode 65: reward: 1.000, steps: 243\n",
      "Episode 66: reward: 1.000, steps: 78\n",
      "Episode 67: reward: 1.000, steps: 78\n",
      "Episode 68: reward: 1.000, steps: 75\n",
      "Episode 69: reward: 1.000, steps: 71\n",
      "Episode 70: reward: 1.000, steps: 74\n",
      "Episode 71: reward: 1.000, steps: 130\n",
      "Episode 72: reward: 1.000, steps: 93\n",
      "Episode 73: reward: 1.000, steps: 148\n",
      "Episode 74: reward: 1.000, steps: 226\n",
      "Episode 75: reward: 1.000, steps: 181\n",
      "Episode 76: reward: 1.000, steps: 235\n",
      "Episode 77: reward: 1.000, steps: 82\n",
      "Episode 78: reward: 1.000, steps: 211\n",
      "Episode 79: reward: 1.000, steps: 74\n",
      "Episode 80: reward: 1.000, steps: 100\n",
      "Episode 81: reward: 1.000, steps: 73\n",
      "Episode 82: reward: 1.000, steps: 74\n",
      "Episode 83: reward: 1.000, steps: 105\n",
      "Episode 84: reward: 1.000, steps: 90\n",
      "Episode 85: reward: 1.000, steps: 133\n",
      "Episode 86: reward: 1.000, steps: 170\n",
      "Episode 87: reward: 1.000, steps: 139\n",
      "Episode 88: reward: 1.000, steps: 52\n",
      "Episode 89: reward: 1.000, steps: 84\n",
      "Episode 90: reward: 1.000, steps: 114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 91: reward: 1.000, steps: 55\n",
      "Episode 92: reward: 1.000, steps: 107\n",
      "Episode 93: reward: 1.000, steps: 73\n",
      "Episode 94: reward: 1.000, steps: 64\n",
      "Episode 95: reward: 1.000, steps: 83\n",
      "Episode 96: reward: 1.000, steps: 86\n",
      "Episode 97: reward: 1.000, steps: 92\n",
      "Episode 98: reward: 1.000, steps: 99\n",
      "Episode 99: reward: 1.000, steps: 255\n",
      "Episode 100: reward: 1.000, steps: 56\n",
      "0.99\n"
     ]
    }
   ],
   "source": [
    "Game = Game_Menu(800,600)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
