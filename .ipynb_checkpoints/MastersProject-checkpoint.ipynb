{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.0.1 (SDL 2.0.14, Python 3.6.10)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "import pygame_menu\n",
    "import pygame.freetype\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box\n",
    "\n",
    "from rl.agents import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "#import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r\"C:/Users/chris/OneDrive/Documents/Masters Project/\")\n",
    "from ipynb.fs.full.Games.SpaceInvaders import Space_Invaders\n",
    "from ipynb.fs.full.Games.Asteroids import Asteroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Custom_OpenAI_Env(Env):\n",
    "    def __init__(self, screen_width, screen_height, screen, surface, game):\n",
    "        # Actions we can take; left, right, shoot, no_action\n",
    "        self.action_space = Discrete(4)\n",
    "        # Array of observation, dont know how to implement this as image based input.\n",
    "        self.observation_space = Box(0, 255, shape=(1, screen_width * screen_height, ))\n",
    "        #(screen_height * screen_width * 3,))# Box(low=0, high=255, \n",
    "                                            #shape=(screen_height * screen_width, 1), dtype=np.uint8)\n",
    "        print(\"observation space shape = \", self.observation_space.shape)\n",
    "        #Initialise pygame\n",
    "        self.game = game# Space_Invaders(screen_width, screen_height, screen, surface)\n",
    "        \n",
    "        # Initial state should be image of the first frame of the game\n",
    "        self.state = self.game.get_state()\n",
    "        self.start_state = self.state\n",
    "        \n",
    "        self.surface = surface\n",
    "        self.screen = screen\n",
    "        \n",
    "        \n",
    "    def step(self, action):\n",
    "        # Apply action\n",
    "        self.game.execute_action(action)\n",
    "        \n",
    "        #Call the update loop before getting the state\n",
    "        self.game.update()\n",
    "        self.state = self.game.get_state()\n",
    "        #print(\"state shape: \", self.state.shape)\n",
    "        # Calculate reward\n",
    "        #Calculated as follows: if player is still alive, +0.001 (perhaps), if player is dead, -1\n",
    "        #increasing function dependant on how many aliens are left (the less there are the higher\n",
    "        #the value from value of 0 - 1 with 0 being all still alive, 1 being all dead, game over)\n",
    "        reward = self.game.calculate_reward()\n",
    "        \n",
    "        # Check if shower is done\n",
    "        #if function equals 1, return done\n",
    "        done = self.game.done\n",
    "            \n",
    "        # Set placeholder for info\n",
    "        info = {}\n",
    "\n",
    "        # Return step information\n",
    "        return self.state, reward, done, info\n",
    "    \n",
    "    def render(self, mode):\n",
    "        #Put render loop in here\n",
    "        self.game.render(mode=mode)\n",
    "    def reset(self):\n",
    "        #Restart the game\n",
    "        self.state = self.start_state\n",
    "        self.game.reset()\n",
    "        return self.game.get_state()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Game menu class controlling the functionality of the entire framework\n",
    "class Game_Menu:\n",
    "\n",
    "    def __init__(self, width, height):\n",
    "        self.screen = None\n",
    "        self.menu = None\n",
    "        self.surface = None\n",
    "        self.game = None\n",
    "        self.icon_surface = None\n",
    "    \n",
    "        pygame.display.init()\n",
    "        \n",
    "        self.surface = pygame.display.set_mode((800, 600))\n",
    "        self.menu = pygame_menu.Menu(400, 550, 'Welcome',\n",
    "                               theme=pygame_menu.themes.THEME_BLUE)\n",
    "\n",
    "        self.menu.add_selector('Difficulty :', [('Hard', 1), ('Easy', 2)], onchange=self.set_difficulty)\n",
    "        self.menu.add_button('Space Invaders', self.start_space_invaders)\n",
    "        self.menu.add_button('Space Invaders - Training', self.start_space_invaders_training)\n",
    "        self.menu.add_button('Asteroids', self.start_asteroids)\n",
    "        self.menu.add_button('Quit', pygame_menu.events.EXIT)\n",
    "\n",
    "        self.icon_surface = pygame.image.load('Dependencies/Resources/Masterslogo.png')\n",
    "        pygame.display.set_icon(self.icon_surface)\n",
    "        pygame.display.set_caption(\"Master's Project\")\n",
    "        \n",
    "        self.menu.mainloop(self.surface)\n",
    "\n",
    "    def set_difficulty(self, value, difficulty):\n",
    "        # Do the job here !\n",
    "        pass\n",
    "    def start_asteroids(self):\n",
    "        self.game = Asteroids(800, 600, self.screen, self.surface, False)\n",
    "        pass\n",
    "    def start_space_invaders(self):\n",
    "        pygame.display.init()\n",
    "        self.game = Space_Invaders(700, 600, self.screen, self.surface, False)\n",
    "        pass\n",
    "    def start_space_invaders_training(self):\n",
    "        self.menu.disable()\n",
    "        training_game = Space_Invaders(230, 200, self.screen, self.surface, True)\n",
    "        #Self.game is env\n",
    "        self.game = Custom_OpenAI_Env(230, 200, self.screen, self.surface, training_game)\n",
    "        \n",
    "        #Initialise state/action arrays\n",
    "        states = self.game.observation_space.shape\n",
    "        print(\"states shape: \", states)\n",
    "        actions = self.game.action_space.n\n",
    "        #Initialise the DRL model\n",
    "        model = self.build_model(states, actions)\n",
    "        model.summary()\n",
    "\n",
    "        #Initialise the DQN agent\n",
    "        dqn = self.build_agent(model, actions)\n",
    "        dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "        print(\"beginning training\")\n",
    "        #Fit with openAI gym\n",
    "        dqn.fit(self.game, nb_steps=1000, visualize=True, verbose=1)\n",
    "        print(\"training complete\")\n",
    "        #Test with scores\n",
    "        scores = dqn.test(self.game, nb_episodes=5, visualize=True)\n",
    "        print(np.mean(scores.history['episode_reward']))\n",
    "\n",
    "        #Visualize the testing\n",
    "        #_ = dqn.test(env, nb_episodes=15, visualize=True)\n",
    "\n",
    "    def save(self, model, name='default-model'):\n",
    "        #was dqn now model\n",
    "        model.save_weights(name + '.h5f', overwrite=True)\n",
    "        \n",
    "    def load(self, model, name):\n",
    "        dqn.load_weights(name + '.h5f')\n",
    "        \n",
    "    def start_asteroids_training(self):\n",
    "        training_game = Asteroids(800, 600, screen, surface, True)\n",
    "        self.game = Custom_OpenAI_Env(800, 600, self.screen, self.surface, training_game)\n",
    "        pass\n",
    "    \n",
    "    def build_model(self, states, actions):\n",
    "      model = Sequential()    \n",
    "      model.add(Dense(100, activation='relu', input_shape=states))\n",
    "      model.add(Flatten())\n",
    "      model.add(Dense(100, activation='relu'))\n",
    "      model.add(Dense(4, activation='linear'))\n",
    "      return model\n",
    "\n",
    "    #Function to create DQN model \n",
    "    def build_agent(self, model, actions):\n",
    "        policy = BoltzmannQPolicy()\n",
    "        memory = SequentialMemory(limit=50000, window_length=1)\n",
    "        dqn = DQNAgent(model=model, memory=memory, policy=policy, \n",
    "                      nb_actions=actions, nb_steps_warmup=10, target_model_update=1e-2)\n",
    "        return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "game scale:  1\n",
      "player position: 115.0 180.0\n",
      "observation space shape =  (1, 46000)\n",
      "states shape:  (1, 46000)\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 1, 100)            4600100   \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 404       \n",
      "=================================================================\n",
      "Total params: 4,610,604\n",
      "Trainable params: 4,610,604\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "beginning training\n",
      "Training for 1000 steps ...\n",
      "game scale:  1\n",
      "Interval 1 (0 steps performed)\n",
      "WARNING:tensorflow:From D:\\Anaconda\\envs\\masters\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_v1.py:2070: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "    8/10000 [..............................] - ETA: 10:03 - reward: 0.0000e+00"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\masters\\lib\\site-packages\\rl\\memory.py:40: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   30/10000 [..............................] - ETA: 12:12 - reward: 0.0000e+00successfully shot\n",
      "   59/10000 [..............................] - ETA: 10:07 - reward: 0.0017successfully shot\n",
      "   89/10000 [..............................] - ETA: 9:22 - reward: 0.0022successfully shot\n",
      "  122/10000 [..............................] - ETA: 8:56 - reward: 0.0025successfully shot\n",
      "  130/10000 [..............................] - ETA: 8:52 - reward: 0.0023collision detected 146 128 30\n",
      "training instance finished\n",
      "  132/10000 [..............................] - ETA: 8:51 - reward: 0.0023training instance finished\n",
      "game scale:  1\n",
      "  165/10000 [..............................] - ETA: 8:45 - reward: 0.0018successfully shot\n",
      "  195/10000 [..............................] - ETA: 8:37 - reward: 0.0021successfully shot\n",
      "  225/10000 [..............................] - ETA: 8:30 - reward: 0.0022successfully shot\n",
      "  255/10000 [..............................] - ETA: 8:24 - reward: 0.0024successfully shot\n",
      "  290/10000 [..............................] - ETA: 8:17 - reward: 0.0024successfully shot\n",
      "  322/10000 [..............................] - ETA: 8:12 - reward: 0.0025successfully shot\n",
      "  352/10000 [>.............................] - ETA: 8:09 - reward: 0.0026successfully shot\n",
      "  382/10000 [>.............................] - ETA: 8:06 - reward: 0.0026successfully shot\n",
      "  432/10000 [>.............................] - ETA: 8:00 - reward: 0.0023successfully shot\n",
      "  469/10000 [>.............................] - ETA: 7:57 - reward: 0.0023successfully shot\n",
      "  501/10000 [>.............................] - ETA: 7:54 - reward: 0.0024successfully shot\n",
      "  536/10000 [>.............................] - ETA: 7:52 - reward: 0.0022successfully shot\n",
      "  568/10000 [>.............................] - ETA: 7:51 - reward: 0.0021successfully shot\n",
      "  602/10000 [>.............................] - ETA: 7:48 - reward: 0.0020successfully shot\n",
      "  631/10000 [>.............................] - ETA: 7:46 - reward: 0.0021successfully shot\n",
      "  664/10000 [>.............................] - ETA: 7:44 - reward: 0.0021successfully shot\n",
      "  695/10000 [=>............................] - ETA: 7:42 - reward: 0.0022successfully shot\n",
      "  724/10000 [=>............................] - ETA: 7:40 - reward: 0.0021successfully shot\n",
      "  756/10000 [=>............................] - ETA: 7:38 - reward: 0.0020successfully shot\n",
      "  787/10000 [=>............................] - ETA: 7:37 - reward: 0.0019successfully shot\n",
      "  819/10000 [=>............................] - ETA: 7:35 - reward: 0.0018successfully shot\n",
      "  850/10000 [=>............................] - ETA: 7:34 - reward: 0.0018successfully shot\n",
      "  880/10000 [=>............................] - ETA: 7:32 - reward: 0.0018successfully shot\n",
      "  923/10000 [=>............................] - ETA: 7:29 - reward: 0.0018successfully shot\n",
      "  954/10000 [=>............................] - ETA: 7:28 - reward: 0.0018successfully shot\n",
      "  991/10000 [=>............................] - ETA: 7:26 - reward: 0.0017successfully shot\n",
      " 1000/10000 [==>...........................] - ETA: 7:26 - reward: 0.0017done, took 49.626 seconds\n",
      "training complete\n",
      "Testing for 5 episodes ...\n",
      "game scale:  1\n",
      "collision detected 207 200 30\n",
      "training instance finished\n",
      "training instance finished\n",
      "Episode 1: reward: 0.000, steps: 2301\n",
      "game scale:  1\n",
      "collision detected 210 200 30\n",
      "training instance finished\n",
      "training instance finished\n",
      "Episode 2: reward: 0.000, steps: 564\n",
      "game scale:  1\n"
     ]
    }
   ],
   "source": [
    "Game = Game_Menu(800,600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
