{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.0.1 (SDL 2.0.14, Python 3.6.10)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "\n",
    "import pygame_menu\n",
    "import pygame.freetype\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box\n",
    "\n",
    "\n",
    "from rl.agents import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "\n",
    "from tensorflow.keras.backend import clear_session\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import os.path\n",
    "\n",
    "#Import local modules\n",
    "sys.path.append(os.getcwd())\n",
    "from ipynb.fs.full.Games.SpaceInvaders import Space_Invaders\n",
    "from ipynb.fs.full.Games.Asteroids import Asteroids\n",
    "pygame.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Custom_OpenAI_Env(Env):\n",
    "    def __init__(self, screen_width, screen_height, game, \n",
    "                 action_space = Discrete(5)):\n",
    "        # Assign action and observation space\n",
    "        self.action_space = action_space\n",
    "        self.observation_space = Box(0, 255, shape=(1, screen_width, screen_height, ))\n",
    "\n",
    "        #Initialise the game\n",
    "        self.game = game\n",
    "\n",
    "        #Assign colour, get the initial game state and record it \n",
    "        self.state = self.game.get_state(colour = self.colour)\n",
    "        self.start_state = self.state\n",
    "    \n",
    "        print(\"initialisation complete\")\n",
    "        \n",
    "        self.delay = 1000\n",
    "        \n",
    "    def step(self, action):\n",
    "        # Apply action\n",
    "        #print(\"action: \", action)\n",
    "        self.game.execute_action(action)\n",
    "        #Call the update loop before getting the state\n",
    "        self.game.update()\n",
    "        self.state = self.game.get_state(colour = self.colour)\n",
    "        #Calculate step-based reward\n",
    "        reward = self.game.calculate_reward()\n",
    "        done = self.game.done\n",
    "        # Set placeholder for info\n",
    "        info = {}\n",
    "        # Return step information\n",
    "        return self.state, reward, done, info\n",
    "    \n",
    "    def render(self, mode):\n",
    "        self.game.render()\n",
    "        \n",
    "    def reset(self):\n",
    "        #Restart the game\n",
    "        self.state = self.start_state\n",
    "        self.game.reset()\n",
    "        return self.game.get_state()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Game menu class controlling the functionality of the entire framework\n",
    "WINDOW_LENGTH = 3\n",
    "class Game_Menu:\n",
    "\n",
    "    def __init__(self, width, height):\n",
    "        self.menu = None\n",
    "        self.surface = None\n",
    "        self.game = None\n",
    "        self.icon_surface = None\n",
    "        \n",
    "        #Set standard network parameters\n",
    "        self.learning_rates = [0.0001, 0.000001, 0.0000001]\n",
    "        self.episodes = [1, 10, 100]\n",
    "        self.steps = [3000, 50000, 100000]\n",
    "\n",
    "        #Standard parameter indices\n",
    "        self.learning_rate = 0\n",
    "        self.episode = 0\n",
    "        self.step = 0\n",
    "        self.visualize = False\n",
    "\n",
    "        #Asteroid specific parameters\n",
    "        self.scales = [[800,600, 1.0], [200,200, 0.65], [150,150, 0.45]]\n",
    "        self.intensity_rates = [0, 1, 3]\n",
    "        self.player_speeds = [1, 2, 3]\n",
    "\n",
    "        #Scale, player speed and colour are common to both games\n",
    "        self.scale = 0\n",
    "        self.intensity = 0\n",
    "        self.player_speed = 0\n",
    "        self.homogenous_controls = False\n",
    "        self.colour = False\n",
    "        \n",
    "        #Space invaders specific parameters\n",
    "        self.enemy_speeds = [0.5, 1.0, 2.0]\n",
    "        self.enemy_speed = 0\n",
    "        \n",
    "        #Transfer mode \n",
    "        self.transfer = -1\n",
    "        self.test_env = -1\n",
    "        \n",
    "        #Initialise pygame\n",
    "        pygame.display.init()\n",
    "        self.surface = pygame.display.set_mode((800, 600))\n",
    "        self.icon_surface = pygame.image.load(os.path.join(os.getcwd(), \"Dependencies/Resources\", \"Masterslogo.png\"))\n",
    "        pygame.display.set_icon(self.icon_surface)\n",
    "        pygame.display.set_caption(\"Master's Project\")\n",
    "        \n",
    "        #Start the main menu\n",
    "        self.main_menu()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        #Reset all adjustable variables when returning to the main menu.\n",
    "        self.enemy_speed = 0\n",
    "        self.scale = 0\n",
    "        self.intensity = 0\n",
    "        self.player_speed = 0\n",
    "        self.homogenous_controls = False\n",
    "        self.colour = False\n",
    "        self.learning_rate = 0\n",
    "        self.episode = 0\n",
    "        self.step = 0\n",
    "        self.visualize = False\n",
    "        \n",
    "    def main_menu(self):\n",
    "        self.reset_parameters()\n",
    "        self.surface = pygame.display.set_mode((800, 600))\n",
    "        #Disable any menu if it exists\n",
    "        if self.menu:\n",
    "            self.menu.disable()\n",
    "        #Initialise the main menu interface\n",
    "        self.menu = pygame_menu.Menu(600, 800, 'Main Menu',\n",
    "                         theme=pygame_menu.themes.THEME_DARK)\n",
    "        self.menu.add_button('Space Invaders', self.start_space_invaders)\n",
    "        self.menu.add_button('Space Invaders - Training', self.set_game_mode)\n",
    "        self.menu.add_button('Asteroids', self.start_asteroids)\n",
    "        self.menu.add_button('Asteroids - Training', self.set_standard_parameters)\n",
    "        self.menu.add_button('Transfer Learning', self.transfer_menu)\n",
    "        self.menu.add_button('Test - Space Invaders', self.set_test_space_invaders)\n",
    "        self.menu.add_button('Test - Asteroids', self.set_test_asteroids)\n",
    "        self.menu.add_button('Quit', pygame_menu.events.EXIT)\n",
    "        \n",
    "        self.menu.mainloop(self.surface)\n",
    "\n",
    "    def transfer_menu(self):\n",
    "        #Transfer menu, always disable because this will never be the first\n",
    "        #menu created\n",
    "        self.menu.disable()\n",
    "        self.menu = pygame_menu.Menu(600, 800, 'Transfer Learning',\n",
    "                               theme=pygame_menu.themes.THEME_DARK)\n",
    "        self.menu.add_button('Space Invaders -> Asteroids', self.set_transfer_SA)\n",
    "        self.menu.add_button('Asteroids -> Space Invaders', self.set_transfer_AS)\n",
    "        self.menu.add_button('Back', self.main_menu)\n",
    "        \n",
    "        self.menu.mainloop(self.surface)\n",
    "    \n",
    "    \n",
    "    #Common DQN network settings\n",
    "    def set_standard_param(self, arg, param):\n",
    "        #Learning rate \n",
    "        if param == 0:\n",
    "            print(\"changing learning rate\")\n",
    "            if self.learning_rate < 2:\n",
    "                self.learning_rate += 1\n",
    "            else:\n",
    "                self.learning_rate = 0\n",
    "        #Steps\n",
    "        elif param == 1:\n",
    "            if self.step < 2:\n",
    "                self.step += 1\n",
    "            else:\n",
    "                self.step = 0\n",
    "        #Episodes\n",
    "        elif param == 2:\n",
    "            if self.episode < 2:\n",
    "                self.episode += 1\n",
    "            else:\n",
    "                self.episode = 0\n",
    "        #Visualize\n",
    "        elif param == 3:\n",
    "            self.visualize = 1 if self.visualize == 0 else 1 \n",
    "            \n",
    "    def set_game_mode(self):\n",
    "        self.set_standard_parameters(1)\n",
    "        \n",
    "    def set_test_asteroids(self):\n",
    "        self.test_env = 0\n",
    "        self.test_model()\n",
    "    \n",
    "    def set_test_space_invaders(self):\n",
    "        self.test_env = 1\n",
    "        self.test_model()\n",
    "        \n",
    "    #Set transfer mode: Space invaders to Asteroids\n",
    "    def set_transfer_SA(self):\n",
    "        self.transfer = 0\n",
    "        self.set_standard_parameters()\n",
    "        \n",
    "    #Set transfer mode: Asteroids to Space Invaders\n",
    "    def set_transfer_AS(self):\n",
    "        self.transfer = 1\n",
    "        self.set_standard_parameters(1)\n",
    "    \n",
    "    #Setup menu for standard network parameters\n",
    "    def set_standard_parameters(self, gamemode = 0):\n",
    "        print(\"standard params called\")\n",
    "        self.menu.disable()\n",
    "        self.gamemode = gamemode\n",
    "        self.menu = pygame_menu.Menu(600, 800, 'Select Variables',\n",
    "                               theme=pygame_menu.themes.THEME_DARK)\n",
    "\n",
    "        self.menu.add_selector('Learning rate :', [('1e-1', 0),\n",
    "                                                   ('1e-2', 0),\n",
    "                                                   ('1e-3', 0)],\n",
    "                                                   onchange=self.set_standard_param)\n",
    "        self.menu.add_selector('Steps :', [('1000', 1),\n",
    "                                           ('5000', 1),\n",
    "                                           ('10000', 1),],\n",
    "                                           onchange=self.set_standard_param)\n",
    "        self.menu.add_selector('Episodes: ', [('1', 2),\n",
    "                                           ('5', 2),\n",
    "                                           ('10', 2),],\n",
    "                                           onchange=self.set_standard_param)\n",
    "        self.menu.add_selector('Visualize', [('False', 3), ('True', 3)],\n",
    "                               onchange=self.set_standard_param)\n",
    "        self.menu.add_button('Back', self.main_menu)\n",
    "        \n",
    "        #Continue based on what game mode is selected\n",
    "        if gamemode == 0:\n",
    "            self.menu.add_button('Continue - Asteroids', self.set_asteroids_parameters)\n",
    "        else:\n",
    "            self.menu.add_button('Continue - Space Invaders', self.set_space_invaders_parameters)\n",
    "        \n",
    "        self.menu.mainloop(self.surface)\n",
    "    \n",
    "    def set_mode_parameter(self, arg, param):\n",
    "        #0 scale, 1 intensity, 2, player speed, 3 homo controls, 4 colours, 5 enemy speed\n",
    "        if param == 0:\n",
    "            self.scale += 1 if self.scale < 2 else 0\n",
    "        if param == 1:\n",
    "            self.intensity += 1 if self.intensity < 2 else 0        \n",
    "        if param == 2:\n",
    "            self.player_speed += 1 if self.player_speed < 2 else 0        \n",
    "        if param == 3:\n",
    "            self.homogenous_controls = True if self.homogenous_controls == False else False\n",
    "        if param == 4:\n",
    "            self.colour = True if self.colour == False else False\n",
    "        if param == 5:\n",
    "            self.enemy_speed += 1 if self.enemy_speed < 2 else 0\n",
    "            \n",
    "    #Menu for setting asteroids game mode specific parameters\n",
    "    def set_asteroids_parameters(self):\n",
    "        self.menu.disable()\n",
    "        self.menu = pygame_menu.Menu(600, 800, 'Select Gameplay Variables',\n",
    "                               theme=pygame_menu.themes.THEME_DARK)\n",
    "\n",
    "        self.menu.add_selector('Scale :', [('(800, 600)', 0), ('(400, 300)', 0),('(200, 150)', 0)],\n",
    "                               onchange=self.set_mode_parameter)\n",
    "        self.menu.add_selector('Intensity:', [('0.1', 1), ('0.5', 1),('1.0', 1)],\n",
    "                               onchange=self.set_mode_parameter)\n",
    "        self.menu.add_selector('Player Speed :', [('1', 2), ('1.5', 2),('3', 2)],\n",
    "                               onchange=self.set_mode_parameter)\n",
    "        self.menu.add_selector('Homogenous Controls: ', [('False', 3), ('True', 3)],\n",
    "                        onchange=self.set_mode_parameter)\n",
    "        self.menu.add_selector('Colour Input: ', [('False', 4), ('True', 4)],\n",
    "                        onchange=self.set_mode_parameter)\n",
    "        self.menu.add_button('Start Training', self.start_asteroids_training)\n",
    "        self.menu.add_button('Back', self.main_menu)\n",
    "        \n",
    "        self.menu.mainloop(self.surface)\n",
    "    \n",
    "    #Menu for setting Space invaders game mode specific parameters\n",
    "    def set_space_invaders_parameters(self):\n",
    "        self.menu.disable()\n",
    "        self.menu = pygame_menu.Menu(600, 800, 'Select Gameplay Variables',\n",
    "                               theme=pygame_menu.themes.THEME_DARK)\n",
    "\n",
    "        self.menu.add_selector('Scale :', [('(800, 600)', 0), ('(300, 300)', 0),\n",
    "                                          ('(150, 150)', 0)],\n",
    "                               onchange=self.set_mode_parameter)\n",
    "        self.menu.add_selector('Enemy Speed:', [('0.5', 5), ('1.0', 5),\n",
    "                                          ('2.0', 5)],\n",
    "                               onchange=self.set_mode_parameter)\n",
    "        self.menu.add_selector('Player Speed :', [('1', 2), ('1.5', 2),\n",
    "                                          ('3', 2)],\n",
    "                               onchange=self.set_mode_parameter)\n",
    "        self.menu.add_selector('Colour Input: ', [('False', 4), ('True', 4)],\n",
    "                        onchange=self.set_mode_parameter)\n",
    "        \n",
    "        self.menu.add_button('Start Training', self.start_space_invaders_training)\n",
    "        self.menu.add_button('Back', self.main_menu)\n",
    "        \n",
    "        self.menu.mainloop(self.surface)\n",
    "\n",
    "    #Start asteroids as a normal player\n",
    "    def start_asteroids(self):\n",
    "        self.game = Asteroids(800, 600, self.surface, False, 0.125)\n",
    "\n",
    "    #Start space invaders as a normal player\n",
    "    def start_space_invaders(self):\n",
    "        self.game = Space_Invaders(800, 600, self.surface, False, 1.0)\n",
    "        \n",
    "    def start_asteroids_training(self):\n",
    "        self.menu.disable()\n",
    "        print(\"starting asteroid training\")\n",
    "        training_game = Asteroids(self.scales[self.scale][0], self.scales[self.scale][1], \n",
    "                                  self.surface, True, self.scales[self.scale][2], #add player speed\n",
    "                                  player_speed = self.player_speeds[self.player_speed],\n",
    "                                  player_rtspeed = self.player_speeds[self.player_speed], \n",
    "                                  intensity_modifier = self.intensity_rates[self.intensity],\n",
    "                                  has_colour = self.colour)\n",
    "        \n",
    "        self.game = Custom_OpenAI_Env(self.scales[self.scale][0], self.scales[self.scale][1],\n",
    "                                       training_game)\n",
    "        self.build_model()\n",
    "        self.train_model()\n",
    "        if self.transfer == -1:\n",
    "            self.save(\"asteroids\")\n",
    "        else:\n",
    "            self.save(\"Space-Asteroids-Transfer\")\n",
    "        self.main_menu()\n",
    "    \n",
    "    def start_space_invaders_training(self):\n",
    "        self.menu.disable()\n",
    "        print(\"Starting space invaders training\")\n",
    "        training_game = Space_Invaders(self.scales[self.scale][0], self.scales[self.scale][1]\n",
    "                                       , self.surface, True, self.scales[self.scale][2],\n",
    "                                       enemy_speed = self.enemy_speeds[self.enemy_speed],\n",
    "                                       player_speed = self.player_speeds[self.player_speed],\n",
    "                                       game_intensity_modifier = self.intensity_rates[self.intensity],\n",
    "                                       homogenous_controls = self.homogenous_controls,\n",
    "                                       has_colour = self.colour)\n",
    "        \n",
    "        #Assign new action space based on homogenous control setting\n",
    "        action_space= Discrete(5)\n",
    "        #if self.homogenous_controls:\n",
    "        #    action_space = Discrete(5)\n",
    "        self.game = Custom_OpenAI_Env(self.scales[self.scale][0], self.scales[self.scale][1],\n",
    "                                     training_game,\n",
    "                                     action_space = action_space)\n",
    "        self.build_model()\n",
    "        self.train_model()\n",
    "        if self.transfer == -1:\n",
    "            self.save(\"space-invaders\")\n",
    "        else:\n",
    "            self.save(\"Asteroids-Space-Transfer\")\n",
    "        self.main_menu()\n",
    "        \n",
    "    def build_model(self, metrics = ['mae']):\n",
    "        print(\"building model\")\n",
    "        #Initialise state/action arrays\n",
    "        states = self.game.observation_space.shape\n",
    "        states = (WINDOW_LENGTH, states[1], states[2])\n",
    "        print(states)\n",
    "        self.actions = self.game.action_space.n\n",
    "        \n",
    "        #Initialise the DRL model based on transfer setting\n",
    "        #if self.transfer == -1:\n",
    "        self.model = self.build_network(states, self.actions)\n",
    "\n",
    "        if self.transfer == 0:\n",
    "            print(\"loading space invaders original model.\")\n",
    "            if self.load(\"space-invaders\") != False:\n",
    "                print(\"found space invaders\")\n",
    "                self.load(\"space-invaders\")\n",
    "                #self.load(\"space-invaders\")\n",
    "            else:\n",
    "                print(\"calling this cause why not?\")\n",
    "                self.model = self.build_network(states, self.actions)\n",
    "                \n",
    "        elif self.transfer == 1:\n",
    "            print(\"loading asteroids original model\")\n",
    "            if self.load(\"asteroids\") != False:\n",
    "                self.load(\"asteroids\")\n",
    "            else:\n",
    "                self.model = self.build_network(states, self.actions)\n",
    "\n",
    "        \n",
    "        self.dqn = self.build_agent(self.model, self.actions)\n",
    "        self.dqn.compile(Adam(beta_1 = 0.99, lr=self.learning_rates[self.learning_rate]), metrics=metrics)\n",
    "        \n",
    "    def train_model(self, episodes = 5, verbose = 1):\n",
    "        #Initialise the DQN agent\n",
    "        #Debug info\n",
    "        print(\"parameters: \", \"\\nlearning rate: \", self.learning_rates[self.learning_rate],\n",
    "             \"\\nepisodes: \",  self.episodes[self.episode],\n",
    "              \"\\nsteps: \", self.steps[self.step],\n",
    "              \"\\nvisualize: \", self.visualize,\n",
    "              \"\\nscale: \", self.scales[self.scale],\n",
    "              \"\\nintensity (asteroids only): \", self.intensity_rates[self.intensity],\n",
    "              \"\\nplayer speed : \", self.player_speeds[self.player_speed],\n",
    "              \"\\nhomogenous controls (space invaders only): \", self.homogenous_controls,\n",
    "              \"\\ncolour: \", self.colour)\n",
    "\n",
    "        print(\"beginning training\")\n",
    "        #Fit with openAI gym\n",
    "        self.dqn.fit(self.game, nb_steps=self.steps[self.step], visualize=self.visualize, verbose=2)\n",
    "\n",
    "        print(\"training complete\")\n",
    "        self.test_model()\n",
    "\n",
    "    \n",
    "    def test_model(self):\n",
    "        #Test with scores\n",
    "        action_space= Discrete(5)\n",
    "        \n",
    "        if self.test_env == 1:\n",
    "            training_game = Space_Invaders(self.scales[2][0], self.scales[2][1]\n",
    "                                    , self.surface, True, self.scales[2][2],\n",
    "                                    homogenous_controls = self.homogenous_controls)\n",
    "            self.game = Custom_OpenAI_Env(self.scales[2][0], self.scales[2][1],\n",
    "                                         training_game, colour=self.colour,\n",
    "                                         action_space = action_space)\n",
    "        elif self.test_env == 0:\n",
    "\n",
    "            training_game = Asteroids(self.scales[2][0], self.scales[2][1], \n",
    "                                self.surface, True, self.scales[2][2],\n",
    "                                intensity_modifier = self.intensity_rates[self.intensity])\n",
    "            self.game = Custom_OpenAI_Env(self.scales[2][0], self.scales[2][1],\n",
    "                                           training_game, colour=self.colour)\n",
    "        \n",
    "        if self.test_env != -1:\n",
    "            self.build_model()\n",
    "            \n",
    "        scores = self.dqn.test(self.game, nb_episodes=100, visualize=True)\n",
    "        print(np.mean(scores.history['episode_reward']))\n",
    "        \n",
    "        #if self.test_env == -1:\n",
    "        #    self.main_menu()\n",
    "        \n",
    "    def save(self, name='default-model'):\n",
    "        #was dqn now model\n",
    "        self.dqn.save_weights(name + \".hdf5\", overwrite=True)\n",
    "        \n",
    "    def load(self, name):\n",
    "        #Load existing weights into the blank model \n",
    "        if os.path.isfile(name + '.hdf5'):\n",
    "            return self.model.load_weights(name + '.hdf5')\n",
    "        print(\"file not found, creating default model.\")\n",
    "        return False\n",
    "    \n",
    "    def build_network(self, states, actions):\n",
    "        model = Sequential()\n",
    "        print(\"shape : \", len(states), actions)\n",
    "        #Convolutional layers\n",
    "        model.add(Conv2D(16, (10,10), activation='relu', input_shape=states, padding='same'))\n",
    "        model.add(MaxPooling2D((2, 2), padding = 'same'))\n",
    "        model.add(LeakyReLU(alpha=0.1))\n",
    "        model.add(Conv2D(32, (5,5), activation='relu', input_shape=states, padding='same'))\n",
    "        model.add(MaxPooling2D((2, 2), padding = 'same'))\n",
    "        model.add(LeakyReLU(alpha=0.1))\n",
    "        model.add(Flatten())\n",
    "        #Fully connected layers\n",
    "        model.add(Dense(256, activation='relu'))\n",
    "        model.add(LeakyReLU(alpha=0.1))\n",
    "        model.add(Dense(32, activation='relu'))\n",
    "        model.add(LeakyReLU(alpha=0.1))\n",
    "        model.add(Dense(32, activation='relu'))\n",
    "        model.add(LeakyReLU(alpha=0.1))\n",
    "        model.add(Dense(16, activation='relu'))\n",
    "        model.add(LeakyReLU(alpha=0.1))\n",
    "        model.add(Dense(self.actions, activation='relu'))\n",
    "        #Debug summary of the model built\n",
    "        print(model.summary())\n",
    "        return model\n",
    "\n",
    "    #Function to create DQN model \n",
    "    def build_agent(self, model, actions):\n",
    "        policy = BoltzmannQPolicy()\n",
    "        #policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=1., value_min=.1, value_test=.2, nb_steps=10000)\n",
    "        memory = SequentialMemory(limit=1000, window_length=WINDOW_LENGTH)\n",
    "        dqn = DQNAgent(model=model, memory=memory, policy=policy,\n",
    "                      enable_dueling_network=True, dueling_type='avg', \n",
    "                       nb_actions=actions, nb_steps_warmup=1000\n",
    "                      )\n",
    "        return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standard params called\n",
      "changing learning rate\n",
      "changing learning rate\n",
      "Starting space invaders training\n",
      "initialisation complete\n",
      "building model\n",
      "(3, 150, 150)\n",
      "shape :  3 5\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 3, 150, 16)        240016    \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 2, 75, 16)         0         \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 2, 75, 16)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 2, 75, 32)         12832     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 1, 38, 32)         0         \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 1, 38, 32)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1216)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               311552    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 5)                 85        \n",
      "=================================================================\n",
      "Total params: 574,293\n",
      "Trainable params: 574,293\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "parameters:  \n",
      "learning rate:  1e-07 \n",
      "episodes:  100 \n",
      "steps:  100000 \n",
      "visualize:  1 \n",
      "scale:  [150, 150, 0.45] \n",
      "intensity (asteroids only):  0 \n",
      "player speed :  1 \n",
      "homogenous controls (space invaders only):  False \n",
      "colour:  True\n",
      "beginning training\n",
      "Training for 100000 steps ...\n",
      "WARNING:tensorflow:From C:\\Users\\Chris\\anaconda3\\envs\\masters\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_v1.py:2070: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "   153/100000: episode: 1, duration: 2.574s, episode steps: 153, steps per second:  59, episode reward:  0.300, mean reward:  0.002 [ 0.000,  0.100], mean action: 2.660 [0.000, 4.000],  loss: --, mae: --, mean_q: --\n",
      "   509/100000: episode: 2, duration: 5.866s, episode steps: 356, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.975 [2.000, 4.000],  loss: --, mae: --, mean_q: --\n",
      "   699/100000: episode: 3, duration: 3.134s, episode steps: 190, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.911 [2.000, 4.000],  loss: --, mae: --, mean_q: --\n",
      "  1026/100000: episode: 4, duration: 9.558s, episode steps: 327, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.957 [2.000, 4.000],  loss: 0.893144, mae: 7.153330, mean_q: 11.373405\n",
      "  3069/100000: episode: 5, duration: 332.851s, episode steps: 2043, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.162 [0.000, 4.000],  loss: 1.107869, mae: 7.873816, mean_q: 13.206165\n",
      "  3322/100000: episode: 6, duration: 41.565s, episode steps: 253, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.913 [0.000, 4.000],  loss: 1.026926, mae: 6.077527, mean_q: 11.050783\n",
      "  3818/100000: episode: 7, duration: 81.268s, episode steps: 496, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.655 [1.000, 4.000],  loss: 0.877281, mae: 6.461475, mean_q: 11.200444\n",
      "  3987/100000: episode: 8, duration: 27.811s, episode steps: 169, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.959 [2.000, 4.000],  loss: 0.848688, mae: 7.474440, mean_q: 12.460595\n",
      "  4314/100000: episode: 9, duration: 53.542s, episode steps: 327, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.911 [2.000, 4.000],  loss: 0.848660, mae: 7.812884, mean_q: 12.727262\n",
      "  4495/100000: episode: 10, duration: 29.812s, episode steps: 181, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.917 [2.000, 4.000],  loss: 0.789323, mae: 7.683311, mean_q: 12.414246\n",
      "  4679/100000: episode: 11, duration: 30.206s, episode steps: 184, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.913 [0.000, 4.000],  loss: 0.792125, mae: 7.887407, mean_q: 12.530608\n",
      "  5007/100000: episode: 12, duration: 53.825s, episode steps: 328, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.915 [1.000, 4.000],  loss: 0.842674, mae: 7.684549, mean_q: 12.469576\n",
      "  5574/100000: episode: 13, duration: 92.768s, episode steps: 567, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.536 [1.000, 4.000],  loss: 0.686591, mae: 7.461851, mean_q: 12.274064\n",
      "  6131/100000: episode: 14, duration: 90.985s, episode steps: 557, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.524 [1.000, 4.000],  loss: 0.734496, mae: 7.594076, mean_q: 12.710883\n",
      "  6334/100000: episode: 15, duration: 33.233s, episode steps: 203, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.867 [1.000, 4.000],  loss: 0.842910, mae: 7.868835, mean_q: 12.940117\n",
      "  6662/100000: episode: 16, duration: 53.764s, episode steps: 328, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.924 [0.000, 4.000],  loss: 0.966178, mae: 7.659756, mean_q: 12.794493\n",
      "  6987/100000: episode: 17, duration: 53.111s, episode steps: 325, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.914 [0.000, 4.000],  loss: 0.834785, mae: 7.349090, mean_q: 12.568107\n",
      "  7156/100000: episode: 18, duration: 27.745s, episode steps: 169, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.888 [2.000, 4.000],  loss: 0.951211, mae: 7.031148, mean_q: 12.094509\n",
      "  7344/100000: episode: 19, duration: 30.724s, episode steps: 188, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.809 [0.000, 4.000],  loss: 0.793836, mae: 6.958196, mean_q: 12.126878\n",
      "  7640/100000: episode: 20, duration: 48.257s, episode steps: 296, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.902 [2.000, 4.000],  loss: 0.611528, mae: 7.021093, mean_q: 12.159744\n",
      "  7981/100000: episode: 21, duration: 55.658s, episode steps: 341, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.880 [1.000, 4.000],  loss: 0.612858, mae: 7.002574, mean_q: 12.234229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  8210/100000: episode: 22, duration: 37.483s, episode steps: 229, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.895 [2.000, 4.000],  loss: 0.621430, mae: 7.032332, mean_q: 12.274657\n",
      "  8786/100000: episode: 23, duration: 93.731s, episode steps: 576, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.488 [1.000, 4.000],  loss: 0.637991, mae: 6.924659, mean_q: 12.168443\n",
      "  9016/100000: episode: 24, duration: 37.542s, episode steps: 230, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.839 [2.000, 4.000],  loss: 0.812476, mae: 7.311093, mean_q: 12.515945\n",
      "  9237/100000: episode: 25, duration: 36.081s, episode steps: 221, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.824 [1.000, 4.000],  loss: 0.934108, mae: 7.222859, mean_q: 12.685339\n",
      "  9539/100000: episode: 26, duration: 49.166s, episode steps: 302, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.907 [2.000, 4.000],  loss: 0.879139, mae: 7.151465, mean_q: 12.599939\n",
      " 11818/100000: episode: 27, duration: 370.804s, episode steps: 2279, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.271 [0.000, 4.000],  loss: 0.908316, mae: 6.944495, mean_q: 13.132092\n",
      " 13772/100000: episode: 28, duration: 319.495s, episode steps: 1954, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.153 [1.000, 4.000],  loss: 0.903745, mae: 6.735184, mean_q: 13.434496\n",
      " 14014/100000: episode: 29, duration: 40.370s, episode steps: 242, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.806 [0.000, 4.000],  loss: 0.982646, mae: 5.897342, mean_q: 11.686413\n",
      " 14263/100000: episode: 30, duration: 41.432s, episode steps: 249, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.815 [1.000, 4.000],  loss: 0.991015, mae: 5.604302, mean_q: 11.037829\n",
      " 14715/100000: episode: 31, duration: 74.548s, episode steps: 452, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.754 [0.000, 4.000],  loss: 0.970485, mae: 6.023636, mean_q: 11.654166\n",
      " 16933/100000: episode: 32, duration: 364.930s, episode steps: 2218, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.220 [0.000, 4.000],  loss: 0.790762, mae: 6.917952, mean_q: 13.222797\n",
      " 17573/100000: episode: 33, duration: 104.445s, episode steps: 640, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.347 [1.000, 4.000],  loss: 0.849079, mae: 5.662225, mean_q: 11.763121\n",
      " 17926/100000: episode: 34, duration: 57.677s, episode steps: 353, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.799 [0.000, 4.000],  loss: 1.014113, mae: 6.705052, mean_q: 13.180284\n",
      " 18380/100000: episode: 35, duration: 74.063s, episode steps: 454, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.659 [1.000, 4.000],  loss: 0.873448, mae: 6.785331, mean_q: 12.777925\n",
      " 18699/100000: episode: 36, duration: 52.085s, episode steps: 319, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.815 [2.000, 4.000],  loss: 0.664775, mae: 6.578607, mean_q: 12.364878\n",
      " 18976/100000: episode: 37, duration: 45.264s, episode steps: 277, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.845 [0.000, 4.000],  loss: 0.552581, mae: 6.365333, mean_q: 12.124187\n",
      " 19340/100000: episode: 38, duration: 59.494s, episode steps: 364, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.791 [2.000, 4.000],  loss: 0.612163, mae: 6.248563, mean_q: 12.015944\n",
      " 23288/100000: episode: 39, duration: 652.386s, episode steps: 3948, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.432 [1.000, 4.000],  loss: 0.690291, mae: 6.117543, mean_q: 13.511393\n",
      " 23848/100000: episode: 40, duration: 91.551s, episode steps: 560, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.432 [1.000, 4.000],  loss: 0.670732, mae: 5.344847, mean_q: 10.616210\n",
      " 24116/100000: episode: 41, duration: 43.836s, episode steps: 268, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.739 [0.000, 4.000],  loss: 0.858285, mae: 6.340595, mean_q: 11.536765\n",
      " 24341/100000: episode: 42, duration: 36.870s, episode steps: 225, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.796 [0.000, 4.000],  loss: 0.967841, mae: 6.537395, mean_q: 12.085636\n",
      " 24524/100000: episode: 43, duration: 30.097s, episode steps: 183, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.770 [1.000, 4.000],  loss: 0.939600, mae: 6.520512, mean_q: 12.476435\n",
      " 24748/100000: episode: 44, duration: 36.739s, episode steps: 224, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.830 [0.000, 4.000],  loss: 0.922596, mae: 6.654478, mean_q: 12.447665\n",
      " 25094/100000: episode: 45, duration: 56.686s, episode steps: 346, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.772 [0.000, 4.000],  loss: 1.052662, mae: 6.226220, mean_q: 12.100516\n",
      " 27152/100000: episode: 46, duration: 337.577s, episode steps: 2058, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.132 [1.000, 4.000],  loss: 0.782825, mae: 6.781424, mean_q: 13.103808\n",
      " 27422/100000: episode: 47, duration: 45.245s, episode steps: 270, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.793 [2.000, 4.000],  loss: 0.784736, mae: 5.426983, mean_q: 10.963494\n",
      " 27679/100000: episode: 48, duration: 42.998s, episode steps: 257, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.817 [2.000, 4.000],  loss: 0.754821, mae: 5.481438, mean_q: 10.900153\n",
      " 28281/100000: episode: 49, duration: 99.921s, episode steps: 602, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.382 [1.000, 4.000],  loss: 0.817079, mae: 6.042526, mean_q: 11.940418\n",
      " 28748/100000: episode: 50, duration: 77.353s, episode steps: 467, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.578 [1.000, 4.000],  loss: 0.770673, mae: 6.616810, mean_q: 12.652619\n",
      " 29207/100000: episode: 51, duration: 76.009s, episode steps: 459, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.601 [0.000, 4.000],  loss: 0.943356, mae: 6.827024, mean_q: 12.526912\n",
      " 29667/100000: episode: 52, duration: 76.135s, episode steps: 460, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.657 [1.000, 4.000],  loss: 0.816340, mae: 6.698023, mean_q: 12.200262\n",
      " 30056/100000: episode: 53, duration: 64.375s, episode steps: 389, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.779 [2.000, 4.000],  loss: 0.710319, mae: 6.617077, mean_q: 12.130264\n",
      " 30692/100000: episode: 54, duration: 105.331s, episode steps: 636, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.327 [0.000, 4.000],  loss: 0.736292, mae: 6.516199, mean_q: 11.956376\n",
      " 30910/100000: episode: 55, duration: 36.157s, episode steps: 218, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.821 [2.000, 4.000],  loss: 0.828291, mae: 6.648530, mean_q: 12.346626\n",
      " 31179/100000: episode: 56, duration: 44.626s, episode steps: 269, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.829 [0.000, 4.000],  loss: 0.899108, mae: 6.633809, mean_q: 12.448423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 33437/100000: episode: 57, duration: 376.011s, episode steps: 2258, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.151 [1.000, 4.000],  loss: 0.747612, mae: 6.487613, mean_q: 12.904465\n",
      " 33780/100000: episode: 58, duration: 57.152s, episode steps: 343, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.743 [0.000, 4.000],  loss: 0.673948, mae: 5.220836, mean_q: 11.323541\n",
      " 34000/100000: episode: 59, duration: 36.506s, episode steps: 220, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.855 [2.000, 4.000],  loss: 0.792949, mae: 5.626153, mean_q: 11.736444\n",
      " 34288/100000: episode: 60, duration: 47.550s, episode steps: 288, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.753 [1.000, 4.000],  loss: 1.037877, mae: 5.873625, mean_q: 12.299480\n",
      " 34645/100000: episode: 61, duration: 58.865s, episode steps: 357, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.739 [1.000, 4.000],  loss: 1.017130, mae: 5.972459, mean_q: 11.988278\n",
      " 35184/100000: episode: 62, duration: 88.851s, episode steps: 539, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.486 [1.000, 4.000],  loss: 0.861118, mae: 6.074797, mean_q: 11.799001\n",
      " 35663/100000: episode: 63, duration: 78.807s, episode steps: 479, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.524 [1.000, 4.000],  loss: 0.737336, mae: 6.511276, mean_q: 12.157830\n",
      " 35826/100000: episode: 64, duration: 26.961s, episode steps: 163, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.853 [0.000, 4.000],  loss: 1.058039, mae: 6.748320, mean_q: 12.513963\n",
      " 36290/100000: episode: 65, duration: 76.688s, episode steps: 464, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.593 [1.000, 4.000],  loss: 1.028206, mae: 6.717424, mean_q: 12.476358\n",
      " 38137/100000: episode: 66, duration: 304.032s, episode steps: 1847, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.049 [0.000, 4.000],  loss: 0.679229, mae: 6.864481, mean_q: 13.168863\n",
      " 38393/100000: episode: 67, duration: 42.360s, episode steps: 256, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.844 [1.000, 4.000],  loss: 0.705049, mae: 5.810354, mean_q: 11.669220\n",
      " 38668/100000: episode: 68, duration: 45.532s, episode steps: 275, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.760 [2.000, 4.000],  loss: 0.783887, mae: 5.426371, mean_q: 10.927180\n",
      " 38934/100000: episode: 69, duration: 44.045s, episode steps: 266, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.801 [2.000, 4.000],  loss: 0.802139, mae: 5.637204, mean_q: 10.997814\n",
      " 39120/100000: episode: 70, duration: 30.824s, episode steps: 186, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.747 [0.000, 4.000],  loss: 1.005852, mae: 5.919953, mean_q: 11.689259\n",
      " 42823/100000: episode: 71, duration: 629.892s, episode steps: 3703, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.394 [1.000, 4.000],  loss: 0.664904, mae: 6.069966, mean_q: 13.521060\n",
      " 43441/100000: episode: 72, duration: 101.380s, episode steps: 618, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.343 [0.000, 4.000],  loss: 0.663352, mae: 5.036806, mean_q: 11.607667\n",
      " 45394/100000: episode: 73, duration: 319.488s, episode steps: 1953, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.092 [1.000, 4.000],  loss: 0.659182, mae: 6.653771, mean_q: 13.173865\n",
      " 45647/100000: episode: 74, duration: 41.598s, episode steps: 253, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.850 [2.000, 4.000],  loss: 0.633164, mae: 5.696043, mean_q: 11.514729\n",
      " 46287/100000: episode: 75, duration: 108.181s, episode steps: 640, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.322 [0.000, 4.000],  loss: 0.727007, mae: 5.698731, mean_q: 11.296511\n",
      " 48182/100000: episode: 76, duration: 320.713s, episode steps: 1895, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.090 [0.000, 4.000],  loss: 0.629764, mae: 6.795856, mean_q: 13.343546\n",
      " 50462/100000: episode: 77, duration: 379.931s, episode steps: 2280, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.179 [0.000, 4.000],  loss: 0.534817, mae: 6.178714, mean_q: 12.502095\n",
      " 50761/100000: episode: 78, duration: 50.065s, episode steps: 299, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.813 [0.000, 4.000],  loss: 0.570274, mae: 5.189145, mean_q: 11.402326\n",
      " 50986/100000: episode: 79, duration: 37.689s, episode steps: 225, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.738 [0.000, 4.000],  loss: 0.793796, mae: 5.387444, mean_q: 11.757055\n",
      " 51151/100000: episode: 80, duration: 27.723s, episode steps: 165, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.788 [0.000, 4.000],  loss: 0.768414, mae: 5.665031, mean_q: 12.207678\n",
      " 51332/100000: episode: 81, duration: 30.227s, episode steps: 181, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.746 [2.000, 4.000],  loss: 0.969333, mae: 5.941877, mean_q: 12.347093\n",
      " 51934/100000: episode: 82, duration: 99.981s, episode steps: 602, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.336 [1.000, 4.000],  loss: 0.853397, mae: 6.129277, mean_q: 11.924492\n",
      " 52148/100000: episode: 83, duration: 35.602s, episode steps: 214, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.855 [1.000, 4.000],  loss: 0.727131, mae: 6.525452, mean_q: 12.428344\n",
      " 52337/100000: episode: 84, duration: 31.397s, episode steps: 189, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.810 [2.000, 4.000],  loss: 0.799641, mae: 6.438670, mean_q: 12.244207\n",
      " 52518/100000: episode: 85, duration: 30.145s, episode steps: 181, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.807 [0.000, 4.000],  loss: 0.745548, mae: 6.416890, mean_q: 12.204050\n",
      " 52734/100000: episode: 86, duration: 35.984s, episode steps: 216, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.819 [0.000, 4.000],  loss: 0.911545, mae: 6.527736, mean_q: 12.277637\n",
      " 53059/100000: episode: 87, duration: 54.019s, episode steps: 325, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.791 [2.000, 4.000],  loss: 1.035739, mae: 6.103913, mean_q: 11.826754\n",
      " 53323/100000: episode: 88, duration: 43.850s, episode steps: 264, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.826 [2.000, 4.000],  loss: 0.925512, mae: 5.890649, mean_q: 11.635992\n",
      " 53672/100000: episode: 89, duration: 57.916s, episode steps: 349, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.794 [2.000, 4.000],  loss: 0.973780, mae: 5.702806, mean_q: 11.671363\n",
      " 54233/100000: episode: 90, duration: 92.916s, episode steps: 561, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.403 [1.000, 4.000],  loss: 0.782501, mae: 5.819889, mean_q: 11.658640\n",
      " 54449/100000: episode: 91, duration: 35.866s, episode steps: 216, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.810 [1.000, 4.000],  loss: 0.690422, mae: 6.260055, mean_q: 12.137889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 54778/100000: episode: 92, duration: 54.564s, episode steps: 329, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.796 [0.000, 4.000],  loss: 0.804757, mae: 6.180227, mean_q: 12.093407\n",
      " 57031/100000: episode: 93, duration: 373.208s, episode steps: 2253, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.146 [1.000, 4.000],  loss: 0.605429, mae: 6.369172, mean_q: 12.762829\n",
      " 57403/100000: episode: 94, duration: 62.106s, episode steps: 372, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.774 [1.000, 4.000],  loss: 0.607281, mae: 5.136412, mean_q: 11.291678\n",
      " 57792/100000: episode: 95, duration: 64.842s, episode steps: 389, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.753 [2.000, 4.000],  loss: 0.702244, mae: 5.667836, mean_q: 11.874360\n",
      " 58164/100000: episode: 96, duration: 61.756s, episode steps: 372, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.750 [1.000, 4.000],  loss: 0.726573, mae: 6.072794, mean_q: 11.847580\n",
      " 58449/100000: episode: 97, duration: 47.223s, episode steps: 285, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.723 [0.000, 4.000],  loss: 0.654693, mae: 6.192532, mean_q: 11.457995\n",
      " 58951/100000: episode: 98, duration: 83.165s, episode steps: 502, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.436 [1.000, 4.000],  loss: 0.679706, mae: 5.964721, mean_q: 11.561892\n",
      " 59445/100000: episode: 99, duration: 81.766s, episode steps: 494, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.425 [1.000, 4.000],  loss: 0.701533, mae: 6.219461, mean_q: 11.966067\n",
      " 59727/100000: episode: 100, duration: 46.752s, episode steps: 282, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.699 [1.000, 4.000],  loss: 0.700726, mae: 6.561148, mean_q: 12.285573\n",
      " 60069/100000: episode: 101, duration: 56.687s, episode steps: 342, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.699 [2.000, 4.000],  loss: 0.819907, mae: 6.319829, mean_q: 12.175967\n",
      " 60291/100000: episode: 102, duration: 36.957s, episode steps: 222, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.788 [1.000, 4.000],  loss: 0.747090, mae: 6.137401, mean_q: 11.743217\n",
      " 60519/100000: episode: 103, duration: 37.836s, episode steps: 228, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.811 [2.000, 4.000],  loss: 0.799910, mae: 5.912199, mean_q: 11.705008\n",
      " 60856/100000: episode: 104, duration: 55.910s, episode steps: 337, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.718 [0.000, 4.000],  loss: 0.858812, mae: 5.826079, mean_q: 11.497405\n",
      " 61069/100000: episode: 105, duration: 35.429s, episode steps: 213, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.756 [1.000, 4.000],  loss: 0.813910, mae: 5.868479, mean_q: 11.368992\n",
      " 61549/100000: episode: 106, duration: 79.561s, episode steps: 480, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.442 [1.000, 4.000],  loss: 0.859687, mae: 5.787732, mean_q: 11.454392\n",
      " 61946/100000: episode: 107, duration: 65.874s, episode steps: 397, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.675 [2.000, 4.000],  loss: 0.895158, mae: 6.119072, mean_q: 11.734982\n",
      " 62603/100000: episode: 108, duration: 108.666s, episode steps: 657, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.236 [1.000, 4.000],  loss: 0.817458, mae: 6.393971, mean_q: 11.834258\n",
      " 64521/100000: episode: 109, duration: 317.437s, episode steps: 1918, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.055 [1.000, 4.000],  loss: 0.592764, mae: 6.668675, mean_q: 13.150572\n",
      " 65213/100000: episode: 110, duration: 114.188s, episode steps: 692, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.360 [1.000, 4.000],  loss: 0.548995, mae: 5.559196, mean_q: 11.258803\n",
      " 65569/100000: episode: 111, duration: 58.810s, episode steps: 356, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.615 [2.000, 4.000],  loss: 0.663996, mae: 6.225421, mean_q: 12.212408\n",
      " 66090/100000: episode: 112, duration: 85.785s, episode steps: 521, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.374 [0.000, 4.000],  loss: 0.581409, mae: 6.316860, mean_q: 12.296440\n",
      " 66298/100000: episode: 113, duration: 34.373s, episode steps: 208, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.774 [1.000, 4.000],  loss: 0.778319, mae: 6.243316, mean_q: 11.955252\n",
      " 66529/100000: episode: 114, duration: 38.169s, episode steps: 231, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.749 [1.000, 4.000],  loss: 0.829387, mae: 6.167123, mean_q: 11.720877\n",
      " 66893/100000: episode: 115, duration: 60.036s, episode steps: 364, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.723 [1.000, 4.000],  loss: 0.913541, mae: 6.099961, mean_q: 11.814588\n",
      " 67143/100000: episode: 116, duration: 41.307s, episode steps: 250, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.808 [2.000, 4.000],  loss: 0.971654, mae: 6.002429, mean_q: 11.651796\n",
      " 67218/100000: episode: 117, duration: 12.499s, episode steps:  75, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.560 [2.000, 4.000],  loss: 1.035707, mae: 5.790262, mean_q: 11.681570\n",
      " 67607/100000: episode: 118, duration: 64.341s, episode steps: 389, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.666 [2.000, 4.000],  loss: 0.916259, mae: 5.910367, mean_q: 11.457794\n",
      " 67851/100000: episode: 119, duration: 40.476s, episode steps: 244, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.836 [0.000, 4.000],  loss: 0.736457, mae: 6.064255, mean_q: 11.297970\n",
      " 68032/100000: episode: 120, duration: 30.009s, episode steps: 181, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.646 [0.000, 4.000],  loss: 1.009926, mae: 6.030290, mean_q: 11.566390\n",
      " 68298/100000: episode: 121, duration: 44.036s, episode steps: 266, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.699 [0.000, 4.000],  loss: 0.911727, mae: 6.087176, mean_q: 11.533177\n",
      " 68837/100000: episode: 122, duration: 88.974s, episode steps: 539, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.364 [0.000, 4.000],  loss: 0.765444, mae: 6.056563, mean_q: 11.691536\n",
      " 69016/100000: episode: 123, duration: 29.655s, episode steps: 179, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.682 [1.000, 4.000],  loss: 0.695133, mae: 6.311290, mean_q: 12.093886\n",
      " 69090/100000: episode: 124, duration: 12.353s, episode steps:  74, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.527 [0.000, 4.000],  loss: 0.615108, mae: 6.337818, mean_q: 12.041990\n",
      " 69362/100000: episode: 125, duration: 44.993s, episode steps: 272, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.713 [2.000, 4.000],  loss: 0.755441, mae: 6.355331, mean_q: 11.856956\n",
      " 69836/100000: episode: 126, duration: 78.254s, episode steps: 474, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.483 [1.000, 4.000],  loss: 0.796495, mae: 6.292920, mean_q: 11.838037\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 70333/100000: episode: 127, duration: 81.933s, episode steps: 497, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.437 [1.000, 4.000],  loss: 0.780588, mae: 6.095946, mean_q: 11.739261\n",
      " 70606/100000: episode: 128, duration: 45.110s, episode steps: 273, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.700 [1.000, 4.000],  loss: 0.734181, mae: 6.253467, mean_q: 11.877147\n",
      " 70792/100000: episode: 129, duration: 30.773s, episode steps: 186, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.763 [2.000, 4.000],  loss: 0.845699, mae: 6.304060, mean_q: 11.896191\n",
      " 71165/100000: episode: 130, duration: 61.618s, episode steps: 373, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.697 [1.000, 4.000],  loss: 0.637113, mae: 6.068664, mean_q: 11.737287\n",
      " 71663/100000: episode: 131, duration: 82.082s, episode steps: 498, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.434 [1.000, 4.000],  loss: 0.637016, mae: 5.909657, mean_q: 11.461069\n",
      " 71837/100000: episode: 132, duration: 28.815s, episode steps: 174, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.764 [2.000, 4.000],  loss: 0.630978, mae: 6.178834, mean_q: 11.633389\n",
      " 72519/100000: episode: 133, duration: 112.393s, episode steps: 682, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.302 [1.000, 4.000],  loss: 0.650680, mae: 6.292455, mean_q: 11.925074\n",
      " 72728/100000: episode: 134, duration: 34.585s, episode steps: 209, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.722 [2.000, 4.000],  loss: 0.763258, mae: 6.446212, mean_q: 12.423365\n",
      " 72913/100000: episode: 135, duration: 30.623s, episode steps: 185, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.703 [1.000, 4.000],  loss: 0.751241, mae: 6.208455, mean_q: 12.268238\n",
      " 73199/100000: episode: 136, duration: 47.201s, episode steps: 286, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.699 [1.000, 4.000],  loss: 0.779528, mae: 6.304121, mean_q: 12.293861\n",
      " 73464/100000: episode: 137, duration: 43.817s, episode steps: 265, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.672 [1.000, 4.000],  loss: 0.844558, mae: 5.942484, mean_q: 11.967856\n",
      " 73766/100000: episode: 138, duration: 49.909s, episode steps: 302, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.702 [0.000, 4.000],  loss: 0.754860, mae: 5.631194, mean_q: 11.552307\n",
      " 73999/100000: episode: 139, duration: 38.506s, episode steps: 233, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.790 [2.000, 4.000],  loss: 0.761324, mae: 5.499312, mean_q: 11.501093\n",
      " 77843/100000: episode: 140, duration: 643.772s, episode steps: 3844, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.406 [1.000, 4.000],  loss: 0.520135, mae: 5.671922, mean_q: 12.957392\n",
      " 78098/100000: episode: 141, duration: 41.825s, episode steps: 255, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.698 [0.000, 4.000],  loss: 0.768061, mae: 4.372291, mean_q: 10.685994\n",
      " 78711/100000: episode: 142, duration: 100.373s, episode steps: 613, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.266 [1.000, 4.000],  loss: 0.719761, mae: 5.010489, mean_q: 10.892861\n",
      " 79042/100000: episode: 143, duration: 54.230s, episode steps: 331, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.722 [0.000, 4.000],  loss: 0.660825, mae: 5.979136, mean_q: 12.105499\n",
      " 79348/100000: episode: 144, duration: 50.212s, episode steps: 306, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.699 [2.000, 4.000],  loss: 0.691637, mae: 5.912926, mean_q: 11.949719\n",
      " 79571/100000: episode: 145, duration: 36.676s, episode steps: 223, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.785 [1.000, 4.000],  loss: 0.738313, mae: 5.725352, mean_q: 11.861320\n",
      " 80036/100000: episode: 146, duration: 76.256s, episode steps: 465, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.523 [0.000, 4.000],  loss: 0.738642, mae: 5.378595, mean_q: 11.444112\n",
      " 82227/100000: episode: 147, duration: 357.556s, episode steps: 2191, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.169 [1.000, 4.000],  loss: 0.494727, mae: 6.107519, mean_q: 12.472481\n",
      " 82535/100000: episode: 148, duration: 50.825s, episode steps: 308, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.646 [1.000, 4.000],  loss: 0.675915, mae: 4.897842, mean_q: 10.913373\n",
      " 82696/100000: episode: 149, duration: 26.688s, episode steps: 161, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.764 [0.000, 4.000],  loss: 0.672863, mae: 4.956068, mean_q: 10.898597\n",
      " 82992/100000: episode: 150, duration: 49.005s, episode steps: 296, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.716 [2.000, 4.000],  loss: 0.772835, mae: 5.249887, mean_q: 11.490820\n",
      " 83571/100000: episode: 151, duration: 95.586s, episode steps: 579, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.383 [1.000, 4.000],  loss: 0.745038, mae: 5.439961, mean_q: 11.501572\n",
      " 83916/100000: episode: 152, duration: 56.964s, episode steps: 345, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.652 [2.000, 4.000],  loss: 0.602865, mae: 5.711329, mean_q: 11.786917\n",
      " 84523/100000: episode: 153, duration: 100.143s, episode steps: 607, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.239 [1.000, 4.000],  loss: 0.635618, mae: 5.892693, mean_q: 11.725928\n",
      " 85014/100000: episode: 154, duration: 80.953s, episode steps: 491, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.342 [0.000, 4.000],  loss: 0.512611, mae: 5.923586, mean_q: 11.814291\n",
      " 85301/100000: episode: 155, duration: 47.338s, episode steps: 287, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.592 [0.000, 4.000],  loss: 0.661485, mae: 6.241545, mean_q: 12.105944\n",
      " 85506/100000: episode: 156, duration: 33.946s, episode steps: 205, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.668 [2.000, 4.000],  loss: 0.673698, mae: 5.780550, mean_q: 11.744370\n",
      " 86078/100000: episode: 157, duration: 94.373s, episode steps: 572, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.292 [0.000, 4.000],  loss: 0.648987, mae: 5.666603, mean_q: 11.580894\n",
      " 86254/100000: episode: 158, duration: 29.159s, episode steps: 176, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.653 [0.000, 4.000],  loss: 0.727020, mae: 5.761292, mean_q: 11.674619\n",
      " 86490/100000: episode: 159, duration: 39.052s, episode steps: 236, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.657 [1.000, 4.000],  loss: 0.765256, mae: 5.959686, mean_q: 11.797649\n",
      " 86773/100000: episode: 160, duration: 46.939s, episode steps: 283, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.636 [1.000, 4.000],  loss: 0.678250, mae: 5.929840, mean_q: 11.817926\n",
      " 87380/100000: episode: 161, duration: 100.204s, episode steps: 607, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.249 [0.000, 4.000],  loss: 0.688922, mae: 5.698905, mean_q: 11.640323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 87662/100000: episode: 162, duration: 46.587s, episode steps: 282, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.550 [0.000, 4.000],  loss: 0.672537, mae: 5.782821, mean_q: 11.822300\n",
      " 88213/100000: episode: 163, duration: 91.007s, episode steps: 551, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.290 [1.000, 4.000],  loss: 0.646173, mae: 5.904868, mean_q: 11.978782\n",
      " 90164/100000: episode: 164, duration: 321.047s, episode steps: 1951, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.026 [1.000, 4.000],  loss: 0.548917, mae: 6.219874, mean_q: 12.795177\n",
      " 90334/100000: episode: 165, duration: 28.303s, episode steps: 170, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.471 [2.000, 4.000],  loss: 0.580765, mae: 5.564011, mean_q: 11.672005\n",
      " 90538/100000: episode: 166, duration: 33.873s, episode steps: 204, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.485 [2.000, 4.000],  loss: 0.577652, mae: 5.325888, mean_q: 10.925488\n",
      " 90722/100000: episode: 167, duration: 30.622s, episode steps: 184, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.478 [2.000, 4.000],  loss: 0.636530, mae: 5.394792, mean_q: 10.809532\n",
      " 90797/100000: episode: 168, duration: 12.655s, episode steps:  75, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.533 [2.000, 4.000],  loss: 0.603502, mae: 5.533220, mean_q: 10.740908\n",
      " 90987/100000: episode: 169, duration: 31.658s, episode steps: 190, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.374 [2.000, 4.000],  loss: 0.734931, mae: 5.716052, mean_q: 10.958237\n",
      " 91212/100000: episode: 170, duration: 37.410s, episode steps: 225, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.462 [0.000, 4.000],  loss: 0.842644, mae: 6.010593, mean_q: 11.374743\n",
      " 91566/100000: episode: 171, duration: 58.594s, episode steps: 354, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.508 [2.000, 4.000],  loss: 0.715380, mae: 5.901509, mean_q: 11.354184\n",
      " 91792/100000: episode: 172, duration: 37.569s, episode steps: 226, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.451 [2.000, 4.000],  loss: 0.807821, mae: 5.848804, mean_q: 11.213716\n",
      " 92005/100000: episode: 173, duration: 35.294s, episode steps: 213, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.427 [1.000, 4.000],  loss: 0.786256, mae: 5.777827, mean_q: 11.319934\n",
      " 92275/100000: episode: 174, duration: 44.745s, episode steps: 270, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.452 [1.000, 4.000],  loss: 0.728634, mae: 5.659692, mean_q: 11.296518\n",
      " 94034/100000: episode: 175, duration: 289.670s, episode steps: 1759, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.027 [1.000, 4.000],  loss: 0.537477, mae: 6.147910, mean_q: 12.607210\n",
      " 94218/100000: episode: 176, duration: 30.427s, episode steps: 184, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.462 [1.000, 4.000],  loss: 0.488244, mae: 5.871628, mean_q: 12.283257\n",
      " 94670/100000: episode: 177, duration: 74.455s, episode steps: 452, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.367 [1.000, 4.000],  loss: 0.505734, mae: 5.463645, mean_q: 11.342538\n",
      " 95229/100000: episode: 178, duration: 92.063s, episode steps: 559, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.231 [0.000, 4.000],  loss: 0.522510, mae: 5.854823, mean_q: 11.343652\n",
      " 95585/100000: episode: 179, duration: 58.744s, episode steps: 356, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.469 [1.000, 4.000],  loss: 0.474317, mae: 6.116297, mean_q: 11.871611\n",
      " 96234/100000: episode: 180, duration: 106.842s, episode steps: 649, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.233 [2.000, 4.000],  loss: 0.482227, mae: 5.884979, mean_q: 11.753762\n",
      " 96458/100000: episode: 181, duration: 36.910s, episode steps: 224, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.375 [1.000, 4.000],  loss: 0.543269, mae: 5.978978, mean_q: 11.998069\n",
      " 96687/100000: episode: 182, duration: 37.786s, episode steps: 229, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.328 [1.000, 4.000],  loss: 0.661062, mae: 6.050294, mean_q: 12.028203\n",
      " 97197/100000: episode: 183, duration: 83.943s, episode steps: 510, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.239 [1.000, 4.000],  loss: 0.645455, mae: 5.934397, mean_q: 12.058632\n",
      " 97704/100000: episode: 184, duration: 83.376s, episode steps: 507, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.274 [1.000, 4.000],  loss: 0.612406, mae: 5.839456, mean_q: 11.760816\n",
      " 98083/100000: episode: 185, duration: 62.412s, episode steps: 379, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.464 [1.000, 4.000],  loss: 0.537618, mae: 6.007236, mean_q: 11.971127\n",
      " 98591/100000: episode: 186, duration: 83.523s, episode steps: 508, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.217 [0.000, 4.000],  loss: 0.519391, mae: 5.831138, mean_q: 11.651738\n",
      " 98667/100000: episode: 187, duration: 12.660s, episode steps:  76, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.382 [0.000, 4.000],  loss: 0.665766, mae: 5.787437, mean_q: 11.730350\n",
      " 99047/100000: episode: 188, duration: 62.596s, episode steps: 380, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.416 [0.000, 4.000],  loss: 0.596372, mae: 5.822323, mean_q: 11.483561\n",
      " 99224/100000: episode: 189, duration: 29.249s, episode steps: 177, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.407 [0.000, 4.000],  loss: 0.603156, mae: 5.805916, mean_q: 11.425516\n",
      " 99622/100000: episode: 190, duration: 65.622s, episode steps: 398, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3.364 [0.000, 4.000],  loss: 0.623351, mae: 5.811144, mean_q: 11.413209\n",
      "done, took 16378.980 seconds\n",
      "training complete\n",
      "Testing for 100 episodes ...\n",
      "Episode 1: reward: 0.000, steps: 235\n",
      "Episode 2: reward: 0.000, steps: 336\n",
      "Episode 3: reward: 0.000, steps: 184\n",
      "Episode 4: reward: 0.000, steps: 385\n",
      "Episode 5: reward: 0.000, steps: 212\n",
      "Episode 6: reward: 0.000, steps: 258\n",
      "Episode 7: reward: 0.000, steps: 1954\n",
      "Episode 8: reward: 0.000, steps: 225\n",
      "Episode 9: reward: 0.000, steps: 529\n",
      "Episode 10: reward: 0.000, steps: 191\n",
      "Episode 11: reward: 0.000, steps: 238\n",
      "Episode 12: reward: 0.000, steps: 227\n",
      "Episode 13: reward: 0.000, steps: 498\n",
      "Episode 14: reward: 0.000, steps: 516\n",
      "Episode 15: reward: 0.000, steps: 203\n",
      "Episode 16: reward: 0.000, steps: 240\n",
      "Episode 17: reward: 0.000, steps: 189\n",
      "Episode 18: reward: 0.000, steps: 214\n",
      "Episode 19: reward: 0.000, steps: 264\n",
      "Episode 20: reward: 0.000, steps: 340\n",
      "Episode 21: reward: 0.000, steps: 211\n",
      "Episode 22: reward: 0.000, steps: 2001\n",
      "Episode 23: reward: 0.000, steps: 684\n",
      "Episode 24: reward: 0.000, steps: 219\n",
      "Episode 25: reward: 0.000, steps: 220\n",
      "Episode 26: reward: 0.000, steps: 245\n",
      "Episode 27: reward: 0.000, steps: 2326\n",
      "Episode 28: reward: 0.000, steps: 267\n",
      "Episode 29: reward: 0.000, steps: 186\n",
      "Episode 30: reward: 0.000, steps: 253\n",
      "Episode 31: reward: 0.000, steps: 205\n",
      "Episode 32: reward: 0.000, steps: 527\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 33: reward: 0.000, steps: 461\n",
      "Episode 34: reward: 0.000, steps: 220\n",
      "Episode 35: reward: 0.000, steps: 234\n",
      "Episode 36: reward: 0.000, steps: 272\n",
      "Episode 37: reward: 0.000, steps: 1909\n",
      "Episode 38: reward: 0.000, steps: 532\n",
      "Episode 39: reward: 0.000, steps: 491\n",
      "Episode 40: reward: 0.000, steps: 173\n",
      "Episode 41: reward: 0.000, steps: 593\n",
      "Episode 42: reward: 0.000, steps: 292\n",
      "Episode 43: reward: 0.000, steps: 291\n",
      "Episode 44: reward: 0.000, steps: 334\n",
      "Episode 45: reward: 0.000, steps: 663\n",
      "Episode 46: reward: 0.000, steps: 361\n",
      "Episode 47: reward: 0.000, steps: 249\n",
      "Episode 48: reward: 0.000, steps: 256\n",
      "Episode 49: reward: 0.000, steps: 374\n",
      "Episode 50: reward: 0.000, steps: 266\n",
      "Episode 51: reward: 0.000, steps: 622\n",
      "Episode 52: reward: 0.000, steps: 176\n",
      "Episode 53: reward: 0.000, steps: 1759\n",
      "Episode 54: reward: 0.000, steps: 460\n",
      "Episode 55: reward: 0.000, steps: 275\n",
      "Episode 56: reward: 0.000, steps: 531\n",
      "Episode 57: reward: 0.000, steps: 271\n",
      "Episode 58: reward: 0.000, steps: 545\n",
      "Episode 59: reward: 0.000, steps: 296\n",
      "Episode 60: reward: 0.000, steps: 537\n",
      "Episode 61: reward: 0.000, steps: 334\n",
      "Episode 62: reward: 0.000, steps: 1926\n",
      "Episode 63: reward: 0.000, steps: 288\n",
      "Episode 64: reward: 0.000, steps: 512\n",
      "Episode 65: reward: 0.000, steps: 292\n",
      "Episode 66: reward: 0.000, steps: 2085\n",
      "Episode 67: reward: 0.000, steps: 573\n",
      "Episode 68: reward: 0.000, steps: 2024\n",
      "Episode 69: reward: 0.000, steps: 174\n",
      "Episode 70: reward: 0.000, steps: 346\n",
      "Episode 71: reward: 0.000, steps: 176\n",
      "Episode 72: reward: 0.000, steps: 2296\n",
      "Episode 73: reward: 0.000, steps: 240\n",
      "Episode 74: reward: 0.000, steps: 273\n",
      "Episode 75: reward: 0.000, steps: 188\n",
      "Episode 76: reward: 0.000, steps: 1856\n",
      "Episode 77: reward: 0.000, steps: 495\n",
      "Episode 78: reward: 0.000, steps: 184\n",
      "Episode 79: reward: 0.000, steps: 232\n",
      "Episode 80: reward: 0.000, steps: 210\n",
      "Episode 81: reward: 0.000, steps: 296\n",
      "Episode 82: reward: 0.000, steps: 259\n",
      "Episode 83: reward: 0.000, steps: 224\n",
      "Episode 84: reward: 0.000, steps: 212\n",
      "Episode 85: reward: 0.000, steps: 206\n",
      "Episode 86: reward: 0.000, steps: 2187\n",
      "Episode 87: reward: 0.000, steps: 272\n",
      "Episode 88: reward: 0.000, steps: 173\n",
      "Episode 89: reward: 0.000, steps: 2360\n",
      "Episode 90: reward: 0.000, steps: 221\n",
      "Episode 91: reward: 0.000, steps: 268\n",
      "Episode 92: reward: 0.000, steps: 221\n",
      "Episode 93: reward: 0.000, steps: 324\n",
      "Episode 94: reward: 0.000, steps: 276\n",
      "Episode 95: reward: 0.000, steps: 2127\n",
      "Episode 96: reward: 0.000, steps: 607\n",
      "Episode 97: reward: 0.000, steps: 272\n",
      "Episode 98: reward: 0.000, steps: 292\n",
      "Episode 99: reward: 0.000, steps: 253\n",
      "Episode 100: reward: 0.000, steps: 306\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "Game = Game_Menu(800,600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
